{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{\\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{{\\bf Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{{\\bf Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{{\\bf Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{{\\bf Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{\\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{\\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{\\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{\\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{\\MakeUppercase{\\coregionalizationScalar}}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{\\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{\\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{\\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{\\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{\\MakeUppercase{\\dataScalar}}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{\\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{\\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{\\degreeScalar}}\n",
    "\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{\\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{\\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{\\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{\\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{\\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{\\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{\\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{\\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{\\MakeUppercase{\\inducingInputScalar}}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{\\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{{\\bf X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{{\\bf \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{\\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{\\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{\\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{\\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{\\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{\\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{\\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{\\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{\\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{\\mappingFunction}}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{\\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{\\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{\\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{\\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{\\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{\\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{\\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{\\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{\\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{\\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{\\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{\\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{{\\bf \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{\\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{\\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{\\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{\\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fidelity Modelling\n",
    "\n",
    "Deep neural networks are a powerful approach to dealing with images, speech and language. All interesting domains where we are gathering a large amount of data. \n",
    "\n",
    "Earlier on in my career, I learned an important lesson. I had discovered Gaussian processes, and was interested in approaches for making them practical. Particularly for 'popular' tasks of the day. One of which was classification of handwritten digits: the MNIST data. \n",
    "\n",
    "With Matthias Seeger and Ralf Herbrich I worked on an algorithm for fitting Gaussian process classifier models that was able to scale as well as the dominant methodology of the day, support vector machines, and perform well. The paper was published at NIPS and the work is known as the \"informative vector machine\". It's a moderately successful work, but it did not have the hoped for response of alerting the wider SVM community to the utility and flexibility of Gaussian process models. \n",
    "\n",
    "It's horses for courses, and where as GP Classifiers are interesting models, it seems fairly pointless to direct efforts at making a methodology work well in a domain where we already have good coverage. The real success of Gaussian process models came in domains where other frameworks were not appropriate. Hyper parameter optimization springs to mind [@Snoek:practical12], a paper with over a thousand citations. My own most successful work, GP-LVM exploits characteristics of GPs that are not associated with other models [@Lawrence:pnpca05].\n",
    "\n",
    "With that in mind my own philosophy is to look explicitly for domains where the characteristics of Gaussian process models can be more usefully exploited, that is not to say that these models can't be used in the more traditional domains, but a lot of effort could be placed there to merely making them also-rans, whereas in domains where uncertainty is critical the balance is tilted in favour of GPs.\n",
    "\n",
    "As a broad domain I'm excited by the domains of probabilistic numerics, surrogate modelling, emulation and uncertainty quantification. There are two reasons for this. Firstly, while I'm not a fan of the term artificial intelligence, or the image instigated in the public mind by the term, or the narrative futures it implies. The truth is that we are faced with a need for an increasing amount of algorithmic decision making. Decision making that is based on data. The dominant framework for achieving those ends, at the moment, is machine learning. That is through the combination of data with the model to form the prediction, followed by *decision making*. The term AI definitely includes decision making as part of its requirements. But a critical challenge is decision making in the presence of uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Quantification\n",
    "\n",
    "Data in our phones, and our devices is all well and good, but things get more interesting when considering the interaction between our virtual world and the physical world. Interaction is key, because we can choose to spend more effort on data acquisition, computation or inference. Each of these requires a decision.\n",
    "\n",
    "Wikipedia defines [Uncertainty Quanitification](https://en.wikipedia.org/wiki/Uncertainty_quantification) as:\n",
    "\n",
    ">Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.\n",
    "\n",
    "And this drives to the heart of what we require for practical decision making systems. Probabilistic numerics associates uncertainty (in the form of probabilities) with our computations. Emulation (or surrogate modelling) is a key component of this approach because an emulator is a model, we can think of it as a machine learning model, of a computational process. For example, we can think of the Gaussian process model in Bayesian optimization as *surrogate* for the process of optimization. A way of assessing our expectations of what that answer will be without running the optimization. Bayesian optimization then involves an *acquisition function* which encodes our decision making framework about what parameters to try next. \n",
    "\n",
    "Bayesian optimization, therefore, presents a microcosm of the world of uncertainty quantification, where we are taking the particular approach of emulating the system of interest, often with a Gaussian process. We know they perform well in these domains. We have\n",
    "\n",
    "The interaction between the physical and virtual worlds was, for me, a major reason for working at Amazon. When I was exploring Amazon, a colleague explained to me that Amazon is a 'bits and atoms' company. It moves stuff and it moves information. For me, an excellent definition of intelligence is the use of *information* to achieve goals with *less resource*. Amazon is therefore an excellent ecosystem for exploring intelligence. However, to do so we need good models of the physical world, and approaches to decision making in the physical world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motor Racing Example\n",
    "\n",
    "But let's move aware from personal corporate allegiances and in to a domain that is more neutral territory for most machine learning practitioners. Formula one motor racing. How can we use information and compute to improve motor racing? There are multiple decisions to make in a Formula One racing team. But one of the critical decisions is the design of the car, or the chassis. In particular, it is important to build a chassis that performs well aerodynamically. We'll consider chassis design as a motivational example.\n",
    "\n",
    "Our basic requirement is that a car should corner well, through improved grip driven by aerodynamic downforce, but should experience low drag, for high speed on straights. Formula One teams have three sources of information they can use judge components they design. First of all there is computational information. Computational fluid dynamics simulations are used to perform virtual assessments of a components performance. They have the advantage of being possible to undertake before a part is even built, and they can explore all track conditions for example sharp corners which are difficult to recreate in a wind tunnel (https://www.youtube.com/watch?v=9oD7Jk7mm8o). They have the disadvantge of being slow and compute intensive, and assumptions are required about the nature of turbulent flow due to the intractability of Navier Stokes equations. A further source of information is the wind tunnel. In a wind tunnel a 1/3 scale model of the car is tested under different driving conditons. Formula One wind tunnels contain a rolling road and spinning wheels to accurately recreate the difficult conditions generated by rotating wheels. Finally, components are tested at tracks on race weekends, and also in regulated testing sessions.\n",
    "\n",
    "So some of the information is in the computer, CFD, some of it can be extracted from a wind tunnel, and some of it is on the track. But in practice the real question is, how will a particular part affect perfomance of a car across a season, or on a particular track? To evaluate this, a further simulation is required, one that places cars in all the different conditions they will experience across a season. Many teams use a surrogate model in this simulator, one that summarizes the understanding of the vehicle performance from all the different experiments: computational, wind tunnel and real world. \n",
    "\n",
    "In the field of uncertainty quantification, such a challenge is known as *multi-fidelity emulation*, and it's widely suggested to achieve it with Gaussian processes [@Kennedy:code00,@Kandasamy:multifidelity16,@Alonso:virtual17,@Perdikaris:multifidelity17,@LeGratiet:cokriging12]\n",
    "\n",
    "https://youtu.be/6ddBAX8-BHQ\n",
    "\n",
    "In the ideal world we could even imagine a system that is automated to gain the most information about a particular car configurations capabilities through automatically selection the right testing scenarios: CFD, wind tunnel, track. In formula one, each of these testing scenarios is artificially regulated by the sports governing body. They regulate CFD compute cycles, they regulate hours of wind tunnel testing and they regulate testing time on the track. So the amount of information is restricted. Intelligent decision making would make optimal use of each of the different modalities. To do that, we need to simultaneously quantify how much information we expect to get from each modality as we build the surrogate model of the car. \n",
    "\n",
    "The parallels between multi-fidelity emulation and Bayesian optimization can easily be seen. And it can also be understood that uncertainty quantification, when combined with decision making, is simply a pragmatic approach to algorithmic decision making or artificial intelligence, as it is now popularly called. But the terms are less emotive. \n",
    "\n",
    "\n",
    "\n",
    "in the public mind  reinterpretation of numerical algorithms as probabilistic. Why? Well in artificial intelligence we are interested in decision making \n",
    "As an example of a domain where deep Gaussian process models are an interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf width=1000 height=500></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mathematical Set Up\n",
    "\n",
    "The traditional approach to this form of multi-fidelity emulation is to assume that\n",
    "\n",
    "$$\\mappingFunction_i\\left(\\inputVector\\right) = \\rho\\mappingFunction_{i-1}\\left(\\inputVector\\right) + \\delta_i\\left(\\inputVector \\right)$$\n",
    "\n",
    "where $\\mappingFunction_{i-1}\\left(\\inputVector\\right)$ is a low fidelity simulation of the problem of interest and $\\mappingFunction_i\\left(\\inputVector\\right)$ is a higher fidelity simulation. The function $delta_i\\left(\\inputVector \\right)$ represents the difference between the lower and higher fidelity simulation, which is considered additive. The additive form of this covariance means that if $\\mappingFunction_{0}\\left(\\inputVector\\right)$ and $\\left\\{\\delta_i\\left(\\inputVector \\right)\\right\\}_{i=1}^m$ are all Gaussian processes, then the process over all fidelities of simuation will be a joint Gaussian process.\n",
    "\n",
    "But with Deep Gaussian processes we can consider the form \n",
    "\n",
    "$$\\mappingFunction_i\\left(\\inputVector\\right) = \\mappingFunctionTwo_{i}\\left(\\mappingFunction_{i-1}\\left(\\inputVector\\right)\\right) + \\delta_i\\left(\\inputVector \\right),$$\n",
    "\n",
    "where the low fidelity representation is non linearly transformed by $\\mappingFunctionTwo(\\cdot)$ before use in the process. This is the approach taken in @Perdikaris:multifidelity17. But once we accept that these models can be composed, a highly flexible framework can emerge. A key point is that the data enters the model at different levels, and represents different aspects. For example in the race car example there are car set ups that we can't explore in the wind tunnel (sharp corners) but can be explored both on track and in CFD simulation. \n",
    "\n",
    "DIAGRAM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Simple example of multifidelity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@incollection{Snoek:practical12,\n",
    "title = {Practical Bayesian Optimization of Machine Learning Algorithms},\n",
    "author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},\n",
    "booktitle = {Advances in Neural Information Processing Systems 25},\n",
    "editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},\n",
    "pages = {2951--2959},\n",
    "year = {2012},\n",
    "publisher = {Curran Associates, Inc.},\n",
    "url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf}\n",
    "}\n",
    "@Article{Lawrence:pnpca05,\n",
    "  author =\t {Neil D. Lawrence},\n",
    "  title =\t {Probabilistic Non-linear Principal Component\n",
    "                  Analysis with {G}aussian Process Latent Variable\n",
    "                  Models},\n",
    "  journal =\t jmlr,\n",
    "  year =\t 2005,\n",
    "  volume =\t 6,\n",
    "  pages =\t {1783--1816},\n",
    "  month =\t 11,\n",
    "  pdf = {http://www.jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf},\n",
    "  abstract =\t {Summarising a high dimensional data set with a low\n",
    "                  dimensional embedding is a standard approach for\n",
    "                  exploring its structure. In this paper we provide an\n",
    "                  overview of some existing techniques for discovering\n",
    "                  such embeddings. We then introduce a novel\n",
    "                  probabilistic interpretation of principal component\n",
    "                  analysis (PCA) that we term dual probabilistic PCA\n",
    "                  (DPPCA). The DPPCA model has the additional\n",
    "                  advantage that the linear mappings from the embedded\n",
    "                  space can easily be non-linearised through Gaussian\n",
    "                  processes. We refer to this model as a Gaussian\n",
    "                  process latent variable model (GP-LVM). Through\n",
    "                  analysis of the GP-LVM objective function, we relate\n",
    "                  the model to popular spectral techniques such as\n",
    "                  kernel PCA and multidimensional scaling. We then\n",
    "                  review a practical algorithm for GP-LVMs in the\n",
    "                  context of large data sets and develop it to also\n",
    "                  handle discrete valued data and missing\n",
    "                  attributes. We demonstrate the model on a range of\n",
    "                  real-world and artificially generated data sets.}\n",
    "}\n",
    "@article{Kennedy:code00,\n",
    " ISSN = {00063444},\n",
    " URL = {http://www.jstor.org/stable/2673557},\n",
    " abstract = {We consider prediction and uncertainty analysis for complex computer codes which can be run at different levels of sophistication. In particular, we wish to improve efficiency by combining expensive runs of the most complex versions of the code with relatively cheap runs from one or more simpler approximations. A Bayesian approach is described in which prior beliefs about the codes are represented in terms of Gaussian processes. An example is presented using two versions of an oil reservoir simulator.},\n",
    " author = {M. C. Kennedy and A. O'Hagan},\n",
    " journal = {Biometrika},\n",
    " number = {1},\n",
    " pages = {1-13},\n",
    " publisher = {[Oxford University Press, Biometrika Trust]},\n",
    " title = {Predicting the Output from a Complex Computer Code When Fast Approximations Are Available},\n",
    " volume = {87},\n",
    " year = {2000}\n",
    "}\n",
    "@ARTICLE{LeGratiet:cokriging12,\n",
    "   author = {{Le Gratiet}, L.},\n",
    "    title = \"{Recursive co-kriging model for Design of Computer experiments with multiple levels of fidelity with an application to hydrodynamic}\",\n",
    "  journal = {ArXiv e-prints},\n",
    "archivePrefix = \"arXiv\",\n",
    "   eprint = {1210.0686},\n",
    " primaryClass = \"math.ST\",\n",
    " keywords = {Mathematics - Statistics Theory},\n",
    "     year = 2012,\n",
    "    month = oct,\n",
    "   adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1210.0686L},\n",
    "  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n",
    "}\n",
    "\n",
    "@ARTICLE{Kandasamy:multifidelity16,\n",
    "   author = {{Kandasamy}, K. and {Dasarathy}, G. and {Oliva}, J.~B. and {Schneider}, J. and \n",
    "\t{Poczos}, B.},\n",
    "    title = \"{Multi-fidelity Gaussian Process Bandit Optimisation}\",\n",
    "  journal = {ArXiv e-prints},\n",
    "archivePrefix = \"arXiv\",\n",
    "   eprint = {1603.06288},\n",
    " primaryClass = \"stat.ML\",\n",
    " keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning},\n",
    "     year = 2016,\n",
    "    month = mar,\n",
    "   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160306288K},\n",
    "  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n",
    "}\n",
    "@inproceedings{Alonso:virtual17,\n",
    "  title = {Virtual vs. {R}eal: Trading Off Simulations and Physical Experiments in Reinforcement Learning with {B}ayesian Optimization},\n",
    "  author = {Marco, Alonso and Berkenkamp, Felix and Hennig, Philipp and Schoellig, Angela P. and Krause, Andreas and Schaal, Stefan and Trimpe, Sebastian},\n",
    "  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},\n",
    "  pages = {1557-1563},\n",
    "  month = may,\n",
    "  year = {2017},\n",
    "  month_numeric = {5}\n",
    "}\n",
    "\n",
    "@article {Perdikaris:multifidelity17,\n",
    "\tauthor = {Perdikaris, P. and Raissi, M. and Damianou, A. and Lawrence, N. D. and Karniadakis, G. E.},\n",
    "\ttitle = {Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling},\n",
    "\tvolume = {473},\n",
    "\tnumber = {2198},\n",
    "\tyear = {2017},\n",
    "\tdoi = {10.1098/rspa.2016.0751},\n",
    "\tpublisher = {The Royal Society},\n",
    "\tabstract = {Multi-fidelity modelling enables accurate inference of quantities of interest by synergistically combining realizations of low-cost/low-fidelity models with a small set of high-fidelity observations. This is particularly effective when the low- and high-fidelity models exhibit strong correlations, and can lead to significant computational gains over approaches that solely rely on high-fidelity models. However, in many cases of practical interest, low-fidelity models can only be well correlated to their high-fidelity counterparts for a specific range of input parameters, and potentially return wrong trends and erroneous predictions if probed outside of their validity regime. Here we put forth a probabilistic framework based on Gaussian process regression and nonlinear autoregressive schemes that is capable of learning complex nonlinear and space-dependent cross-correlations between models of variable fidelity, and can effectively safeguard against low-fidelity models that provide wrong trends. This introduces a new class of multi-fidelity information fusion algorithms that provide a fundamental extension to the existing linear autoregressive methodologies, while still maintaining the same algorithmic complexity and overall computational cost. The performance of the proposed methods is tested in several benchmark problems involving both synthetic and real multi-fidelity datasets from computational fluid dynamics simulations.},\n",
    "\tissn = {1364-5021},\n",
    "\tURL = {http://rspa.royalsocietypublishing.org/content/473/2198/20160751},\n",
    "\teprint = {http://rspa.royalsocietypublishing.org/content/473/2198/20160751.full.pdf},\n",
    "\tjournal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
