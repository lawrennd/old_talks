{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Probabilistic Modelling with with Gaussian Processes\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), Amazon and University of Sheffield\n",
    "\n",
    "**Abstract**: Neural network models are algorithmically simple, but mathematically\n",
    "complex. Gaussian process models are mathematically simple, but\n",
    "algorithmically complex. In this tutorial we will explore Deep Gaussian\n",
    "Process models. They bring advantages in their mathematical simplicity\n",
    "but are challenging in their algorithmic complexity. We will give an\n",
    "overview of Gaussian processes and highlight the algorithmic\n",
    "approximations that allow us to stack Gaussian process models: they are\n",
    "based on variational methods. In the last part of the tutorial will\n",
    "explore a use case exemplar: uncertainty quantification. We end with\n",
    "open questions.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "### What is Machine Learning?\n",
    "\n",
    ". . .\n",
    "\n",
    "$$ \\text{data} + \\text{model} \\xrightarrow{\\text{compute}} \\text{prediction}$$\n",
    "\n",
    ". . .\n",
    "\n",
    "-   **data** : observations, could be actively or passively acquired\n",
    "    (meta-data).\n",
    "\n",
    ". . .\n",
    "\n",
    "-   **model** : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of the\n",
    "    universe. Inductive bias.\n",
    "\n",
    ". . .\n",
    "\n",
    "-   **prediction** : an action to be taken or a categorization or a\n",
    "    quality score.\n",
    "\n",
    ". . .\n",
    "\n",
    "-   Royal Society Report: [Machine Learning: Power and Promise of\n",
    "    Computers that Learn by\n",
    "    Example](https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf)\n",
    "\n",
    "### What is Machine Learning?\n",
    "\n",
    "$$\\text{data} + \\text{model} \\xrightarrow{\\text{compute}} \\text{prediction}$$\n",
    "\n",
    ". . .\n",
    "\n",
    "-   To combine data with a model need:\n",
    "\n",
    ". . .\n",
    "\n",
    "-   **a prediction function** $\\mappingFunction(\\cdot)$ includes our\n",
    "    beliefs about the regularities of the universe\n",
    "\n",
    ". . .\n",
    "\n",
    "-   **an objective function** $\\errorFunction(\\cdot)$ defines the cost\n",
    "    of misprediction.\n",
    "\n",
    "### Artificial Intelligence\n",
    "\n",
    "-   Machine learning is a mainstay because of importance of prediction.\n",
    "\n",
    "### Uncertainty\n",
    "\n",
    "-   Uncertainty in prediction arises from:\n",
    "-   scarcity of training data and\n",
    "-   mismatch between the set of prediction functions we choose and all\n",
    "    possible prediction functions.\n",
    "-   Also uncertainties in objective, leave those for another day.\n",
    "\n",
    "### Neural Networks and Prediction Functions\n",
    "\n",
    "-   adaptive non-linear function models inspired by simple neuron models\n",
    "    [@McCulloch:neuron43]\n",
    "\n",
    "-   have become popular because of their ability to model data.\n",
    "\n",
    "-   can be composed to form highly complex functions\n",
    "\n",
    "-   start by focussing on one hidden layer\n",
    "\n",
    "### Prediction Function of One Hidden Layer\n",
    "\n",
    "$$\n",
    "\\mappingFunction(\\inputVector) = \\left.\\mappingVector^{(2)}\\right.^\\top \\activationVector(\\mappingMatrix_{1}, \\inputVector)\n",
    "$$\n",
    "\n",
    "$\\mappingFunction(\\cdot)$ is a scalar function with vector inputs,\n",
    "\n",
    "$\\activationVector(\\cdot)$ is a vector function with vector inputs.\n",
    "\n",
    "-   dimensionality of the vector function is known as the number of\n",
    "    hidden units, or the number of neurons.\n",
    "\n",
    "-   elements of $\\activationVector(\\cdot)$ are the *activation* function\n",
    "    of the neural network\n",
    "\n",
    "-   elements of $\\mappingMatrix_{1}$ are the parameters of the\n",
    "    activation functions.\n",
    "\n",
    "### Relations with Classical Statistics\n",
    "\n",
    "-   In statistics activation functions are known as *basis functions*.\n",
    "\n",
    "-   would think of this as a *linear model*: not linear predictions,\n",
    "    linear in the parameters\n",
    "\n",
    "-   $\\mappingVector_{1}$ are *static* parameters.\n",
    "\n",
    "### Adaptive Basis Functions\n",
    "\n",
    "-   In machine learning we optimize $\\mappingMatrix_{1}$ as well as\n",
    "    $\\mappingMatrix_{2}$ (which would normally be denoted in statistics\n",
    "    by $\\boldsymbol{\\beta}$).\n",
    "\n",
    "-   This tutorial: revisit that decision: follow the path of\n",
    "    @Neal:bayesian94 and @MacKay:bayesian92.\n",
    "\n",
    "-   Consider the probabilistic approach.\n",
    "\n",
    "### Probabilistic Modelling\n",
    "\n",
    "This Bayesian approach is designed to deal with uncertainty arising from\n",
    "fitting our prediction function to the data we have, a reduced data set.\n",
    "\n",
    "The Bayesian approach can be derived from a broader understanding of\n",
    "what our objective is. If we accept that we can jointly represent all\n",
    "things that happen in the world with a probability distribution, then we\n",
    "can interogate that probability to make predictions. So, if we are\n",
    "interested in predictions, $\\dataScalar_*$ at future points input\n",
    "locations of interest, $\\inputVector_*$ given previously training data,\n",
    "$\\dataVector$ and corresponding inputs, $\\inputMatrix$, then we are\n",
    "really interogating the following probability density, $$\n",
    "p(\\dataScalar_*|\\dataVector, \\inputMatrix, \\inputVector_*),\n",
    "$$ there is nothing controversial here, as long as you accept that you\n",
    "have a good joint model of the world around you that relates test data\n",
    "to training data,\n",
    "$p(\\dataScalar_*, \\dataVector, \\inputMatrix, \\inputVector_*)$ then this\n",
    "conditional distribution can be recovered through standard rules of\n",
    "probability\n",
    "($\\text{data} + \\text{model} \\rightarrow \\text{prediction}$).\n",
    "\n",
    "We can construct this joint density through the use of the following\n",
    "decomposition: $$\n",
    "p(\\dataScalar_*|\\dataVector, \\inputMatrix, \\inputVector_*) = \\int p(\\dataScalar_*|\\inputVector_*, \\mappingMatrix) p(\\mappingMatrix | \\dataVector, \\inputMatrix) \\text{d} \\mappingMatrix\n",
    "$$\n",
    "\n",
    "where, for convenience, we are assuming *all* the parameters of the\n",
    "model are now represented by $\\parameterVector$ (which contains\n",
    "$\\mappingMatrix$ and $\\mappingMatrixTwo$) and\n",
    "$p(\\parameterVector | \\dataVector, \\inputMatrix)$ is recognised as the\n",
    "posterior density of the parameters given data and\n",
    "$p(\\dataScalar_*|\\inputVector_*, \\parameterVector)$ is the *likelihood*\n",
    "of an individual test data point given the parameters.\n",
    "\n",
    "The likelihood of the data is normally assumed to be independent across\n",
    "the parameters, $$\n",
    "p(\\dataVector|\\inputMatrix, \\mappingMatrix) \\prod_{i=1}^\\numData p(\\dataScalar_i|\\inputVector_i, \\mappingMatrix),$$\n",
    "\n",
    "and if that is so, it is easy to extend our predictions across all\n",
    "future, potential, locations, $$\n",
    "p(\\dataVector_*|\\dataVector, \\inputMatrix, \\inputMatrix_*) = \\int p(\\dataVector_*|\\inputMatrix_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) \\text{d} \\parameterVector.\n",
    "$$\n",
    "\n",
    "The likelihood is also where the *prediction function* is incorporated.\n",
    "For example in the regression case, we consider an objective based\n",
    "around the Gaussian density, $$\n",
    "p(\\dataScalar_i | \\mappingFunction(\\inputVector_i)) = \\frac{1}{\\sqrt{2\\pi \\dataStd^2}} \\exp\\left(-\\frac{\\left(\\dataScalar_i - \\mappingFunction(\\inputVector_i)\\right)^2}{2\\dataStd^2}\\right)\n",
    "$$\n",
    "\n",
    "In short, that is the classical approach to probabilistic inference, and\n",
    "all approaches to Bayesian neural networks fall within this path. For a\n",
    "deep probabilistic model, we can simply take this one stage further and\n",
    "place a probability distribution over the input locations, $$\n",
    "p(\\dataVector_*|\\dataVector) = \\int p(\\dataVector_*|\\inputMatrix_*, \\parameterVector) p(\\parameterVector | \\dataVector, \\inputMatrix) p(\\inputMatrix) p(\\inputMatrix_*) \\text{d} \\parameterVector \\text{d} \\inputMatrix \\text{d}\\inputMatrix_*\n",
    "$$ and we have *unsupervised learning* (from where we can get deep\n",
    "generative models).\n",
    "\n",
    "### Graphical Models\n",
    "\n",
    "-   Represent joint distribution through *conditional dependencies*.\n",
    "\n",
    "-   E.g. Markov chain\n",
    "\n",
    "$$p(\\dataVector) = p(\\dataScalar_\\numData | \\dataScalar_{\\numData-1}) p(\\dataScalar_{\\numData-1}|\\dataScalar_{\\numData-2}) \\dots p(\\dataScalar_{2} | \\dataScalar_{1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica']}, size=30)\n",
    "rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = daft.PGM(shape=[3, 1],\n",
    "               origin=[0, 0], \n",
    "               grid_unit=5, \n",
    "               node_unit=1.9, \n",
    "               observed_style='shaded',\n",
    "              line_width=3)\n",
    "\n",
    "\n",
    "pgm.add_node(daft.Node(\"y_1\", r\"$y_1$\", 0.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_2\", r\"$y_2$\", 1.5, 0.5, fixed=False))\n",
    "pgm.add_node(daft.Node(\"y_3\", r\"$y_3$\", 2.5, 0.5, fixed=False))\n",
    "pgm.add_edge(\"y_1\", \"y_2\")\n",
    "pgm.add_edge(\"y_2\", \"y_3\")\n",
    "\n",
    "pgm.render().figure.savefig(\"../slides/diagrams/ml/markov.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../slides/diagrams/ml/markov.svg\" align=\"\">\n",
    "\n",
    "### \n",
    "\n",
    "Predict Perioperative Risk of Clostridium Difficile Infection Following\n",
    "Colon Surgery [@Steele:predictive12]\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/bayes-net-diagnosis.png\" width=\"40%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "\n",
    "### Performing Inference\n",
    "\n",
    "-   Easy to write in probabilities\n",
    "\n",
    "-   But underlying this is a wealth of computational challenges.\n",
    "\n",
    "-   High dimensional integrals typically require approximation.\n",
    "\n",
    "Statisticians realized these challenges early on, indeed, so early that\n",
    "they were actually physicists, both Laplace and Gauss worked on models\n",
    "such as this, in Gauss's case he made his career on prediction of the\n",
    "location of the lost planet (later reclassified as a asteroid, then\n",
    "dwarf planet), Ceres. Gauss and Laplace made use of maximum a posteriori\n",
    "estimates for simplifying their computations and Laplace developed\n",
    "Laplace's method (and invented the Gaussian density) to expand around\n",
    "that mode. But classical statistics needs better guarantees around model\n",
    "performance and interpretation, and as a result has focussed more on the\n",
    "*linear* model implied by $$\n",
    "  \\mappingFunction(\\inputVector) = \\left.\\mappingVector^{(2)}\\right.^\\top \\activationVector(\\mappingMatrix_1, \\inputVector)\n",
    "  $$\n",
    "\n",
    "-   Hold $\\mappingMatrix_1$ fixed for given analysis.\n",
    "\n",
    "-   Gaussian prior for $\\mappingMatrix$, $$\n",
    "      \\mappingVector^{(2)} \\sim \\gaussianSamp{\\zerosVector}{\\covarianceMatrix}.\n",
    "      $$ $$\n",
    "      \\dataScalar_i = \\mappingFunction(\\inputVector_i) + \\noiseScalar_i,\n",
    "      $$ where $$\n",
    "      \\noiseScalar_i \\sim \\gaussianSamp{0}{\\dataStd^2}\n",
    "      $$\n",
    "\n",
    "### Linear Gaussian Models\n",
    "\n",
    "-   Normally integrals are complex but for this Gaussian linear case\n",
    "    they are trivial.\n",
    "\n",
    "### Multivariate Gaussian Properties\n",
    "\n",
    "### Recall Univariate Gaussian Properties\n",
    "\n",
    ". . .\n",
    "\n",
    "1.  Sum of Gaussian variables is also Gaussian.\n",
    "\n",
    "$$\\dataScalar_i \\sim \\gaussianSamp{\\mu_i}{\\dataStd_i^2}$$\n",
    "\n",
    ". . .\n",
    "\n",
    "$$\\sum_{i=1}^{\\numData} \\dataScalar_i \\sim \\gaussianSamp{\\sum_{i=1}^\\numData \\mu_i}{\\sum_{i=1}^\\numData\\dataStd_i^2}$$\n",
    "\n",
    ". . .\n",
    "\n",
    "2.  Scaling a Gaussian leads to a Gaussian.\n",
    "\n",
    ". . .\n",
    "\n",
    "$$\\dataScalar \\sim \\gaussianSamp{\\mu}{\\dataStd^2}$$\n",
    "\n",
    ". . .\n",
    "\n",
    "$$\\mappingScalar\\dataScalar\\sim \\gaussianSamp{\\mappingScalar\\mu}{\\mappingScalar^2 \\dataStd^2}$$\n",
    "\n",
    "### Multivariate Consequence\n",
    "\n",
    "[If]{align=\"left\"}\n",
    "\n",
    "$$\\inputVector \\sim \\gaussianSamp{\\boldsymbol{\\mu}}{\\boldsymbol{\\Sigma}}$$\n",
    "\n",
    ". . .\n",
    "\n",
    "[And]{align=\"left\"} $$\\dataVector= \\mappingMatrix\\inputVector$$\n",
    "\n",
    ". . .\n",
    "\n",
    "[Then]{align=\"left\"}\n",
    "$$\\dataVector \\sim \\gaussianSamp{\\mappingMatrix\\boldsymbol{\\mu}}{\\mappingMatrix\\boldsymbol{\\Sigma}\\mappingMatrix^\\top}$$\n",
    "\n",
    "### Linear Gaussian Models\n",
    "\n",
    "1.  linear Gaussian models are easier to deal with\n",
    "2.  Even the parameters *within* the process can be handled, by\n",
    "    considering a particular limit.\n",
    "\n",
    "Let's first of all review the properties of the multivariate Gaussian\n",
    "distribution that make linear Gaussian models easier to deal with. We'll\n",
    "return to the, perhaps surprising, result on the parameters within the\n",
    "nonlinearity, $\\parameterVector$, shortly.\n",
    "\n",
    "To work with linear Gaussian models, to find the marginal likelihood all\n",
    "you need to know is the following rules. If $$\n",
    "\\dataVector = \\mappingMatrix \\inputVector + \\noiseVector,\n",
    "$$ where $\\dataVector$, $\\inputVector$ and $\\noiseVector$ are vectors\n",
    "and we assume that $\\inputVector$ and $\\noiseVector$ are drawn from\n",
    "multivariate Gaussians, $$\\begin{align}\n",
    "\\inputVector & \\sim \\gaussianSamp{\\meanVector}{\\covarianceMatrix}\\\\\n",
    "\\noiseVector & \\sim \\gaussianSamp{\\zerosVector}{\\covarianceMatrixTwo}\n",
    "\\end{align}$$ then we know that $\\dataVector$ is also drawn from a\n",
    "multivariate Gaussian with, $$\n",
    "\\dataVector \\sim \\gaussianSamp{\\mappingMatrix\\meanVector}{\\mappingMatrix\\covarianceMatrix\\mappingMatrix^\\top + \\covarianceMatrixTwo}.\n",
    "$$ With apprioriately defined covariance, $\\covarianceTwoMatrix$, this\n",
    "is actually the marginal likelihood for Factor Analysis, or\n",
    "Probabilistic Principal Component Analysis [@Tipping:probpca99], because\n",
    "we integrated out the inputs (or *latent* variables they would be called\n",
    "in that case).\n",
    "\n",
    "However, we are focussing on what happens in models which are non-linear\n",
    "in the inputs, whereas the above would be *linear* in the inputs. To\n",
    "consider these, we introduce a matrix, called the design matrix. We set\n",
    "each activation function computed at each data point to be $$\n",
    "\\activationScalar_{i,j} = \\activationScalar(\\mappingVector^{(1)}_{j}, \\inputVector_{i})\n",
    "$$ and define the matrix of activations (known as the *design matrix* in\n",
    "statistics) to be, $$\n",
    "\\activationMatrix = \n",
    "\\begin{bmatrix}\n",
    "\\activationScalar_{1, 1} & \\activationScalar_{1, 2} & \\dots & \\activationScalar_{1, \\numHidden} \\\\\n",
    "\\activationScalar_{1, 2} & \\activationScalar_{1, 2} & \\dots & \\activationScalar_{1, \\numData} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\activationScalar_{\\numData, 1} & \\activationScalar_{\\numData, 2} & \\dots & \\activationScalar_{\\numData, \\numHidden}\n",
    "\\end{bmatrix}.\n",
    "$$ By convention this matrix always has $\\numData$ rows and $\\numHidden$\n",
    "columns, now if we define the vector of all noise corruptions,\n",
    "$\\noiseVector = \\left[\\noiseScalar_1, \\dots \\noiseScalar_\\numData\\right]^\\top$.\n",
    "\n",
    "### Matrix Representation of a Neural Network\n",
    "\n",
    "$$\\dataScalar\\left(\\inputVector\\right) = \\activationVector\\left(\\inputVector\\right)^\\top \\mappingVector + \\noiseScalar$$\n",
    "\n",
    ". . .\n",
    "\n",
    "$$\\dataVector = \\activationMatrix\\mappingVector + \\noiseVector$$\n",
    "\n",
    ". . .\n",
    "\n",
    "$$\\noiseVector \\sim \\gaussianSamp{\\zerosVector}{\\dataStd^2\\eye}$$\n",
    "\n",
    "{ If we define the prior distribution over the vector $\\mappingVector$\n",
    "to be Gaussian,} $$\n",
    "\\mappingVector \\sim \\gaussianSamp{\\zerosVector}{\\alpha\\eye},\n",
    "$$\n",
    "\n",
    "{ then we can use rules of multivariate Gaussians to see that,} $$\n",
    "\\dataVector \\sim \\gaussianSamp{\\zerosVector}{\\alpha \\activationMatrix \\activationMatrix^\\top + \\dataStd^2 \\eye}.\n",
    "$$\n",
    "\n",
    "In other words, our training data is distributed as a multivariate\n",
    "Gaussian, with zero mean and a covariance given by $$\n",
    "\\kernelMatrix = \\alpha \\activationMatrix \\activationMatrix^\\top + \\dataStd^2 \\eye.\n",
    "$$\n",
    "\n",
    "This is an $\\numData \\times \\numData$ size matrix. Its elements are in\n",
    "the form of a function. The maths shows that any element, index by $i$\n",
    "and $j$, is a function *only* of inputs associated with data points $i$\n",
    "and $j$, $\\dataVector_i$, $\\dataVector_j$.\n",
    "$\\kernel_{i,j} = \\kernel\\left(\\inputVector_i, \\inputVector_j\\right)$\n",
    "\n",
    "If we look at the portion of this function associated only with\n",
    "$\\mappingFunction(\\cdot)$, i.e. we remove the noise, then we can write\n",
    "down the covariance associated with our neural network, $$\n",
    "\\kernel_\\mappingFunction\\left(\\inputVector_i, \\inputVector_j\\right) = \\alpha \\activationVector\\left(\\mappingMatrix_1, \\inputVector_i\\right)^\\top \\activationVector\\left(\\mappingMatrix_1, \\inputVector_j\\right)\n",
    "$$ so the elements of the covariance or *kernel* matrix are formed by\n",
    "inner products of the rows of the *design matrix*.\n",
    "\n",
    "This is the essence of a Gaussian process. Instead of making assumptions\n",
    "about our density over each data point, $\\dataScalar_i$ as i.i.d. we\n",
    "make a joint Gaussian assumption over our data. The covariance matrix is\n",
    "now a function of both the parameters of the activation function,\n",
    "$\\mappingMatrixTwo$, and the input variables, $\\inputMatrix$. This comes\n",
    "about through integrating out the parameters of the model,\n",
    "$\\mappingVector$.\n",
    "\n",
    "We can basically put anything inside the basis functions, and many\n",
    "people do. These can be deep kernels [@Cho:deep09] or we can learn the\n",
    "parameters of a convolutional neural network inside there.\n",
    "\n",
    "Viewing a neural network in this way is also what allows us to beform\n",
    "sensible *batch* normalizations [@Ioffe:batch15].\n",
    "\n",
    "### Non-degenerate Gaussian Processes\n",
    "\n",
    "-   This process is *degenerate*.\n",
    "\n",
    "-   Covariance function is of rank at most $\\numHidden$.\n",
    "\n",
    "-   As $\\numData \\rightarrow \\infty$, covariance matrix is not full\n",
    "    rank.\n",
    "\n",
    "-   Leading to $\\det{\\kernelMatrix} = 0$\n",
    "\n",
    "### Infinite Networks\n",
    "\n",
    "-   In ML Radford Neal [@Neal:bayesian94] asked \"what would happen if\n",
    "    you took $\\numHidden \\rightarrow \\infty$?\"\n",
    "\n",
    "[<img class=\"\" src=\"../slides/diagrams/neal-infinite-priors.png\" width=\"80%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">](http://www.cs.toronto.edu/~radford/ftp/thesis.pdf)\n",
    "\n",
    "*Page 37 of Radford Neal's 1994 thesis*\n",
    "\n",
    "### Roughly Speaking\n",
    "\n",
    "-   Instead of\n",
    "\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\kernel_\\mappingFunction\\left(\\inputVector_i, \\inputVector_j\\right) & = \\alpha \\activationVector\\left(\\mappingMatrix_1, \\inputVector_i\\right)^\\top \\activationVector\\left(\\mappingMatrix_1, \\inputVector_j\\right)\\\\\n",
    "  & = \\alpha \\sum_k \\activationScalar\\left(\\mappingVector^{(1)}_k, \\inputVector_i\\right) \\activationScalar\\left(\\mappingVector^{(1)}_k, \\inputVector_j\\right)\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "-   Sample infinitely many from a prior density,\n",
    "    $p(\\mappingVector^{(1)})$,\n",
    "\n",
    "$$\n",
    "\\kernel_\\mappingFunction\\left(\\inputVector_i, \\inputVector_j\\right) = \\alpha \\int \\activationScalar\\left(\\mappingVector^{(1)}, \\inputVector_i\\right) \\activationScalar\\left(\\mappingVector^{(1)}, \\inputVector_j\\right) p(\\mappingVector^{(1)}) \\text{d}\\mappingVector^{(1)}\n",
    "$$\n",
    "\n",
    "-   Also applies for non-Gaussian $p(\\mappingVector^{(1)})$ because of\n",
    "    the *central limit theorem*.\n",
    "\n",
    "### Simple Probabilistic Program\n",
    "\n",
    "-   If $$\n",
    "      \\begin{align*} \n",
    "      \\mappingVector^{(1)} & \\sim p(\\cdot)\\\\ \\phi_i & = \\activationScalar\\left(\\mappingVector^{(1)}, \\inputVector_i\\right), \n",
    "      \\end{align*}\n",
    "      $$ has finite variance.\n",
    "\n",
    "-   Then taking number of hidden units to infinity, is also a Gaussian\n",
    "    process.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "-   Chapter 2 of Neal's thesis [@Neal:bayesian94]\n",
    "\n",
    "-   Rest of Neal's thesis. [@Neal:bayesian94]\n",
    "\n",
    "-   David MacKay's PhD thesis [@MacKay:bayesian92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s compute_kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "plot.rejection_samples(compute_kernel, kernel=eq_cov, \n",
    "                       lengthscale=0.25, diagrams='../slides/diagrams/gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_plots('gp_rejection_samples{sample:0>3}.svg', \n",
    "                            '../slides/diagrams/gp', sample=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  {#section-1 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/gp_rejection_sample001.svg\" align=\"\">\n",
    "\n",
    "###  {#section-2 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/gp_rejection_sample002.svg\" align=\"\">\n",
    "\n",
    "###  {#section-3 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/gp_rejection_sample003.svg\" align=\"\">\n",
    "\n",
    "###  {#section-4 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/gp_rejection_sample004.svg\" align=\"\">\n",
    "\n",
    "###  {#section-5 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/gp-rejection-sample005.svg\" align=\"\">\n",
    "\n",
    "<!-- ### Two Dimensional Gaussian Distribution -->\n",
    "<!-- include{_ml/includes/two-d-gaussian.md} -->\n",
    "### Distributions over Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling a Function {#sampling-a-function data-transition=\"none\"}\n",
    "\n",
    "**Multi-variate Gaussians**\n",
    "\n",
    "-   We will consider a Gaussian with a particular structure of\n",
    "    covariance matrix.\n",
    "\n",
    "-   Generate a single sample from this 25 dimensional Gaussian\n",
    "    distribution,\n",
    "    $\\mappingFunctionVector=\\left[\\mappingFunction_{1},\\mappingFunction_{2}\\dots \\mappingFunction_{25}\\right]$.\n",
    "\n",
    "-   We will plot these points against their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s compute_kernel mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s polynomial_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s exponentiated_quadratic mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.two_point_sample(compute_kernel, kernel=exponentiated_quadratic, \n",
    "                      lengthscale=0.5, diagrams='../slides/diagrams/gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            '../slides/diagrams/gp', sample=(0,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution Sample {#gaussian-distribution-sample data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample000.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-1 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample001.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-2 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample002.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-3 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample003.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-4 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample004.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-5 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample005.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-6 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample006.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-7 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample007.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Gaussian Distribution Sample {#gaussian-distribution-sample-8 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample008.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{2}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_2-from-mappingfunction_1 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample009.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{2}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_2-from-mappingfunction_1-1 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample010.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{2}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_2-from-mappingfunction_1-2 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample011.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{2}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_2-from-mappingfunction_1-3 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample012.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Uluru\n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/gp/799px-Uluru_Panorama.jpg\" width=\"\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "\n",
    "### Prediction with Correlated Gaussians\n",
    "\n",
    "-   Prediction of $\\mappingFunction_2$ from $\\mappingFunction_1$\n",
    "    requires *conditional density*.\n",
    "\n",
    "-   Conditional density is *also* Gaussian. $$\n",
    "    p(\\mappingFunction_2|\\mappingFunction_1) = \\gaussianDist{\\mappingFunction_2}{\\frac{\\kernelScalar_{1, 2}}{\\kernelScalar_{1, 1}}\\mappingFunction_1}{ \\kernelScalar_{2, 2} - \\frac{\\kernelScalar_{1,2}^2}{\\kernelScalar_{1,1}}}\n",
    "    $$ where covariance of joint density is given by $$\n",
    "    \\kernelMatrix = \\begin{bmatrix} \\kernelScalar_{1, 1} & \\kernelScalar_{1, 2}\\\\ \\kernelScalar_{2, 1} & \\kernelScalar_{2, 2}\\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            '../slides/diagrams/gp', sample=(13,17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of $\\mappingFunction_{8}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_8-from-mappingfunction_1 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample013.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{8}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_8-from-mappingfunction_1-1 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample014.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{8}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_8-from-mappingfunction_1-2 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample015.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{8}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_8-from-mappingfunction_1-3 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample016.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Prediction of $\\mappingFunction_{8}$ from $\\mappingFunction_{1}$ {#prediction-of-mappingfunction_8-from-mappingfunction_1-4 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/two_point_sample017.svg\" align=\"\">\n",
    "\n",
    "A 25 dimensional correlated random variable (values ploted against\n",
    "index)\n",
    "\n",
    "### Key Object\n",
    "\n",
    "-   Covariance function, $\\kernelMatrix$\n",
    "\n",
    "-   Determines properties of samples.\n",
    "\n",
    "-   Function of $\\inputMatrix$,\n",
    "    $$\\kernelScalar_{i,j} = \\kernelScalar(\\inputVector_i, \\inputVector_j)$$\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "-   Posterior mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$\\mappingFunction_D(\\inputVector_*) = \\kernelVector(\\inputVector_*, \\inputMatrix) \\kernelMatrix^{-1}\n",
    "\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Posterior covariance\n",
    "    $$\\mathbf{C}_* = \\kernelMatrix_{*,*} - \\kernelMatrix_{*,\\mappingFunctionVector}\n",
    "    \\kernelMatrix^{-1} \\kernelMatrix_{\\mappingFunctionVector, *}$$\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "-   Posterior mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$\\mappingFunction_D(\\inputVector_*) = \\kernelVector(\\inputVector_*, \\inputMatrix) \\boldsymbol{\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Posterior covariance\n",
    "    $$\\covarianceMatrix_* = \\kernelMatrix_{*,*} - \\kernelMatrix_{*,\\mappingFunctionVector}\n",
    "    \\kernelMatrix^{-1} \\kernelMatrix_{\\mappingFunctionVector, *}$$\n",
    "\n",
    "### \n",
    "\n",
    "<img src=\"../slides/diagrams/gp_prior_samples_data.svg\" align=\"\">\n",
    "\n",
    "### \n",
    "\n",
    "<img src=\"../slides/diagrams/gp_rejection_samples.svg\" align=\"\">\n",
    "\n",
    "### \n",
    "\n",
    "<img src=\"../slides/diagrams/gp_prediction.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s eq_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=eq_cov, lengthscale=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='../slides/diagrams/kern', \n",
    "                    filename='eq_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentiated Quadratic Covariance\n",
    "\n",
    "$$\n",
    "\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\exp\\left(-\\frac{\\ltwoNorm{\\inputVector - \\inputVector^\\prime}^2}{2\\ell^2}\\right)\n",
    "$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "<img src=\"../slides/diagrams/kern/eq_covariance.svg\" align=\"\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "<iframe src=\"../slides/diagrams/kern/eq_covariance.html\" width=\"512\" height=\"384\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pods\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "\n",
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='../slides/diagrams/datasets/olympic-marathon.svg', transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didnt have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "![image](../slides/diagrams/Stephen_Kiprotich.jpg) <small>Image from\n",
    "Wikimedia Commons <http://bit.ly/16kMKHQ></small>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "### Olympic Marathon Data\n",
    "\n",
    "<img src=\"../slides/diagrams/datasets/olympic-marathon.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(1870,2030,200)[:,np.newaxis]\n",
    "yt_mean, yt_var = m_full.predict(xt)\n",
    "yt_sd=np.sqrt(yt_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/olympic-marathon-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data GP\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/olympic-marathon-gp.svg\" align=\"\">\n",
    "\n",
    "### \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"40%\">\n",
    "<img class=\"\" src=\"../slides/diagrams/turing-run.jpg\" width=\"40%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "<img class=\"\" src=\"../slides/diagrams/turing-times.gif\" width=\"50%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>\n",
    "*Alan Turing, in 1946 he was only 11 minutes slower than the winner of\n",
    "the 1948 games. Would he have won a hypothetical games held in 1946?\n",
    "Source: [Alan Turing Internet\n",
    "Scrapbook](http://www.turing.org.uk/scrapbook/run.html) *\n",
    "</center>\n",
    "### Basis Function Covariance\n",
    "\n",
    "$$\n",
    "\\kernel(\\inputVector, \\inputVector^\\prime) = \\basisVector(\\inputVector)^\\top \\basisVector(\\inputVector^\\prime)\n",
    "$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"45%\">\n",
    "<img src=\"../slides/diagrams/kern/basis_covariance.svg\" align=\"\">\n",
    "</td>\n",
    "<td width=\"45%\">\n",
    "<img class=\"negate\" src=\"../slides/diagrams/kern/basis_covariance.gif\" width=\"40%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "### Brownian Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s brownian_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.linspace(0, 2, 200)[:, np.newaxis]\n",
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         t, \n",
    "                                         kernel=brownian_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='../slides/diagrams/kern', \n",
    "                    filename='brownian_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\kernelScalar(t, t^\\prime) = \\alpha \\min(t, t^\\prime)\n",
    "$$\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "<img src=\"../slides/diagrams/kern/brownian_covariance.svg\" align=\"\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "<iframe src=\"../slides/diagrams/kern/brownian_covariance.html\" width=\"512\" height=\"384\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "### MLP Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load -s mlp_cov mlai.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot\n",
    "import mlai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K, anim=plot.animate_covariance_function(mlai.compute_kernel, \n",
    "                                         kernel=mlp_cov, lengthscale=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_animation(anim, \n",
    "                    diagrams='../slides/diagrams/kern', \n",
    "                    filename='mlp_covariance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\kernelScalar(\\inputVector, \\inputVector^\\prime) = \\alpha \\arcsin\\left(\\frac{w \\inputVector^\\top \\inputVector^\\prime + b}{\\sqrt{\\left(w \\inputVector^\\top \\inputVector + b + 1\\right)\\left(w \\left.\\inputVector^\\prime\\right.^\\top \\inputVector^\\prime + b + 1\\right)}}\\right)\n",
    "$$\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"50%\">\n",
    "<img src=\"../slides/diagrams/kern/mlp_covariance.svg\" align=\"\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "<iframe src=\"../slides/diagrams/kern/mlp_covariance.html\" width=\"512\" height=\"384\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "### \n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/Planck_CMB.png\" width=\"70%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "\n",
    "### \n",
    "\n",
    "<div style=\"fontsize:120px;vertical-align:middle;\">\n",
    "\n",
    "<img src=\"../slides/diagrams/earth_PNG37.png\" width=\"20%\" style=\"display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;\">$=f\\Bigg($\n",
    "<img src=\"../slides/diagrams/Planck_CMB.png\"  width=\"50%\" style=\"display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;\">$\\Bigg)$\n",
    "\n",
    "</div>\n",
    "\n",
    "### Deep Gaussian Processes\n",
    "\n",
    "### Approximations\n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/sparse-gps-1.png\" width=\"90%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "<center>\n",
    "*Image credit: Kai Arulkumaran *\n",
    "</center>\n",
    "### Approximations\n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/sparse-gps-2.png\" width=\"90%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "<center>\n",
    "*Image credit: Kai Arulkumaran *\n",
    "</center>\n",
    "### Approximations\n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/sparse-gps-3.png\" width=\"45%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "<center>\n",
    "*Image credit: Kai Arulkumaran *\n",
    "</center>\n",
    "### Approximations\n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/sparse-gps-4.png\" width=\"45%\" align=\"center\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "<center>\n",
    "*Image credit: Kai Arulkumaran *\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import GPy\n",
    "\n",
    "import mlai\n",
    "import teaching_plots as plot \n",
    "from gp_tutorial import gpplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise_var = 0.01\n",
    "X = np.zeros((50, 1))\n",
    "X[:25, :] = np.linspace(0,3,25)[:,None] # First cluster of inputs/covariates\n",
    "X[25:, :] = np.linspace(7,10,25)[:,None] # Second cluster of inputs/covariates\n",
    "\n",
    "# Sample response variables from a Gaussian process with exponentiated quadratic covariance.\n",
    "k = GPy.kern.RBF(1)\n",
    "y = np.random.multivariate_normal(np.zeros(N),k.K(X)+np.eye(N)*np.sqrt(noise_var)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(X,y)\n",
    "_ = m_full.optimize(messages=True) # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2)\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/sparse-demo-full-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Gaussian Process Fit\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/sparse-demo-full-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(1)\n",
    "Z = np.hstack(\n",
    "        (np.linspace(2.5,4.,3),\n",
    "        np.linspace(7,8.5,3)))[:,None]\n",
    "m = GPy.models.SparseGPRegression(X,y,kernel=kern,Z=Z)\n",
    "m.noise_var = noise_var\n",
    "m.inducing_inputs.constrain_fixed()\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inducing Variable Fit\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m.optimize(messages=True)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/sparse-demo-full-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inducing Variable Param Optimize\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.randomize()\n",
    "m.inducing_inputs.unconstrain()\n",
    "_ = m.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2,xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inducing Variable Full Optimize\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.num_inducing=8\n",
    "m.randomize()\n",
    "M = 8\n",
    "m.set_Z(np.random.rand(M,1)*12)\n",
    "\n",
    "_ = m.optimize(messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, ax=ax, xlabel='$x$', ylabel='$y$', fontsize=20, portion=0.2, xlim=xlim, ylim=ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eight Optimized Inducing Variables\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg\" align=\"\">\n",
    "\n",
    "### Full Gaussian Process Fit\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/sparse-demo-full-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.log_likelihood(), m_full.log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Review\n",
    "\n",
    "-   *A Unifying Framework for Gaussian Process Pseudo-Point\n",
    "    Approximations using Power Expectation Propagation*\n",
    "    @Thang:unifying17\n",
    "\n",
    "-   *Deep Gaussian Processes and Variational Propagation of Uncertainty*\n",
    "    @Damianou:thesis2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.deep_nn(diagrams='../slides/diagrams/deepgp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-nn1.svg\" align=\"\">\n",
    "\n",
    "### Deep Neural Network\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-nn2.svg\" align=\"\">\n",
    "\n",
    "### Mathematically\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hiddenVector_{1} &= \\basisFunction\\left(\\mappingMatrix_1 \\inputVector\\right)\\\\\n",
    "    \\hiddenVector_{2} &=  \\basisFunction\\left(\\mappingMatrix_2\\hiddenVector_{1}\\right)\\\\\n",
    "    \\hiddenVector_{3} &= \\basisFunction\\left(\\mappingMatrix_3 \\hiddenVector_{2}\\right)\\\\\n",
    "    \\dataVector &= \\mappingVector_4 ^\\top\\hiddenVector_{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "-   Potential problem: if number of nodes in two adjacent layers is big,\n",
    "    corresponding $\\mappingMatrix$ is also very big and there is the\n",
    "    potential to overfit.\n",
    "\n",
    "-   Proposed solution: dropout.\n",
    "\n",
    "-   Alternative solution: parameterize $\\mappingMatrix$ with its SVD. $$\n",
    "      \\mappingMatrix = \\eigenvectorMatrix\\eigenvalueMatrix\\eigenvectwoMatrix^\\top\n",
    "      $$ or $$\n",
    "      \\mappingMatrix = \\eigenvectorMatrix\\eigenvectwoMatrix^\\top\n",
    "      $$ where if $\\mappingMatrix \\in \\Re^{k_1\\times k_2}$ then\n",
    "    $\\eigenvectorMatrix\\in \\Re^{k_1\\times q}$ and\n",
    "    $\\eigenvectwoMatrix \\in \\Re^{k_2\\times q}$, i.e. we have a low rank\n",
    "    matrix factorization for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.low_rank_approximation(diagrams='../slides/diagrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Rank Approximation\n",
    "\n",
    "<img src=\"../slides/diagrams/wisuvt.svg\" align=\"\">\n",
    "<center>\n",
    "*Pictorial representation of the low rank form of the matrix\n",
    "$\\mappingMatrix$ *\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.deep_nn_bottleneck(diagrams='../slides/diagrams/deepgp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-nn-bottleneck1.svg\" align=\"\">\n",
    "\n",
    "### Deep Neural Network\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-nn-bottleneck2.svg\" align=\"\">\n",
    "\n",
    "### Mathematically\n",
    "\n",
    "The network can now be written mathematically as $$\n",
    "\\begin{align}\n",
    "  \\latentVector_{1} &= \\eigenvectwoMatrix^\\top_1 \\inputVector\\\\\n",
    "  \\hiddenVector_{1} &= \\basisFunction\\left(\\eigenvectorMatrix_1 \\latentVector_{1}\\right)\\\\\n",
    "  \\latentVector_{2} &= \\eigenvectwoMatrix^\\top_2 \\hiddenVector_{1}\\\\\n",
    "  \\hiddenVector_{2} &= \\basisFunction\\left(\\eigenvectorMatrix_2 \\latentVector_{2}\\right)\\\\\n",
    "  \\latentVector_{3} &= \\eigenvectwoMatrix^\\top_3 \\hiddenVector_{2}\\\\\n",
    "  \\hiddenVector_{3} &= \\basisFunction\\left(\\eigenvectorMatrix_3 \\latentVector_{3}\\right)\\\\\n",
    "  \\dataVector &= \\mappingVector_4^\\top\\hiddenVector_{3}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### A Cascade of Neural Networks\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\latentVector_{1} &= \\eigenvectwoMatrix^\\top_1 \\inputVector\\\\\n",
    "  \\latentVector_{2} &= \\eigenvectwoMatrix^\\top_2 \\basisFunction\\left(\\eigenvectorMatrix_1 \\latentVector_{1}\\right)\\\\\n",
    "  \\latentVector_{3} &= \\eigenvectwoMatrix^\\top_3 \\basisFunction\\left(\\eigenvectorMatrix_2 \\latentVector_{2}\\right)\\\\\n",
    "  \\dataVector &= \\mappingVector_4 ^\\top \\latentVector_{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Cascade of Gaussian Processes\n",
    "\n",
    "-   Replace each neural network with a Gaussian process $$\n",
    "    \\begin{align}\n",
    "      \\latentVector_{1} &= \\mappingFunctionVector_1\\left(\\inputVector\\right)\\\\\n",
    "      \\latentVector_{2} &= \\mappingFunctionVector_2\\left(\\latentVector_{1}\\right)\\\\\n",
    "      \\latentVector_{3} &= \\mappingFunctionVector_3\\left(\\latentVector_{2}\\right)\\\\\n",
    "      \\dataVector &= \\mappingFunctionVector_4\\left(\\latentVector_{3}\\right)\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "-   Equivalent to prior over parameters, take width of each layer to\n",
    "    infinity.\n",
    "\n",
    "### Mathematically\n",
    "\n",
    "-   Composite *multivariate* function\n",
    "\n",
    "$$\n",
    "  \\mathbf{g}(\\inputVector)=\\mappingFunctionVector_5(\\mappingFunctionVector_4(\\mappingFunctionVector_3(\\mappingFunctionVector_2(\\mappingFunctionVector_1(\\inputVector))))).\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica'],'size':30})\n",
    "rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = plot.horizontal_chain(depth=5)\n",
    "pgm.render().figure.savefig(\"../slides/diagrams/deepgp/deep-markov.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent to Markov Chain\n",
    "\n",
    "-   Composite *multivariate* function $$\n",
    "      p(\\dataVector|\\inputVector)= p(\\dataVector|\\mappingFunctionVector_5)p(\\mappingFunctionVector_5|\\mappingFunctionVector_4)p(\\mappingFunctionVector_4|\\mappingFunctionVector_3)p(\\mappingFunctionVector_3|\\mappingFunctionVector_2)p(\\mappingFunctionVector_2|\\mappingFunctionVector_1)p(\\mappingFunctionVector_1|\\inputVector)\n",
    "      $$\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-markov.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica'], 'size':15})\n",
    "rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = plot.vertical_chain(depth=5)\n",
    "pgm.render().figure.savefig(\"../slides/diagrams/deepgp/deep-markov-vertical.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-markov-vertical.svg\" align=\"\">\n",
    "\n",
    "### Why Deep?\n",
    "\n",
    "-   Gaussian processes give priors over functions.\n",
    "\n",
    "-   Elegant properties:\n",
    "-   e.g. *Derivatives* of process are also Gaussian distributed (if they\n",
    "    exist).\n",
    "\n",
    "-   For particular covariance functions they are universal\n",
    "    approximators, i.e. all functions can have support under the prior.\n",
    "\n",
    "-   Gaussian derivatives might ring alarm bells.\n",
    "\n",
    "-   E.g. a priori they dont believe in function jumps.\n",
    "\n",
    "### Stochastic Process Composition\n",
    "\n",
    "-   From a process perspective: *process composition*.\n",
    "\n",
    "-   A (new?) way of constructing more complex *processes* based on\n",
    "    simpler components.\n",
    "\n",
    "### \n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-markov-vertical.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm = plot.vertical_chain(depth=5, shape=[2, 7])\n",
    "pgm.add_node(daft.Node('y_2', r'$\\mathbf{y}_2$', 1.5, 3.5, observed=True))\n",
    "pgm.add_edge('f_2', 'y_2')\n",
    "pgm.render().figure.savefig(\"../slides/diagrams/deepgp/deep-markov-vertical-side.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/deep-markov-vertical-side.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_3(diagrams='../../slides/diagrams/dimred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty for Probabilistic Approaches {#difficulty-for-probabilistic-approaches data-transition=\"None\"}\n",
    "\n",
    "-   Propagate a probability distribution through a non-linear mapping.\n",
    "\n",
    "-   Normalisation of distribution becomes intractable.\n",
    "\n",
    "<img src=\"../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_2(diagrams='../../slides/diagrams/dimred/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty for Probabilistic Approaches {#difficulty-for-probabilistic-approaches-1 data-transition=\"None\"}\n",
    "\n",
    "-   Propagate a probability distribution through a non-linear mapping.\n",
    "\n",
    "-   Normalisation of distribution becomes intractable.\n",
    "\n",
    "<img src=\"../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.non_linear_difficulty_plot_1(diagrams='../../slides/diagrams/dimred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty for Probabilistic Approaches {#difficulty-for-probabilistic-approaches-2 data-transition=\"None\"}\n",
    "\n",
    "-   Propagate a probability distribution through a non-linear mapping.\n",
    "\n",
    "-   Normalisation of distribution becomes intractable.\n",
    "\n",
    "<img src=\"../slides/diagrams/dimred/gaussian-through-nonlinear.svg\" align=\"center\">\n",
    "\n",
    "### Deep Gaussian Processes\n",
    "\n",
    "-   Deep architectures allow abstraction of features\n",
    "    [@Bengio:deep09; @Hinton:fast06; @Salakhutdinov:quantitative08]\n",
    "\n",
    "-   We use variational approach to stack GP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.stack_gp_sample(kernel=GPy.kern.Linear,\n",
    "                     diagrams=\"../../slides/diagrams/deepgp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('stack-gp-sample-Linear-{sample:0>1}.svg', \n",
    "                            directory='../../slides/diagrams/deepgp', sample=(0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked PCA\n",
    "\n",
    "<img src=\"../slides/diagrams/stack-pca-sample-4.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.stack_gp_sample(kernel=GPy.kern.RBF,\n",
    "                     diagrams=\"../../slides/diagrams/deepgp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('stack-gp-sample-RBF-{sample:0>1}.svg', \n",
    "                            directory='../../slides/diagrams/deepgp', sample=(0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked GP\n",
    "\n",
    "<img src=\"../slides/diagrams/stack-gp-sample-4.svg\" align=\"\">\n",
    "\n",
    "### Analysis of Deep GPs\n",
    "\n",
    "-   *Avoiding pathologies in very deep networks* @Duvenaud:pathologies14\n",
    "    show that the derivative distribution of the process becomes more\n",
    "    *heavy tailed* as number of layers increase.\n",
    "\n",
    "-   *How Deep Are Deep Gaussian Processes?* @Dunlop:deep2017 perform a\n",
    "    theoretical analysis possible through conditional Gaussian Markov\n",
    "    property.\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('XhIvygQYFFQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pods\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "\n",
    "xlim = (1875,2030)\n",
    "ylim = (2.5, 6.5)\n",
    "yhat = (y-offset)/scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='../slides/diagrams/datasets/olympic-marathon.svg', transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"70%\">\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didnt have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "</td>\n",
    "<td width=\"30%\">\n",
    "![image](../slides/diagrams/Stephen_Kiprotich.jpg) <small>Image from\n",
    "Wikimedia Commons <http://bit.ly/16kMKHQ></small>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "### Olympic Marathon Data\n",
    "\n",
    "<img src=\"../slides/diagrams/datasets/olympic-marathon.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(1870,2030,200)[:,np.newaxis]\n",
    "yt_mean, yt_var = m_full.predict(xt)\n",
    "yt_sd=np.sqrt(yt_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig,\n",
    "                  filename='../slides/diagrams/gp/olympic-marathon-gp.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data GP\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/olympic-marathon-gp.svg\" align=\"\">\n",
    "\n",
    "### \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td width=\"40%\">\n",
    "<img class=\"\" src=\"../slides/diagrams/turing-run.jpg\" width=\"40%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "</td>\n",
    "<td width=\"50%\">\n",
    "<img class=\"\" src=\"../slides/diagrams/turing-times.gif\" width=\"50%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>\n",
    "*Alan Turing, in 1946 he was only 11 minutes slower than the winner of\n",
    "the 1948 games. Would he have won a hypothetical games held in 1946?\n",
    "Source: [Alan Turing Internet\n",
    "Scrapbook](http://www.turing.org.uk/scrapbook/run.html) *\n",
    "</center>\n",
    "### Deep GP Fit\n",
    "\n",
    "-   Can a Deep Gaussian process help?\n",
    "\n",
    "-   Deep GP is one GP feeding into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 1\n",
    "m = deepgp.DeepGP([y.shape[1],hidden,x.shape[1]],Y=yhat, X=x, inits=['PCA','PCA'], \n",
    "                  kernels=[GPy.kern.RBF(hidden,ARD=True),\n",
    "                           GPy.kern.RBF(x.shape[1],ARD=True)], # the kernels for each layer\n",
    "                  num_inducing=50, back_constraint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(self, noise_factor=0.01, linear_factor=1):\n",
    "    \"\"\"Helper function for deep model initialization.\"\"\"\n",
    "    self.obslayer.likelihood.variance = self.Y.var()*noise_factor\n",
    "    for layer in self.layers:\n",
    "        if type(layer.X) is GPy.core.parameterization.variational.NormalPosterior:\n",
    "            if layer.kern.ARD:\n",
    "                var = layer.X.mean.var(0)\n",
    "            else:\n",
    "                var = layer.X.mean.var()\n",
    "        else:\n",
    "            if layer.kern.ARD:\n",
    "                var = layer.X.var(0)\n",
    "            else:\n",
    "                var = layer.X.var()\n",
    "\n",
    "        # Average 0.5 upcrossings in four standard deviations. \n",
    "        layer.kern.lengthscale = linear_factor*np.sqrt(layer.kern.input_dim)*2*4*np.sqrt(var)/(2*np.pi)\n",
    "# Bind the new method to the Deep GP object.\n",
    "deepgp.DeepGP.initialize=initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the initalization\n",
    "m.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staged_optimize(self, iters=(1000,1000,10000), messages=(False, False, True)):\n",
    "    \"\"\"Optimize with parameters constrained and then with parameters released\"\"\"\n",
    "    for layer in self.layers:\n",
    "        # Fix the scale of each of the covariance functions.\n",
    "        layer.kern.variance.fix(warning=False)\n",
    "        layer.kern.lengthscale.fix(warning=False)\n",
    "\n",
    "        # Fix the variance of the noise in each layer.\n",
    "        layer.likelihood.variance.fix(warning=False)\n",
    "\n",
    "    self.optimize(messages=messages[0],max_iters=iters[0])\n",
    "    \n",
    "    for layer in self.layers:\n",
    "        layer.kern.lengthscale.constrain_positive(warning=False)\n",
    "    self.obslayer.kern.variance.constrain_positive(warning=False)\n",
    "\n",
    "\n",
    "    self.optimize(messages=messages[1],max_iters=iters[1])\n",
    "\n",
    "    for layer in self.layers:\n",
    "        layer.kern.variance.constrain_positive(warning=False)\n",
    "        layer.likelihood.variance.constrain_positive(warning=False)\n",
    "    self.optimize(messages=messages[2],max_iters=iters[2])\n",
    "    \n",
    "# Bind the new method to the Deep GP object.\n",
    "deepgp.DeepGP.staged_optimize=staged_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.staged_optimize(messages=(True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_output(m, scale=scale, offset=offset, ax=ax, xlabel='year', ylabel='pace min/km', \n",
    "          fontsize=20, portion=0.2)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig, filename='../slides/diagrams/deepgp/olympic-marathon-deep-gp.svg', \n",
    "                transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data Deep GP\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/olympic-marathon-deep-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_sample(self, X, **kwargs):\n",
    "    \"\"\"Give a sample from the posterior of the deep GP.\"\"\"\n",
    "    Z = X\n",
    "    for i, layer in enumerate(reversed(self.layers)):\n",
    "        Z = layer.posterior_samples(Z, size=1, **kwargs)[:, :, 0]\n",
    " \n",
    "    return Z\n",
    "deepgp.DeepGP.posterior_sample = posterior_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot.model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, \n",
    "                  xlabel='year', ylabel='pace min/km', portion = 0.225)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(figure=fig, filename='../slides/diagrams/deepgp/olympic-marathon-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data Deep GP {#olympic-marathon-data-deep-gp-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/olympic-marathon-deep-gp-samples.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(self, scale=1.0, offset=0.0, xlabel='input', ylabel='output', \n",
    "              xlim=None, ylim=None, fontsize=20, portion=0.2,dataset=None, \n",
    "              diagrams='../diagrams'):\n",
    "    \"\"\"Visualize the layers in a deep GP with one-d input and output.\"\"\"\n",
    "    depth = len(self.layers)\n",
    "    if dataset is None:\n",
    "        fname = 'deep-gp-layer'\n",
    "    else:\n",
    "        fname = dataset + '-deep-gp-layer'\n",
    "    filename = os.path.join(diagrams, fname)\n",
    "    last_name = xlabel\n",
    "    last_x = self.X\n",
    "    for i, layer in enumerate(reversed(self.layers)):\n",
    "        if i>0:\n",
    "            plt.plot(last_x, layer.X.mean, 'r.',markersize=10)\n",
    "            last_x=layer.X.mean\n",
    "            ax=plt.gca()\n",
    "            name = 'layer ' + str(i)\n",
    "            plt.xlabel(last_name, fontsize=fontsize)\n",
    "            plt.ylabel(name, fontsize=fontsize)\n",
    "            last_name=name\n",
    "            mlai.write_figure(filename=filename + '-' + str(i-1) + '.svg', \n",
    "                              transparent=True, frameon=True)\n",
    "            \n",
    "        if i==0 and xlim is not None:\n",
    "            xt = plot.pred_range(np.array(xlim), portion=0.0)\n",
    "        elif i>0:\n",
    "            xt = plot.pred_range(np.array(next_lim), portion=0.0)\n",
    "        else:\n",
    "            xt = plot.pred_range(last_x, portion=portion)\n",
    "        yt_mean, yt_var = layer.predict(xt)\n",
    "        if layer==self.obslayer:\n",
    "            yt_mean = yt_mean*scale + offset\n",
    "            yt_var *= scale*scale\n",
    "        yt_sd = np.sqrt(yt_var)\n",
    "        gpplot(xt,yt_mean,yt_mean-2*yt_sd,yt_mean+2*yt_sd)\n",
    "        ax = plt.gca()\n",
    "        if i>0:\n",
    "            ax.set_xlim(next_lim)\n",
    "        elif xlim is not None:\n",
    "            ax.set_xlim(xlim)\n",
    "        next_lim = plt.gca().get_ylim()\n",
    "        \n",
    "    plt.plot(last_x, self.Y*scale + offset, 'r.',markersize=10)\n",
    "    plt.xlabel(last_name, fontsize=fontsize)\n",
    "    plt.ylabel(ylabel, fontsize=fontsize)\n",
    "    mlai.write_figure(filename=filename + '-' + str(i) + '.svg', \n",
    "                      transparent=True, frameon=True)\n",
    "\n",
    "    if ylim is not None:\n",
    "        ax=plt.gca()\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "# Bind the new method to the Deep GP object.\n",
    "deepgp.DeepGP.visualize=visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(scale=scale, offset=offset, xlabel='year',\n",
    "            ylabel='pace min/km',xlim=xlim, ylim=ylim,\n",
    "            dataset='olympic-marathon',\n",
    "            diagrams='../slides/diagrams/deepgp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "pods.notebook.display_plots('olympic-marathon-deep-gp-layer-{sample:0>1}.svg', \n",
    "                            '../slides/diagrams/deepgp', sample=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Data Latent 1 {#olympic-marathon-data-latent-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-0.svg\" align=\"\">\n",
    "\n",
    "### Olympic Marathon Data Latent 2 {#olympic-marathon-data-latent-2 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-1.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(x, portion):     \n",
    "    scale = (x.max()-x.min())/(1-2*portion)\n",
    "    offset = x.min() - portion*scale\n",
    "    return (x-offset)/scale, scale, offset\n",
    "\n",
    "def visualize_pinball(self, ax=None, scale=1.0, offset=0.0, xlabel='input', ylabel='output', \n",
    "                  xlim=None, ylim=None, fontsize=20, portion=0.2, points=50, vertical=True):\n",
    "    \"\"\"Visualize the layers in a deep GP with one-d input and output.\"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "    depth = len(self.layers)\n",
    "\n",
    "    last_name = xlabel\n",
    "    last_x = self.X\n",
    "\n",
    "    # Recover input and output scales from output plot\n",
    "    plot_model_output(self, scale=scale, offset=offset, ax=ax, \n",
    "                      xlabel=xlabel, ylabel=ylabel, \n",
    "                      fontsize=fontsize, portion=portion)\n",
    "    xlim=ax.get_xlim()\n",
    "    xticks=ax.get_xticks()\n",
    "    xtick_labels=ax.get_xticklabels().copy()\n",
    "    ylim=ax.get_ylim()\n",
    "    yticks=ax.get_yticks()\n",
    "    ytick_labels=ax.get_yticklabels().copy()\n",
    "\n",
    "    # Clear axes and start again\n",
    "    ax.cla()\n",
    "    if vertical:\n",
    "        ax.set_xlim((0, 1))\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        ax.set_ylim((depth, 0))\n",
    "    else:\n",
    "        ax.set_ylim((0, 1))\n",
    "        ax.set_xlim((0, depth))\n",
    "        \n",
    "    ax.set_axis_off()#frame_on(False)\n",
    "\n",
    "\n",
    "    def pinball(x, y, y_std, color_scale=None, \n",
    "                layer=0, depth=1, ax=None, \n",
    "                alpha=1.0, portion=0.0, vertical=True):  \n",
    "\n",
    "        scaledx, xscale, xoffset = scale_data(x, portion=portion)\n",
    "        scaledy, yscale, yoffset = scale_data(y, portion=portion)\n",
    "        y_std /= yscale\n",
    "\n",
    "        # Check whether data is anti-correlated on output\n",
    "        if np.dot((scaledx-0.5).T, (scaledy-0.5))<0:\n",
    "            scaledy=1-scaledy\n",
    "            flip=-1\n",
    "        else:\n",
    "            flip=1\n",
    "\n",
    "        if color_scale is not None:\n",
    "            color_scale, _, _=scale_data(color_scale, portion=0)\n",
    "        scaledy = (1-alpha)*scaledx + alpha*scaledy\n",
    "\n",
    "        def color_value(x, cmap=None, width=None, centers=None):\n",
    "            \"\"\"Return color as a function of position along x axis\"\"\"\n",
    "            if cmap is None:\n",
    "                cmap = np.asarray([[1, 0, 0], [1, 1, 0], [0, 1, 0]])\n",
    "            ncenters = cmap.shape[0]\n",
    "            if centers is None:\n",
    "                centers = np.linspace(0+0.5/ncenters, 1-0.5/ncenters, ncenters)\n",
    "            if width is None:\n",
    "                width = 0.25/ncenters\n",
    "            \n",
    "            r = (x-centers)/width\n",
    "            weights = np.exp(-0.5*r*r).flatten()\n",
    "            weights /=weights.sum()\n",
    "            weights = weights[:, np.newaxis]\n",
    "            return np.dot(cmap.T, weights).flatten()\n",
    "\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            if color_scale is not None:\n",
    "                color = color_value(color_scale[i])\n",
    "            else:\n",
    "                color=(1, 0, 0)\n",
    "            x_plot = np.asarray((scaledx[i], scaledy[i])).flatten()\n",
    "            y_plot = np.asarray((layer, layer+alpha)).flatten()\n",
    "            if vertical:\n",
    "                ax.plot(x_plot, y_plot, color=color, alpha=0.5, linewidth=3)\n",
    "                ax.plot(x_plot, y_plot, color='k', alpha=0.5, linewidth=0.5)\n",
    "            else:\n",
    "                ax.plot(y_plot, x_plot, color=color, alpha=0.5, linewidth=3)\n",
    "                ax.plot(y_plot, x_plot, color='k', alpha=0.5, linewidth=0.5)\n",
    "\n",
    "            # Plot error bars that increase as sqrt of distance from start.\n",
    "            std_points = 50\n",
    "            stdy = np.linspace(0, alpha,std_points)\n",
    "            stdx = np.sqrt(stdy)*y_std[i]\n",
    "            stdy += layer\n",
    "            mean_vals = np.linspace(scaledx[i], scaledy[i], std_points)\n",
    "            upper = mean_vals+stdx \n",
    "            lower = mean_vals-stdx \n",
    "            fillcolor=color\n",
    "            x_errorbars=np.hstack((upper,lower[::-1]))\n",
    "            y_errorbars=np.hstack((stdy,stdy[::-1]))\n",
    "            if vertical:\n",
    "                ax.fill(x_errorbars,y_errorbars,\n",
    "                        color=fillcolor, alpha=0.1)\n",
    "                ax.plot(scaledy[i], layer+alpha, '.',markersize=10, color=color, alpha=0.5)\n",
    "            else:\n",
    "                ax.fill(y_errorbars,x_errorbars,\n",
    "                        color=fillcolor, alpha=0.1)\n",
    "                ax.plot(layer+alpha, scaledy[i], '.',markersize=10, color=color, alpha=0.5)\n",
    "            # Marker to show end point\n",
    "        return flip\n",
    "\n",
    "\n",
    "    # Whether final axis is flipped\n",
    "    flip = 1\n",
    "    first_x=last_x\n",
    "    for i, layer in enumerate(reversed(self.layers)):     \n",
    "        if i==0:\n",
    "            xt = plot.pred_range(last_x, portion=portion, points=points)\n",
    "            color_scale=xt\n",
    "        yt_mean, yt_var = layer.predict(xt)\n",
    "        if layer==self.obslayer:\n",
    "            yt_mean = yt_mean*scale + offset\n",
    "            yt_var *= scale*scale\n",
    "        yt_sd = np.sqrt(yt_var)\n",
    "        flip = flip*pinball(xt,yt_mean,yt_sd,color_scale,portion=portion, \n",
    "                            layer=i, ax=ax, depth=depth,vertical=vertical)#yt_mean-2*yt_sd,yt_mean+2*yt_sd)\n",
    "        xt = yt_mean\n",
    "    # Make room for axis labels\n",
    "    if vertical:\n",
    "        ax.set_ylim((2.1, -0.1))\n",
    "        ax.set_xlim((-0.02, 1.02))\n",
    "        ax.set_yticks(range(depth,0,-1))\n",
    "    else:\n",
    "        ax.set_xlim((-0.1, 2.1))\n",
    "        ax.set_ylim((-0.02, 1.02))\n",
    "        ax.set_xticks(range(0, depth))\n",
    "        \n",
    "    def draw_axis(ax, scale=1.0, offset=0.0, level=0.0, flip=1, \n",
    "                  label=None,up=False, nticks=10, ticklength=0.05,\n",
    "                  vertical=True,\n",
    "                 fontsize=20):\n",
    "        def clean_gap(gap):\n",
    "            nsf = np.log10(gap)\n",
    "            if nsf>0:\n",
    "                nsf = np.ceil(nsf)\n",
    "            else:\n",
    "                nsf = np.floor(nsf)\n",
    "            lower_gaps = np.asarray([0.005, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                                     0.1, 0.25, 0.5, \n",
    "                                     1, 1.5, 2, 2.5, 3, 4, 5, 10, 25, 50, 100])\n",
    "            upper_gaps = np.asarray([1, 2, 3, 4, 5, 10])\n",
    "            if nsf >2 or nsf<-2:\n",
    "                d = np.abs(gap-upper_gaps*10**nsf)\n",
    "                ind = np.argmin(d)\n",
    "                return upper_gaps[ind]*10**nsf\n",
    "            else:\n",
    "                d = np.abs(gap-lower_gaps)\n",
    "                ind = np.argmin(d)\n",
    "                return lower_gaps[ind]\n",
    "            \n",
    "        tickgap = clean_gap(scale/(nticks-1))\n",
    "        nticks = int(scale/tickgap) + 1\n",
    "        tickstart = np.round(offset/tickgap)*tickgap\n",
    "        ticklabels = np.asarray(range(0, nticks))*tickgap + tickstart\n",
    "        ticks = (ticklabels-offset)/scale\n",
    "        axargs = {'color':'k', 'linewidth':1}\n",
    "        \n",
    "        if not up:\n",
    "            ticklength=-ticklength\n",
    "        tickspot = np.linspace(0, 1, nticks)\n",
    "        if flip < 0:\n",
    "            ticks = 1-ticks\n",
    "        for tick, ticklabel in zip(ticks, ticklabels):\n",
    "            if vertical:\n",
    "                ax.plot([tick, tick], [level, level-ticklength], **axargs)\n",
    "                ax.text(tick, level-ticklength*2, ticklabel, horizontalalignment='center', \n",
    "                        fontsize=fontsize/2)\n",
    "                ax.text(0.5, level-5*ticklength, label, horizontalalignment='center', fontsize=fontsize)\n",
    "            else:\n",
    "                ax.plot([level, level-ticklength], [tick, tick],  **axargs)\n",
    "                ax.text(level-ticklength*2, tick, ticklabel, horizontalalignment='center', \n",
    "                        fontsize=fontsize/2)\n",
    "                ax.text(level-5*ticklength, 0.5, label, horizontalalignment='center', fontsize=fontsize)\n",
    "        \n",
    "        if vertical:\n",
    "            xlim = list(ax.get_xlim())\n",
    "            if ticks.min()<xlim[0]:\n",
    "                xlim[0] = ticks.min()\n",
    "            if ticks.max()>xlim[1]:\n",
    "                xlim[1] = ticks.max()\n",
    "            ax.set_xlim(xlim)\n",
    "            \n",
    "            ax.plot([ticks.min(), ticks.max()], [level, level], **axargs)\n",
    "        else:\n",
    "            ylim = list(ax.get_ylim())\n",
    "            if ticks.min()<ylim[0]:\n",
    "                ylim[0] = ticks.min()\n",
    "            if ticks.max()>ylim[1]:\n",
    "                ylim[1] = ticks.max()\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.plot([level, level], [ticks.min(), ticks.max()], **axargs)\n",
    "\n",
    "\n",
    "    _, xscale, xoffset = scale_data(first_x, portion=portion)\n",
    "    _, yscale, yoffset = scale_data(yt_mean, portion=portion)\n",
    "    draw_axis(ax=ax, scale=xscale, offset=xoffset, level=0.0, label=xlabel, \n",
    "              up=True, vertical=vertical)\n",
    "    draw_axis(ax=ax, scale=yscale, offset=yoffset, \n",
    "              flip=flip, level=depth, label=ylabel, up=False, vertical=vertical)\n",
    "    \n",
    "    #for txt in xticklabels:\n",
    "    #    txt.set\n",
    "# Bind the new method to the Deep GP object.\n",
    "deepgp.DeepGP.visualize_pinball=visualize_pinball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(ax=ax, scale=scale, offset=offset, points=30, portion=0.1,\n",
    "                    xlabel='year', ylabel='pace km/min', vertical=True)\n",
    "mlai.write_figure(figure=fig, filename='../slides/diagrams/deepgp/olympic-marathon-deep-gp-pinball.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olympic Marathon Pinball Plot\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/olympic-marathon-deep-gp-pinball.svg\" align=\"\">\n",
    "\n",
    "### Step Function\n",
    "\n",
    "Next we consider a simple step function data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_low=25\n",
    "num_high=25\n",
    "gap = -.1\n",
    "noise=0.0001\n",
    "x = np.vstack((np.linspace(-1, -gap/2.0, num_low)[:, np.newaxis],\n",
    "              np.linspace(gap/2.0, 1, num_high)[:, np.newaxis]))\n",
    "y = np.vstack((np.zeros((num_low, 1)), np.ones((num_high,1))))\n",
    "scale = np.sqrt(y.var())\n",
    "offset = y.mean()\n",
    "yhat = (y-offset)/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "_ = ax.set_xlabel('$x$', fontsize=20)\n",
    "_ = ax.set_ylabel('$y$', fontsize=20)\n",
    "xlim = (-2, 2)\n",
    "ylim = (-0.6, 1.6)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/datasets/step-function.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function Data {#step-function-data data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/datasets/step-function.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_output(m_full, scale=scale, offset=offset, ax=ax, fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "mlai.write_figure(figure=fig,filename='../../slides/diagrams/gp/step-function-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function Data GP {#step-function-data-gp data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/step-function-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 1, 1, 1,x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i)]\n",
    "m = deepgp.DeepGP(layers,Y=yhat, X=x, \n",
    "                  inits=inits, \n",
    "                  kernels=kernels, # the kernels for each layer\n",
    "                  num_inducing=20, back_constraint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.initialize()\n",
    "m.staged_optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_output(m, scale=scale, offset=offset, ax=ax, fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(filename='../../slides/diagrams/deepgp/step-function-deep-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function Data Deep GP {#step-function-data-deep-gp data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, portion = 0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/step-function-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function Data Deep GP {#step-function-data-deep-gp-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp-samples.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(offset=offset, scale=scale, xlim=xlim, ylim=ylim,\n",
    "            dataset='step-function',\n",
    "            diagrams='../../slides/diagrams/deepgp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function Data Latent 1 {#step-function-data-latent-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp-layer-0.svg\" align=\"\">\n",
    "\n",
    "### Step Function Data Latent 2 {#step-function-data-latent-2 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp-layer-1.svg\" align=\"\">\n",
    "\n",
    "### Step Function Data Latent 3 {#step-function-data-latent-3 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp-layer-2.svg\" align=\"\">\n",
    "\n",
    "### Step Function Data Latent 4 {#step-function-data-latent-4 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp-layer-3.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(offset=offset, ax=ax, scale=scale, xlim=xlim, ylim=ylim, portion=0.1, points=50)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/step-function-deep-gp-pinball.svg', \n",
    "                  transparent=True, frameon=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function Pinball Plot {#step-function-pinball-plot data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/step-function-deep-gp-pinball.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "data = pods.datasets.mcycle()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "scale=np.sqrt(y.var())\n",
    "offset=y.mean()\n",
    "yhat = (y - offset)/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x, y, 'r.',markersize=10)\n",
    "_ = ax.set_xlabel('time', fontsize=20)\n",
    "_ = ax.set_ylabel('acceleration', fontsize=20)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "mlai.write_figure(filename='../../slides/diagrams/datasets/motorcycle-helmet.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motorcycle Helmet Data {#motorcycle-helmet-data data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/datasets/motorcycle-helmet.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_output(m_full, scale=scale, offset=offset, ax=ax, xlabel='time', ylabel='acceleration/$g$', fontsize=20, portion=0.5)\n",
    "xlim=(-20,80)\n",
    "ylim=(-180,120)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig,filename='../../slides/diagrams/gp/motorcycle-helmet-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motorcycle Helmet Data GP {#motorcycle-helmet-data-gp data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/motorcycle-helmet-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 1, x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i)]\n",
    "m = deepgp.DeepGP(layers,Y=yhat, X=x, \n",
    "                  inits=inits, \n",
    "                  kernels=kernels, # the kernels for each layer\n",
    "                  num_inducing=20, back_constraint=False)\n",
    "\n",
    "\n",
    "\n",
    "m.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.staged_optimize(iters=(1000,1000,10000), messages=(True, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_output(m, scale=scale, offset=offset, ax=ax, xlabel='time', ylabel='acceleration/$g$', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(filename='../../slides/diagrams/deepgp/motorcycle-helmet-deep-gp.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motorcycle Helmet Data Deep GP {#motorcycle-helmet-data-deep-gp data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/motorcycle-helmet-deep-gp.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_sample(m, scale=scale, offset=offset, samps=10, ax=ax, xlabel='time', ylabel='acceleration/$g$', portion = 0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-samples.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motorcycle Helmet Data Deep GP {#motorcycle-helmet-data-deep-gp-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-samples.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize(xlim=xlim, ylim=ylim, scale=scale,offset=offset, \n",
    "            xlabel=\"time\", ylabel=\"acceleration/$g$\", portion=0.5,\n",
    "            dataset='motorcycle-helmet',\n",
    "            diagrams='../../slides/diagrams/deepgp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motorcycle Helmet Data Latent 1 {#motorcycle-helmet-data-latent-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-layer-0.svg\" align=\"\">\n",
    "\n",
    "### Motorcycle Helmet Data Latent 2 {#motorcycle-helmet-data-latent-2 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-layer-1.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "m.visualize_pinball(ax=ax, xlabel='time', ylabel='acceleration/g', \n",
    "                    points=50, scale=scale, offset=offset, portion=0.1)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-pinball.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motorcycle Helmet Pinball Plot {#motorcycle-helmet-pinball-plot data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-pinball.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pods.datasets.robot_wireless()\n",
    "\n",
    "x = np.linspace(0,1,215)[:, np.newaxis]\n",
    "y = data['Y']\n",
    "offset = y.mean()\n",
    "scale = np.sqrt(y.var())\n",
    "yhat = (y-offset)/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "plt.plot(data['X'][:, 1], data['X'][:, 2], 'r.', markersize=5)\n",
    "ax.set_xlabel('x position', fontsize=20)\n",
    "ax.set_ylabel('y position', fontsize=20)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/datasets/robot-wireless-ground-truth.svg', transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot Wireless Ground Truth {#robot-wireless-ground-truth data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/datasets/robot-wireless-ground-truth.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim=1\n",
    "xlim = (-0.3, 1.3)\n",
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "_ = ax.plot(x.flatten(), y[:, output_dim], \n",
    "            'r.', markersize=5)\n",
    "\n",
    "ax.set_xlabel('time', fontsize=20)\n",
    "ax.set_ylabel('signal strength', fontsize=20)\n",
    "xlim = (-0.2, 1.2)\n",
    "ylim = (-0.6, 2.0)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/datasets/robot-wireless-dim-' + str(output_dim) + '.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot WiFi Data {#robot-wifi-data data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/datasets/robot-wireless-dim-1.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_full = GPy.models.GPRegression(x,yhat)\n",
    "_ = m_full.optimize() # Optimize parameters of covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_output(m_full, output_dim=output_dim, scale=scale, offset=offset, ax=ax, \n",
    "                  xlabel='time', ylabel='signal strength', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(filename='../../slides/diagrams/gp/robot-wireless-gp-dim-' + str(output_dim)+ '.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot WiFi Data GP {#robot-wifi-data-gp data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/gp/robot-wireless-gp-dim-1.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [y.shape[1], 10, 5, 2, 2, x.shape[1]]\n",
    "inits = ['PCA']*(len(layers)-1)\n",
    "kernels = []\n",
    "for i in layers[1:]:\n",
    "    kernels += [GPy.kern.RBF(i, ARD=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = deepgp.DeepGP(layers,Y=y, X=x, inits=inits, \n",
    "                  kernels=kernels,\n",
    "                  num_inducing=50, back_constraint=False)\n",
    "m.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.staged_optimize(messages=(True,True,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_output(m, output_dim=output_dim, scale=scale, offset=offset, ax=ax, \n",
    "                  xlabel='time', ylabel='signal strength', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/robot-wireless-deep-gp-dim-' + str(output_dim)+ '.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot WiFi Data Deep GP {#robot-wifi-data-deep-gp data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/robot-wireless-deep-gp-dim-1.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=plot.big_wide_figsize)\n",
    "plot_model_sample(m, output_dim=output_dim, scale=scale, offset=offset, samps=10, ax=ax,\n",
    "                  xlabel='time', ylabel='signal strength', fontsize=20, portion=0.5)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlim(xlim)\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/robot-wireless-deep-gp-samples-dim-' + str(output_dim)+ '.svg', \n",
    "                  transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot WiFi Data Deep GP {#robot-wifi-data-deep-gp-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/robot-wireless-deep-gp-samples-dim-1.svg\" align=\"\">\n",
    "\n",
    "### Robot WiFi Data Latent Space {#robot-wifi-data-latent-space data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/robot-wireless-ground-truth.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "ax.plot(m.layers[-2].latent_space.mean[:, 0], \n",
    "        m.layers[-2].latent_space.mean[:, 1], \n",
    "        'r.-', markersize=5)\n",
    "\n",
    "ax.set_xlabel('latent dimension 1', fontsize=20)\n",
    "ax.set_ylabel('latent dimension 2', fontsize=20)\n",
    "\n",
    "mlai.write_figure(figure=fig, filename='../../slides/diagrams/deepgp/robot-wireless-latent-space.svg', \n",
    "            transparent=True, frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot WiFi Data Latent Space {#robot-wifi-data-latent-space-1 data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deepgp/robot-wireless-latent-space.svg\" align=\"\">\n",
    "\n",
    "### Motion Capture {#motion-capture data-transition=\"none\"}\n",
    "\n",
    "-   High five data.\n",
    "\n",
    "-   Model learns structure between two interacting subjects.\n",
    "\n",
    "### Shared LVM {#shared-lvm data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/shared.svg\" align=\"\">\n",
    "\n",
    "###  {#section-17 data-transition=\"none\"}\n",
    "\n",
    "<img class=\"negate\" src=\"../slides/diagrams/deep-gp-high-five2.png\" width=\"100%\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "\n",
    "\\credit{Zhenwen Dai and Neil D. Lawrence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from IPython.display import display\n",
    "\n",
    "import deepgp\n",
    "import GPy\n",
    "\n",
    "from gp_tutorial import ax_default, meanplot, gpplot\n",
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "digits = [0,1,2,3,4]\n",
    "N_per_digit = 100\n",
    "Y = []\n",
    "labels = []\n",
    "for d in digits:\n",
    "    imgs = mnist['data'][mnist['target']==d]\n",
    "    Y.append(imgs[np.random.permutation(imgs.shape[0])][:N_per_digit])\n",
    "    labels.append(np.ones(N_per_digit)*d)\n",
    "Y = np.vstack(Y).astype(np.float64)\n",
    "labels = np.hstack(labels)\n",
    "Y /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent = 2\n",
    "num_hidden_2 = 5\n",
    "m = deepgp.DeepGP([Y.shape[1],num_hidden_2,num_latent],\n",
    "                  Y,\n",
    "                  kernels=[GPy.kern.RBF(num_hidden_2,ARD=True), \n",
    "                           GPy.kern.RBF(num_latent,ARD=False)], \n",
    "                  num_inducing=50, back_constraint=False, \n",
    "                  encoder_dims=[[200],[200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.obslayer.likelihood.variance[:] = Y.var()*0.01\n",
    "for layer in m.layers:\n",
    "    layer.kern.variance.fix(warning=False)\n",
    "    layer.likelihood.variance.fix(warning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize(messages=False,max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in m.layers:\n",
    "    layer.kern.variance.constrain_positive(warning=False)\n",
    "m.optimize(messages=False,max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in m.layers:\n",
    "    layer.likelihood.variance.constrain_positive(warning=False)\n",
    "m.optimize(messages=True,max_iters=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc(\"font\", **{'family':'sans-serif','sans-serif':['Helvetica'],'size':20})\n",
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "for d in digits:\n",
    "    ax.plot(m.layer_1.X.mean[labels==d,0],m.layer_1.X.mean[labels==d,1],'.',label=str(d))\n",
    "_ = plt.legend()\n",
    "mlai.write_figure(figure=fig, filename=\"../../slides/diagrams/deepgp/usps-digits-latent.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  {#section-18 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/usps-digits-latent.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.obslayer.kern.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
    "for i in range(5):\n",
    "    for j in range(i):\n",
    "        dims=[i, j]\n",
    "        ax.cla()\n",
    "        for d in digits:\n",
    "            ax.plot(m.obslayer.X.mean[labels==d,dims[0]],\n",
    "                 m.obslayer.X.mean[labels==d,dims[1]],\n",
    "                 '.', label=str(d))\n",
    "        plt.legend()\n",
    "        plt.xlabel('dimension ' + str(dims[0]))\n",
    "        plt.ylabel('dimension ' + str(dims[1]))\n",
    "        mlai.write_figure(figure=fig, filename=\"../../slides/diagrams/deepgp/usps-digits-hidden-\" + str(dims[0]) + '-' + str(dims[1]) + '.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  {#section-19 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/usps-digits-hidden-1-0.svg\" align=\"\">\n",
    "\n",
    "###  {#section-20 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/usps-digits-hidden-2-0.svg\" align=\"\">\n",
    "\n",
    "###  {#section-21 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/usps-digits-hidden-3-0.svg\" align=\"\">\n",
    "\n",
    "###  {#section-22 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/usps-digits-hidden-4-0.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 10\n",
    "cols = 20\n",
    "t=np.linspace(-1, 1, rows*cols)[:, None]\n",
    "kern = GPy.kern.RBF(1,lengthscale=0.05)\n",
    "cov = kern.K(t, t)\n",
    "x = np.random.multivariate_normal(np.zeros(rows*cols), cov, num_latent).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = m.predict(x)\n",
    "fig, axs = plt.subplots(rows,cols,figsize=(10,6))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        #v = np.random.normal(loc=yt[0][i*cols+j, :], scale=np.sqrt(yt[1][i*cols+j, :]))\n",
    "        v = yt[0][i*cols+j, :]\n",
    "        axs[i,j].imshow(v.reshape(28,28), \n",
    "                        cmap='gray', interpolation='none',\n",
    "                        aspect='equal')\n",
    "        axs[i,j].set_axis_off()\n",
    "mlai.write_figure(figure=fig, filename=\"../../slides/diagrams/deepgp/digit-samples-deep-gp.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  {#section-23 data-transition=\"none\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/digit-samples-deep-gp.svg\" align=\"\">\n",
    "\n",
    "### Deep Health {#deep-health data-transition=\"None\"}\n",
    "\n",
    "<img src=\"../slides/diagrams/deep-health.svg\" align=\"center\">\n",
    "\n",
    "### At this Year's NIPS\n",
    "\n",
    "-   *Gaussian process based nonlinear latent structure discovery in\n",
    "    multivariate spike train data* @Anqi:gpspike2017\n",
    "-   *Doubly Stochastic Variational Inference for Deep Gaussian\n",
    "    Processes* @Salimbeni:doubly2017\n",
    "-   *Deep Multi-task Gaussian Processes for Survival Analysis with\n",
    "    Competing Risks* @Alaa:deep2017\n",
    "-   *Counterfactual Gaussian Processes for Reliable Decision-making and\n",
    "    What-if Reasoning* @Schulam:counterfactual17\n",
    "\n",
    "### Some Other Works\n",
    "\n",
    "-   *Deep Survival Analysis* @Ranganath-survival16\n",
    "-   *Recurrent Gaussian Processes* @Mattos:recurrent15\n",
    "-   *Gaussian Process Based Approaches for Survival Analysis*\n",
    "    @Saul:thesis2016\n",
    "\n",
    "### Uncertainty Quantification\n",
    "\n",
    "-   Deep nets are powerful approach to images, speech, language.\n",
    "\n",
    "-   Proposal: Deep GPs may also be a great approach, but better to\n",
    "    deploy according to natural strengths.\n",
    "\n",
    "### Uncertainty Quantification\n",
    "\n",
    "-   Probabilistic numerics, surrogate modelling, emulation, and UQ.\n",
    "\n",
    "-   Not a fan of AI as a term.\n",
    "\n",
    "-   But we are faced with increasing amounts of *algorithmic decision\n",
    "    making*.\n",
    "\n",
    "### ML and Decision Making\n",
    "\n",
    "-   When trading off decisions: compute or acquire data?\n",
    "\n",
    "-   There is a critical need for uncertainty.\n",
    "\n",
    "### Uncertainty Quantification\n",
    "\n",
    "> Uncertainty quantification (UQ) is the science of quantitative\n",
    "> characterization and reduction of uncertainties in both computational\n",
    "> and real world applications. It tries to determine how likely certain\n",
    "> outcomes are if some aspects of the system are not exactly known.\n",
    "\n",
    "-   Interaction between physical and virtual worlds of major interest\n",
    "    for Amazon.\n",
    "\n",
    "### Example: Formula One Racing\n",
    "\n",
    "-   Designing an F1 Car requires CFD, Wind Tunnel, Track Testing etc.\n",
    "\n",
    "-   How to combine them?\n",
    "\n",
    "### Mountain Car Simulator\n",
    "\n",
    "<img class=\"\" src=\"../slides/diagrams/uq/mountaincar.png\" width=\"negate\" align=\"\" style=\"background:none; border:none; box-shadow:none;\">\n",
    "\n",
    "### Car Dynamics\n",
    "\n",
    "$$\\inputVector_{t+1} = \\mappingFunction(\\inputVector_{t},\\textbf{u}_{t})$$\n",
    "\n",
    "where $\\textbf{u}_t$ is the action force, $\\inputVector_t = (p_t, v_t)$\n",
    "is the vehicle state\n",
    "\n",
    "### Policy\n",
    "\n",
    "-   Assume policy is linear with parameters $\\boldsymbol{\\theta}$\n",
    "\n",
    "$$\\pi(\\inputVector,\\theta)= \\theta_0 + \\theta_p p + \\theta_vv.$$\n",
    "\n",
    "### Emulate the Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Goal is find $\\theta$ such that\n",
    "\n",
    "$$\\theta^* = arg \\max_{\\theta} R_T(\\theta).$$\n",
    "\n",
    "-   Reward is computed as 100 for target, minus squared sum of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mountain_car as mc\n",
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func = lambda x: mc.run_simulation(env, x)[0]\n",
    "objective = GPyOpt.core.task.SingleObjective(obj_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- We define the input space of the emulator\n",
    "\n",
    "space= [{'name':'postion_parameter', 'type':'continuous', 'domain':(-1.2, +1)},\n",
    "        {'name':'velocity_parameter', 'type':'continuous', 'domain':(-1/0.07, +1/0.07)},\n",
    "        {'name':'constant', 'type':'continuous', 'domain':(-1, +1)}]\n",
    "\n",
    "design_space = GPyOpt.Design_space(space=space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_space)\n",
    "acquisition = GPyOpt.acquisitions.AcquisitionEI(model, design_space, optimizer=aquisition_optimizer)\n",
    "evaluator = GPyOpt.core.evaluators.Sequential(acquisition) # Collect points sequentially, no parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.experiment_design.random_design import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_initial_points = 25\n",
    "random_design = RandomDesign(design_space)\n",
    "initial_design = random_design.get_samples(n_initial_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_controller = initial_design[0,:]\n",
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(random_controller), render=True)\n",
    "anim=mc.animate_frames(frames, 'Random linear controller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.save_frames(frames, \n",
    "                  diagrams='../slides/diagrams/uq', \n",
    "                  filename='mountain_car_random.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Linear Controller\n",
    "\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_random.html\" width=\"1024\" height=\"768\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "bo = GPyOpt.methods.ModularBayesianOptimization(model, design_space, objective, acquisition, evaluator, initial_design)\n",
    "bo.run_optimization(max_iter = max_iter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(bo.x_opt), render=True)\n",
    "anim=mc.animate_frames(frames, 'Best controller after 50 iterations of Bayesian optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.save_frames(frames, \n",
    "                  diagrams='../slides/diagrams/uq', \n",
    "                  filename='mountain_car_simulated.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Controller after 50 Iterations of Bayesian Optimization\n",
    "\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_simulated.html\" width=\"1024\" height=\"768\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "### Data Efficient Emulation\n",
    "\n",
    "-   For standard Bayesian Optimization ignored *dynamics* of the car.\n",
    "\n",
    "-   For more data efficiency, first *emulate* the dynamics.\n",
    "\n",
    "-   Then do Bayesian optimization of the *emulator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_dynamics = [{'name':'position', 'type':'continuous', 'domain':[-1.2, +0.6]},\n",
    "                  {'name':'velocity', 'type':'continuous', 'domain':[-0.07, +0.07]},\n",
    "                  {'name':'action', 'type':'continuous', 'domain':[-1, +1]}]\n",
    "design_space_dynamics = GPyOpt.Design_space(space=space_dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Use a Gaussian process to model $$\\Delta v_{t+1} = v_{t+1} - v_{t}$$\n",
    "    and $$\\Delta x_{t+1} = p_{t+1} - p_{t}$$\n",
    "\n",
    "-   Two processes, one with mean $v_{t}$ one with mean $p_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)\n",
    "velocity_model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from GPyOpt.experiment_design.random_design import RandomDesign\n",
    "import mountain_car as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Random locations of the inputs\n",
    "n_initial_points = 500\n",
    "random_design_dynamics = RandomDesign(design_space_dynamics)\n",
    "initial_design_dynamics = random_design_dynamics.get_samples(n_initial_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Simulation of the (normalized) outputs\n",
    "y = np.zeros((initial_design_dynamics.shape[0], 2))\n",
    "for i in range(initial_design_dynamics.shape[0]):\n",
    "    y[i, :] = mc.simulation(initial_design_dynamics[i, :])\n",
    "\n",
    "# Normalize the data from the simulation\n",
    "y_normalisation = np.std(y, axis=0)\n",
    "y_normalised = y/y_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulator Training\n",
    "\n",
    "-   Used 500 randomly selected points to train emulators.\n",
    "\n",
    "-   Can make proces smore efficient through *experimental design*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_model.updateModel(initial_design_dynamics, y[:, [0]], None, None)\n",
    "velocity_model.updateModel(initial_design_dynamics, y[:, [1]], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = mc.plot_control(velocity_model)\n",
    "interact(control.plot_slices, control=(-1, 1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "### Emulator Accuracy-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_gains = np.atleast_2d([0, .6, 1])  # change the valus of the linear controller to observe the trayectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.emu_sim_comparison(env, controller_gains, [position_model, velocity_model], \n",
    "                      max_steps=500, diagrams='../slides/diagrams/uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Emulation and Simulation\n",
    "\n",
    "<img src=\"../slides/diagrams/uq/emu_sim_comparison.svg\" align=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Optimize control parameters with emulator\n",
    "car_initial_location = np.asarray([-0.58912799, 0]) \n",
    "\n",
    "### --- Reward objective function using the emulator\n",
    "obj_func_emulator = lambda x: mc.run_emulation([position_model, velocity_model], x, car_initial_location)[0]\n",
    "objective_emulator = GPyOpt.core.task.SingleObjective(obj_func_emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Elements of the optimization that will use the multi-fidelity emulator\n",
    "model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space= [{'name':'linear_1', 'type':'continuous', 'domain':(-1/1.2, +1)},\n",
    "        {'name':'linear_2', 'type':'continuous', 'domain':(-1/0.07, +1/0.07)},\n",
    "        {'name':'constant', 'type':'continuous', 'domain':(-1, +1)}]\n",
    "\n",
    "design_space         = GPyOpt.Design_space(space=space)\n",
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_space)\n",
    "\n",
    "random_design = RandomDesign(design_space)\n",
    "initial_design = random_design.get_samples(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition          = GPyOpt.acquisitions.AcquisitionEI(model, design_space, optimizer=aquisition_optimizer)\n",
    "evaluator            = GPyOpt.core.evaluators.Sequential(acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_emulator = GPyOpt.methods.ModularBayesianOptimization(model, design_space, objective_emulator, acquisition, evaluator, initial_design)\n",
    "bo_emulator.run_optimization(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(bo_emulator.x_opt), render=True)\n",
    "anim=mc.animate_frames(frames, 'Best controller using the emulator of the dynamics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.save_frames(frames, \n",
    "                  diagrams='../slides/diagrams/uq', \n",
    "                  filename='mountain_car_emulated.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Efficiency\n",
    "\n",
    "-   Our emulator used only 500 calls to the simulator.\n",
    "\n",
    "-   Optimizing the simulator directly required 37,500 calls to the\n",
    "    simulator.\n",
    "\n",
    "### Best Controller using Emulator of Dynamics\n",
    "\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_emulated.html\" width=\"1024\" height=\"768\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "500 calls to the simulator vs 37,500 calls to the simulator\n",
    "\n",
    "$$\\mappingFunction_i\\left(\\inputVector\\right) = \\rho\\mappingFunction_{i-1}\\left(\\inputVector\\right) + \\delta_i\\left(\\inputVector \\right)$$\n",
    "\n",
    "### Multi-Fidelity Emulation\n",
    "\n",
    "$$\\mappingFunction_i\\left(\\inputVector\\right) = \\mappingFunctionTwo_{i}\\left(\\mappingFunction_{i-1}\\left(\\inputVector\\right)\\right) + \\delta_i\\left(\\inputVector \\right),$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Collect points from low and high fidelity simulator --- ###\n",
    "\n",
    "space = GPyOpt.Design_space([\n",
    "        {'name':'position', 'type':'continuous', 'domain':(-1.2, +1)},\n",
    "        {'name':'velocity', 'type':'continuous', 'domain':(-0.07, +0.07)},\n",
    "        {'name':'action', 'type':'continuous', 'domain':(-1, +1)}])\n",
    "\n",
    "n_points = 250\n",
    "random_design = GPyOpt.experiment_design.RandomDesign(space)\n",
    "x_random = random_design.get_samples(n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mountain_car as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_position_hf = np.zeros((n_points, 1))\n",
    "d_velocity_hf = np.zeros((n_points, 1))\n",
    "d_position_lf = np.zeros((n_points, 1))\n",
    "d_velocity_lf = np.zeros((n_points, 1))\n",
    "\n",
    "# --- Collect high fidelity points\n",
    "for i in range(0, n_points):\n",
    "    d_position_hf[i], d_velocity_hf[i] = mc.simulation(x_random[i, :])\n",
    "\n",
    "# --- Collect low fidelity points  \n",
    "for i in range(0, n_points):\n",
    "    d_position_lf[i], d_velocity_lf[i] = mc.low_cost_simulation(x_random[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Optimize controller parameters \n",
    "obj_func = lambda x: mc.run_simulation(env, x)[0]\n",
    "obj_func_emulator = lambda x: mc.run_emulation([position_model, velocity_model], x, car_initial_location)[0]\n",
    "objective_multifidelity = GPyOpt.core.task.SingleObjective(obj_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.experiment_design.random_design import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPyOpt.models.GPModel(optimize_restarts=5, verbose=False, exact_feval=True, ARD=True)\n",
    "space= [{'name':'linear_1', 'type':'continuous', 'domain':(-1/1.2, +1)},\n",
    "        {'name':'linear_2', 'type':'continuous', 'domain':(-1/0.07, +1/0.07)},\n",
    "        {'name':'constant', 'type':'continuous', 'domain':(-1, +1)}]\n",
    "\n",
    "design_space = GPyOpt.Design_space(space=space)\n",
    "aquisition_optimizer = GPyOpt.optimization.AcquisitionOptimizer(design_space)\n",
    "\n",
    "n_initial_points = 25\n",
    "random_design = RandomDesign(design_space)\n",
    "initial_design = random_design.get_samples(n_initial_points)\n",
    "acquisition = GPyOpt.acquisitions.AcquisitionEI(model, design_space, optimizer=aquisition_optimizer)\n",
    "evaluator = GPyOpt.core.evaluators.Sequential(acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_multifidelity = GPyOpt.methods.ModularBayesianOptimization(model, design_space, objective_multifidelity, acquisition, evaluator, initial_design)\n",
    "bo_multifidelity.run_optimization(max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, frames = mc.run_simulation(env, np.atleast_2d(bo_multifidelity.x_opt), render=True)\n",
    "anim=mc.animate_frames(frames, 'Best controller with multi-fidelity emulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.save_frames(frames, \n",
    "                  diagrams='../slides/diagrams/uq', \n",
    "                  filename='mountain_car_multi_fidelity.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Controller with Multi-Fidelity Emulator\n",
    "\n",
    "<iframe src=\"../slides/diagrams/uq/mountain_car_multi_fidelity.html\" width=\"1024\" height=\"768\" allowtransparency=\"true\" frameborder=\"0\">\n",
    "</iframe>\n",
    "250 observations of high fidelity simulator and 250 of the low fidelity\n",
    "simulator\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "Stefanos Eleftheriadis, John Bronskill, Hugh Salimbeni, Rich Turner,\n",
    "Zhenwen Dai, Javier Gonzalez, Andreas Damianou, Mark Pullin.\n",
    "\n",
    "### Ongoing Code\n",
    "\n",
    "-   Powerful framework but\n",
    "\n",
    "-   Software isn't there yet.\n",
    "\n",
    "-   Our focus: Gaussian Processes driven by MXNet\n",
    "\n",
    "-   Composition of GPs, Neural Networks, Other Models\n",
    "\n",
    "### Thanks!\n",
    "\n",
    "-   twitter: @lawrennd\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)\n",
    "\n",
    "### References {#references .unnumbered .allowframebreaks}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
