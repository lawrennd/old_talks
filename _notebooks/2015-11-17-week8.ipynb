{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: Latent Variable Modelling\n",
    "### [Neil D. Lawrence](http://inverseprobability.com), University of Sheffield\n",
    "### 2015-11-17\n",
    "\n",
    "**Abstract**: In this lecture we turn to *unsupervised learning*. Specifically, we\n",
    "introduce the idea of a latent variable model. Latent variable models\n",
    "are a probabilistic perspective on unsupervised learning which lead to\n",
    "dimensionality reduction algorithms.\n",
    "\n",
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "%\\newcommand{\\tk}[1]{\\textbf{TK}: #1}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\det}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "% Already defined by latex\n",
    "%\\newcommand{\\vec}{#1:}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$\n",
    "\n",
    "<!-- Front matter -->\n",
    "<!-- Front matter -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!--Back matter-->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods\n",
    "import mlai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Last time: Looked at Bayesian Regression.\n",
    "-   Introduced priors and marginal likelihoods.\n",
    "-   This time: Unsupervised Learning\n",
    "\n",
    "-   Supervised learning is learning where each data has a label\n",
    "    (e.g. regression output)\n",
    "-   In unsupervised learning we have no labels for the data.\n",
    "-   Often thought of as structure discovery.\n",
    "    -   Finding features in the data\n",
    "    -   Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('cluster_data{counter:0>2}.svg', directory='../slides/diagrams/ml', counter=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   One common approach, not deeply covered in this course.\n",
    "-   Associate each data point, $\\dataVector_{i, :}$ with one of $k$\n",
    "    different discrete groups.\n",
    "-   For example:\n",
    "    -   Clustering animals into discrete groups. Are animals discrete or\n",
    "        continuous?\n",
    "    -   Clustering into different different *political* affiliations.\n",
    "-   Humans do seem to like clusters:\n",
    "    -   Very useful when interacting with biologists.\n",
    "-   Subtle difference between clustering and *vector quantisation*\n",
    "\n",
    "-   Little anecdote.\n",
    "\n",
    "-   To my mind difference is in clustering there should be a reduction\n",
    "    in data density between samples.\n",
    "-   This definition is not universally applied.\n",
    "-   For today's purposes we merge them:\n",
    "    -   Determine how to allocate each point to a group and *harder*\n",
    "        total number of groups.\n",
    "-   Simple algorithm for allocating points to groups.\n",
    "-   *Require*: Set of $k$ cluster centres & assignment of each points to\n",
    "    a cluster.\n",
    "\n",
    "1.  Initialize cluster centres as randomly selected data points.\n",
    "    2.  Assign each data point to *nearest* cluster centre.\n",
    "    3.  Update each cluster centre by setting it to the mean of assigned\n",
    "        data points.\n",
    "    4.  Repeat 2 and 3 until cluster allocations do not change.\n",
    "\n",
    "-   This minimizes the objective $$\n",
    "    E=\\sum_{j=1}^K \\sum_{i\\ \\text{allocated to}\\ j}  \\left(\\dataVector_{i, :} - \\meanVector_{j, :}\\right)^\\top\\left(\\dataVector_{i, :} - \\meanVector_{j, :}\\right)\n",
    "    $$ *i.e.* it minimizes thesum of Euclidean squared distances betwen\n",
    "    points and their associated centres.\n",
    "-   The minimum is *not* guaranteed to be *global* or *unique*.\n",
    "-   This objective is a non-convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('kmeans_clustering_{counter:0>3}.svg', directory='../slides/diagrams/ml', \n",
    "                            text_top='kmeans_clustering_{counter:0>3}.tex', counter=(0, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Spectral clustering (@Shi:normalized00,@Ng:spectral02)\n",
    "    -   Allows clusters which aren't convex hulls.\n",
    "-   Dirichlet process\n",
    "    -   A probabilistic formulation for a clustering algorithm that is\n",
    "        *non-parametric*.\n",
    "    -   Loosely speaking it allows infinite clusters\n",
    "    -   In practice useful for dealing with previously unknown species\n",
    "        (e.g. a \"Black Swan Event\").\n",
    "-   USPS Data Set Handwritten Digit\n",
    "-   3648 dimensions (64 rows, 57 columns)\n",
    "-   Space contains much more than just this digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_six{counter:0>3}.png', directory='../slides/diagrams/ml', counter=IntSlider(0, 0, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Even if we sample every nanonsecond from now until end of universe\n",
    "    you won't see original six!\n",
    "\n",
    "-   Rotate a prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imrotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_image = np.hstack([np.zeros((rows, 3)), six_image, np.zeros((rows, 4))])\n",
    "dim_one = np.asarray(six_image.shape)\n",
    "angles = range(360)\n",
    "i = 0\n",
    "Y = np.zeros((len(angles), np.prod(dim_one)))\n",
    "for angle in angles:\n",
    "    rot_image = imrotate(six_image, angle, interp='nearest')\n",
    "    dim_two = np.asarray(rot_image.shape)\n",
    "    start = [int(round((dim_two[0] - dim_one[0])/2)), int(round((dim_two[1] - dim_one[1])/2))]\n",
    "    crop_image = rot_image[start[0]+np.array(range(dim_one[0])), start[1]+np.array(range(dim_one[1]))]\n",
    "    Y[i, :] = crop_image.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_six_rotate{counter:0>3}.png', directory='../slides/diagrams/ml', counter=(0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Pure rotation is too simple\n",
    "    -   In practice data may undergo several distortions.\n",
    "-   For high dimensional data with *structure*:\n",
    "    -   We expect fewer distortions than dimensions;\n",
    "    -   Therefore we expect the data to live on a lower dimensional\n",
    "        manifold.\n",
    "    -   Conclusion: Deal with high dimensional data by looking for a\n",
    "        lower dimensional non-linear embedding.\n",
    "-   PCA (@Hotelling:analysis33) is a linear embedding.\n",
    "-   Today its presented as:\n",
    "    -   Rotate to find 'directions' in data with maximal variance.\n",
    "    -   How do we find these directions?\n",
    "-   Algorithmically we do this by diagonalizing the sample covariance\n",
    "    matrix $$\n",
    "    \\mathbf{S}=\\frac{1}{\\numData}\\sum_{i=1}^\\numData \\left(\\dataVector_{i, :}-\\meanVector\\right)\\left(\\dataVector_{i, :} - \\meanVector\\right)^\\top\n",
    "    $$\n",
    "\n",
    "-   Find directions in the data,\n",
    "    $\\latentVector = \\mathbf{U}\\dataVector$, for which variance is\n",
    "    maximized.\n",
    "\n",
    "-   Solution is found via constrained optimisation (which uses [Lagrange\n",
    "    multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)): $$\n",
    "    L\\left(\\mathbf{u}_{1},\\lambda_{1}\\right)=\\mathbf{u}_{1}^{\\top}\\mathbf{S}\\mathbf{u}_{1}+\\lambda_{1}\\left(1-\\mathbf{u}_{1}^{\\top}\\mathbf{u}_{1}\\right)\n",
    "    $$\n",
    "\n",
    "-   Gradient with respect to $\\mathbf{u}_{1}$\n",
    "    $$\\frac{\\text{d}L\\left(\\mathbf{u}_{1},\\lambda_{1}\\right)}{\\text{d}\\mathbf{u}_{1}}=2\\mathbf{S}\\mathbf{u}_{1}-2\\lambda_{1}\\mathbf{u}_{1}$$\n",
    "    rearrange to form\n",
    "    $$\\mathbf{S}\\mathbf{u}_{1}=\\lambda_{1}\\mathbf{u}_{1}.$$ Which is\n",
    "    known as an [*eigenvalue\n",
    "    problem*](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).\n",
    "-   Further directions that are *orthogonal* to the first can also be\n",
    "    shown to be eigenvectors of the covariance.\n",
    "\n",
    "-   Represent data, $\\dataMatrix$, with a lower dimensional set of\n",
    "    latent variables $\\latentMatrix$.\n",
    "-   Assume a linear relationship of the form $$\n",
    "    \\dataVector_{i,:}=\\mappingMatrix\\latentVector_{i,:}+\\noiseVector_{i,:},\n",
    "    $$ where $$\n",
    "    \\noiseVector_{i,:} \\sim \\gaussianSamp{\\zerosVector}{\\noiseStd^2\\eye}\n",
    "    $$\n",
    "\n",
    "**Probabilistic PCA**\n",
    "\n",
    "-   Define *linear-Gaussian relationship* between latent variables and\n",
    "    data.\n",
    "-   **Standard** Latent variable approach:\n",
    "    -   Define Gaussian prior over *latent space*, $\\latentMatrix$.\n",
    "-   Integrate out *latent variables*.\n",
    "\n",
    "\\begin{tikzpicture}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Define nodes\n",
    "\\node[obs]\n",
    "(Y) {$\\dataMatrix$};\n",
    "\\node[const, above=of Y, xshift=-1.2cm] (W)\n",
    "{$\\mappingMatrix$};\n",
    "\\node[latent, above=of Y, xshift=1.2cm]  (X)\n",
    "{$\\latentMatrix$};\n",
    "\\node[const, right=1cm of Y]            (sigma)\n",
    "{$\\dataStd^2$};\n",
    "\n",
    "% Connect the nodes\n",
    "\\edge\n",
    "{X,W,sigma} {Y} ; %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\end{tikzpicture}\n",
    "$$\n",
    "p\\left(\\dataMatrix|\\latentMatrix,\\mappingMatrix\\right)=\\prod_{i=1}^{\\numData}\\gaussianDist{\\dataVector_{i,:}}{\\mappingMatrix\\latentVector_{i,:}}{\\noiseStd^2\\eye}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p\\left(\\latentMatrix\\right)=\\prod_{i=1}^{\\numData}\\gaussianDist{\\latentVector_{i,:}}{\\zerosVector}{\\eye}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p\\left(\\dataMatrix|\\mappingMatrix\\right)=\\prod_{i=1}^{\\numData}\\gaussianDist{\\dataVector_{i,:}}{\\zerosVector}{\\mappingMatrix\\mappingMatrix^{\\top}+\\noiseStd^{2}\\eye}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dataVector_{i,:}=\\mappingMatrix\\latentVector_{i,:}+\\noiseVector_{i,:},\\quad \\latentVector_{i,:} \\sim \\gaussianSamp{\\zerosVector}{\\eye}, \\quad \\noiseVector_{i,:} \\sim \\gaussianSamp{\\zerosVector}{\\noiseStd^{2}\\eye}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mappingMatrix\\latentVector_{i,:} \\sim \\gaussianSamp{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mappingMatrix\\latentVector_{i, :} + \\noiseVector_{i, :} \\sim \\gaussianSamp{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye}\n",
    "$$\n",
    "\n",
    "**Probabilistic PCA Max. Likelihood Soln** (@Tipping:probpca99)\n",
    "\n",
    "\\%\\\\includegraphics\\<1\\>\\[width=0.25\\textwidth\\]{../../../gplvm/tex/diagrams/ppcaGraph}\n",
    "\\begin{tikzpicture}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Define nodes\n",
    "\\node[obs]\n",
    "(Y) {$\\dataMatrix$};\n",
    "\\node[const, above=of Y] (W) {$\\mappingMatrix$};\n",
    "\\node[const, right=1cm of Y]            (sigma) {$\\dataStd^2$};\n",
    "\n",
    "%\n",
    "Connect the nodes\n",
    "\\edge {W,sigma} {Y} ; %\n",
    "    \\end{tikzpicture}\n",
    "$$p\\left(\\dataMatrix|\\mappingMatrix\\right)=\\prod_{i=1}^{\\numData}\\gaussianDist{\\dataVector_{i, :}}{\\zerosVector}{\\mappingMatrix\\mappingMatrix^{\\top}+\\noiseStd^{2}\\eye}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probabilistic PCA Max. Likelihood Soln** (@Tipping:probpca99) $$\n",
    "  p\\left(\\dataMatrix|\\mappingMatrix\\right)=\\prod_{i=1}^{\\numData}\\gaussianDist{\\dataVector_{i,:}}{\\zerosVector}{\\covarianceMatrix},\\quad \\covarianceMatrix=\\mappingMatrix\\mappingMatrix^{\\top}+\\noiseStd^{2}\\eye\n",
    "  $$ $$\n",
    "  \\log p\\left(\\dataMatrix|\\mappingMatrix\\right)=-\\frac{\\numData}{2}\\log\\left|\\covarianceMatrix\\right|-\\frac{1}{2}\\text{tr}\\left(\\covarianceMatrix^{-1}\\dataMatrix^{\\top}\\dataMatrix\\right)+\\text{const.}\n",
    "  $$ If $\\mathbf{U}_{q}$ are first $q$ principal eigenvectors of\n",
    "$n^{-1}\\dataMatrix^{\\top}\\dataMatrix$ and the corresponding eigenvalues\n",
    "are $\\boldsymbol{\\Lambda}_{q}$, $$\n",
    "  \\mappingMatrix=\\mathbf{U}_{q}\\mathbf{L}\\mathbf{R}^{\\top},\\quad\\mathbf{L}=\\left(\\boldsymbol{\\Lambda}_{q}-\\noiseStd^{2}\\eye\\right)^{\\frac{1}{2}}\n",
    "  $$ where $\\mathbf{R}$ is an arbitrary rotation matrix.\n",
    "\n",
    "-   Chapter 7 of @Rogers:book11 up to pg 249.\n",
    "\n",
    "So far in our classes we have focussed mainly on regression problems,\n",
    "which are examples of supervised learning. We have considered the\n",
    "relationship between the likelihood and the objective function and we\n",
    "have shown how we can find paramters by maximizing the likelihood\n",
    "(equivalent to minimizing the objective function) and in the last\n",
    "session we saw how we can *marginalize* the parameters in a process\n",
    "known as Bayesian inference.\n",
    "\n",
    "Now we are going to turn to a different form of learning, commonly known\n",
    "as *unsupervised* learning. In unsupervised learning our data isn't\n",
    "necessarily labelled in any form, but we want models that give us a\n",
    "better understanding of the data. We've actually seen an example of this\n",
    "already with [*matrix factorization* for collaborative\n",
    "filtering](./week2.ipynb), which we introduces in the context of\n",
    "*objective functions*. Now we will introduce a more probabilistic\n",
    "approach to such models, specifically we are interested in *latent\n",
    "variable* modelling.\n",
    "\n",
    "# Latent Variables\n",
    "\n",
    "Latent means hidden, and hidden variables are simply *unobservable*\n",
    "variables. The idea of a latent variable is crucial to the concept of\n",
    "artificial intelligence, machine learning and experimental design. A\n",
    "latent variable could take many forms. We might observe a man walking\n",
    "along a road with a large bag of clothes and we might *infer* that the\n",
    "man is walking to the laundrette. Our observations are a highly complex\n",
    "data space, the response in our eyes is processed through our visual\n",
    "cortex, the combination of the indidivuals limb movememnts and the\n",
    "direction they are walking in all conflate in our heads to cause us to\n",
    "infer that (perhaps) the individual is going to the laundrette. We don't\n",
    "*know* that the man is walking to the laundrette, but we have a model of\n",
    "the world that suggests that it's a likely outcome for the very complex\n",
    "data. In some ways the latent variable can be seen as a *compression* of\n",
    "this very complex scene. If I were writing a book, I might write that \"A\n",
    "man tripped over whilst walking to the laundrette\". In the reader's mind\n",
    "an image of a man, perhaps laden with dirty clothes, may occur. All\n",
    "these ideas come from our expectations of the world around us. We can\n",
    "make further inference about the man, some of it perhaps plausible\n",
    "others less so. The man may be going to the laundrette because his\n",
    "washing machine is broken, or because he doesn't have a large enough\n",
    "flat to have a washing machine, or because he's carrying a duvet, or\n",
    "because he doesn't like ironing. All of these may *increase* in\n",
    "probability given our observation, but they are still *latent*\n",
    "variables. Unless we follow the man back to his appartment, or start\n",
    "making other enquirires about the man, we don't know the true answer.\n",
    "\n",
    "It's clear that to do inference about any complex system we *must*\n",
    "include latent variables. Latent variables are extremely powerful. In\n",
    "robotics, they are used to represent the *state* of the robot. The state\n",
    "of the robot may include its position (in x, y coordinates) its speed,\n",
    "its direction of facing. How are *these* variables unknown to the robot?\n",
    "Well the robot only posesses *sensors*, it can make observations of the\n",
    "nearest object in a certain direction, and it may have a map of its\n",
    "environment. If we represent the state of the robot as its position on a\n",
    "map, it may be uncertain of that position. If you go walking or running\n",
    "in the hills around Sheffield, you can take a very high quality ordnance\n",
    "survey map with you. However, unless you are a really excellent\n",
    "orienteer, when you are far from any given landmark, you will probably\n",
    "be *uncertain* about your true position on the map. These states are\n",
    "also latent variables.\n",
    "\n",
    "In statistical analysis of experiments you try to control for each\n",
    "aspect of the experiment, in particular by *randomization*. So if I'm\n",
    "interested in the ability of a particular fertilizer to improve the\n",
    "yield of a particular plant I may design an experiment where I apply the\n",
    "fertilizer to some plants (the treatment group) and withold the\n",
    "fertilizer from others (the control group). I then test to see whether\n",
    "the yield from the treatment group is better (or worse) than the control\n",
    "group. I may find that I have an excellent yield for the treatment\n",
    "group. However, what if I'd (unknowlingly) planted all my treatment\n",
    "plants in a sunny part of the field, and all the control plants in a\n",
    "shady part of the field. That would also be a latent variable, in this\n",
    "case known as a *confounder*. In statistical experimental design\n",
    "*randomization* is used to attempt to eliminate the correlated effects\n",
    "of these confounders: you aim to ensure that if these confounders *do*\n",
    "exist their effects are not correlated with treatment and contorl. This\n",
    "is known as a [randomized control\n",
    "trial](http://en.wikipedia.org/wiki/Randomized_controlled_trial).\n",
    "\n",
    "Greek philosophers worried a great deal about what was knowable and what\n",
    "was unknowable. Adherents of [philosophical\n",
    "Skeptisism](http://en.wikipedia.org/wiki/Skepticism) were inspired by\n",
    "the idea that since your senses sometimes give you contradictory\n",
    "information, they cannot be trusted, and in extreme cases they chose to\n",
    "*ignore* their senses. This is an acknowledgement that very often the\n",
    "true state of the world cannot be known with precision. Unfortunately,\n",
    "these philosophers didn't have a good understanding of probability, so\n",
    "they were unable to encapsulate their ideas through a *degree* of\n",
    "belief.\n",
    "\n",
    "We often use language to express the compression of a complex behavior\n",
    "or patterns in a simpler way, for example we talk about motives as a\n",
    "useful distallation for a perhaps very complex patter of behavior. In\n",
    "physics we use principles of causation and simple laws to describe the\n",
    "world around us. Such motives or underlying principles are difficult to\n",
    "observe directly, our conclusions about them emerge over a period of\n",
    "time by observing indirect consequences of the latent variables.\n",
    "\n",
    "Epistemic uncertainty allows us to deal with these worries by\n",
    "associating our degree of belief about the state of the world with a\n",
    "probaiblity distribution. This core idea underpins state space\n",
    "modelling, probabilistic graphical models and the wider field of latent\n",
    "variable modelling. In this session we are going to explore the idea in\n",
    "a simple linear system and see how it relates to *factor analysis* and\n",
    "*principal component analysis*.\n",
    "\n",
    "# Your Personality\n",
    "\n",
    "At the beginning of the 20th century there was a great deal of interest\n",
    "amoungst psychologists in formalizing patterns of thought. The approach\n",
    "they used became known as factor analysis. The principle is that we\n",
    "observe a potentially high dimensional vector of characteristics about\n",
    "an individual. To formalize this, social scientists designed\n",
    "questionaires. We can envisage many questions that we may ask, but the\n",
    "assumption is that underlying these questions there are only a few\n",
    "traits that dictate the behavior. These models are known as latent trait\n",
    "models and the analysis is sometimes known as factor analysis. The idea\n",
    "is that there are a few characteristic traits that we are looking to\n",
    "discern. These traits or factors can be extracted by assimilating the\n",
    "high dimensional characteristics of the individual into a few latent\n",
    "factors.\n",
    "\n",
    "## Factor Analysis Model\n",
    "\n",
    "This causes us to consider a model as follows, if we are given a high\n",
    "dimensional vector of features (perhaps questionaire answers) associated\n",
    "with an individual, $\\dataVector$, we assume that these factors are\n",
    "actually generated from a low dimensional vector latent traits, or\n",
    "latent variables, which determine the personality. $$\n",
    "\\dataVector = \\mathbf{f}(\\latentVector) + \\noiseVector\n",
    "$$ where $\\mathbf{f}(\\latentVector)$ is a *vector valued* function that\n",
    "is dependent on the latent traits and $\\noiseVector$ is some corrupting\n",
    "noise. For simplicity, we assume that the function is given by a\n",
    "*linear* relationship, $$\n",
    "\\mathbf{f}(\\latentVector) = \\mappingMatrix\\latentVector\n",
    "$$ where we have introduced a matrix $\\mappingMatrix$ that is sometimes\n",
    "referred to as the *factor loadings* but we also immediately see is\n",
    "related to our *multivariate linear regression* models from the\n",
    "[previous session on linear regression](./week3.ipynb). That is because\n",
    "our vector valued function is of the form $$\n",
    "\\mathbf{f}(\\latentVector) =\n",
    "\\begin{bmatrix} f_1(\\latentVector) \\\\ f_2(\\latentVector) \\\\ \\vdots \\\\\n",
    "f_p(\\latentVector)\\end{bmatrix}\n",
    "$$ where there are $p$ features associated with the individual. If we\n",
    "consider any of these functions individually we have a prediction\n",
    "function that looks like a regression model, $$\n",
    "f_j(\\latentVector) =\n",
    "\\weightVector_{j, :}^\\top \\latentVector,\n",
    "$$ for each element of the vector valued function, where\n",
    "$\\weightVector_{:, j}$ is the $j$th column of the matrix\n",
    "$\\mappingMatrix$. In that context each column of $\\mappingMatrix$ is a\n",
    "vector of *regression weights*. This is a multiple input and multiple\n",
    "output regression. Our inputs (or covariates) have dimensionality\n",
    "greater than 1 and our outputs (or response variables) also have\n",
    "dimensionality greater than one. Just as in a standard regression, we\n",
    "are assuming that we don't observe the function directly (note that this\n",
    "*also* makes the function a *type* of latent variable), but we observe\n",
    "some corrupted variant of the function, where the corruption is given by\n",
    "$\\noiseVector$. Just as in linear regression we can assume that this\n",
    "corruption is given by Gaussian noise, where the noise for the $j$th\n",
    "element of $\\dataVector$ is by, $$\n",
    "\\epsilon_j \\sim \\gaussianSamp{0}{\\noiseStd^2_j}.\n",
    "$$ Of course, just as in a regression problem we also need to make an\n",
    "assumption across the individual data points to form our full\n",
    "likelihood. Our data set now consists of many observations of\n",
    "$\\dataVector$ for diffetent individuals. We store these observations in\n",
    "a *design matrix*, $\\dataMatrix$, where each *row* of $\\dataMatrix$\n",
    "contains the observation for one individual. To emphasize that\n",
    "$\\dataVector$ is a vector derived from a row of $\\dataMatrix$ we\n",
    "represent the observation of the features associated with the $i$th\n",
    "individual by $\\dataVector_{i, :}$, and place each individual in our\n",
    "data matrix, $$\n",
    "\\dataMatrix\n",
    "= \\begin{bmatrix} \\dataVector_{1, :}^\\top \\\\ \\dataVector_{2, :}^\\top \\\\ \\vdots \\\\\n",
    "\\dataVector_{n, :}^\\top\\end{bmatrix},\n",
    "$$ where we have $n$ data points. Our data matrix therefore has $n$ rows\n",
    "and $p$ columns. The point to notice here is that each data obsesrvation\n",
    "appears as a row vector in the design matrix (thus the transpose\n",
    "operation inside the brackets). Our prediction functions are now\n",
    "actually a *matrix value* function, $$\n",
    "\\mathbf{F} = \\latentMatrix\\mappingMatrix^\\top,\n",
    "$$ where for each matrix the data points are in the rows and the data\n",
    "features are in the columns. This implies that if we have $q$ inputs to\n",
    "the function we have $\\mathbf{F}\\in \\Re^{n\\times p}$,\n",
    "$\\mappingMatrix \\in \\Re^{p \\times q}$ and\n",
    "$\\latentMatrix \\in \\Re^{n\\times q}$.\n",
    "\n",
    "### Assignment Question 1\n",
    "\n",
    "Show that, given all the definitions above, if, $$\n",
    "\\mathbf{F} = \\latentMatrix\\mappingMatrix^\\top\n",
    "$$ and the elements of the vector valued function $\\mathbf{F}$ are given\n",
    "by $$\n",
    "f_{i, j} = f_j(\\latentVector_{i, :}),\n",
    "$$ where $\\latentVector_{i, :}$ is the $i$th row of the latent\n",
    "variables, $\\latentMatrix$, then show that $$\n",
    "f_j(\\latentVector_{i, :}) = \\weightVector_{j, :}^\\top\n",
    "\\latentVector_{i, :}\n",
    "$$\n",
    "\n",
    "*10 marks*\n",
    "\n",
    "#### Question 1 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "## Latent Variables\n",
    "\n",
    "The difference between this model and a multiple output regression is\n",
    "that in the regression case we are provided with the covariates\n",
    "$\\latentMatrix$, here they are *latent variables*. These variables are\n",
    "unknown. Just as we have done in the past for unknowns, we now treat\n",
    "them with a probability distribution. In *factor analysis* we assume\n",
    "that the latent variables have a Gaussian density which is independent\n",
    "across both across the latent variables associated with the different\n",
    "data points, and across those associated with different data features,\n",
    "so we have, $$\n",
    "x_{i,j} \\sim\n",
    "\\gaussianSamp{0}{1},\n",
    "$$ and we can write the density governing the latent variable associated\n",
    "with a single point as, $$\n",
    "\\latentVector_{i, :} \\sim \\gaussianSamp{\\zerosVector}{\\eye}.\n",
    "$$ If we consider the values of the function for the $i$th data point as\n",
    "$$\n",
    "\\mathbf{f}_{i, :} =\n",
    "\\mathbf{f}(\\latentVector_{i, :}) = \\mappingMatrix\\latentVector_{i, :} \n",
    "$$ then we can use the rules for multivariate Gaussian relationships to\n",
    "write that $$\n",
    "\\mathbf{f}_{i, :} \\sim \\gaussianSamp{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top}\n",
    "$$ which implies that the distribution for $\\dataVector_{i, :}$ is given\n",
    "by $$\n",
    "\\dataVector_{i, :} = \\sim \\gaussianSamp{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top + \\boldsymbol{\\Sigma}}\n",
    "$$ where $\\boldsymbol{\\Sigma}$ the covariance of the noise variable,\n",
    "$\\epsilon_{i, :}$ which for factor analysis is a diagonal matrix\n",
    "(because we have assumed that the noise was *independent* across the\n",
    "features), $$\n",
    "\\boldsymbol{\\Sigma} = \\begin{bmatrix}\\noiseStd^2_{1} & 0 & 0 & 0\\\\\n",
    "0 & \\noiseStd^2_{2} & 0 & 0\\\\\n",
    "                                     0 & 0 & \\ddots &\n",
    "0\\\\\n",
    "                                     0 & 0 & 0 & \\noiseStd^2_p\\end{bmatrix}.\n",
    "$$ For completeness, we could also add in a *mean* for the data vector\n",
    "$\\meanVector$, $$\n",
    "\\dataVector_{i, :} = \\mappingMatrix \\latentVector_{i, :} +\n",
    "\\meanVector + \\noiseVector_{i, :}\n",
    "$$ which would give our marginal distribution for $\\dataVector_{i, :}$ a\n",
    "mean $\\meanVector$. However, the maximum likelihood solution for\n",
    "$\\meanVector$ turns out to equal the empirical mean of the data, $$\n",
    "\\meanVector = \\frac{1}{\\numData} \\sum_{i=1}^\\numData\n",
    "\\dataVector_{i, :},\n",
    "$$ *regardless* of the form of the covariance,\n",
    "$\\covarianceMatrix = \\mappingMatrix\\mappingMatrix^\\top + \\boldsymbol{\\Sigma}$.\n",
    "As a result it is very common to simply preprocess the data and ensure\n",
    "it is zero mean. We will follow that convention for this session.\n",
    "\n",
    "The prior density over latent variables is independent, and the\n",
    "likelihood is independent, that means that the marginal likelihood here\n",
    "is also independent over the data points. Factor analysis was developed\n",
    "mainly in psychology and the social sciences for understanding\n",
    "personality and intelligence. [Charles\n",
    "Spearman](http://en.wikipedia.org/wiki/Charles_Spearman) was concerned\n",
    "with the measurements of \"the abilities of man\" and is credited with the\n",
    "earliest version of factor analysis.\n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "In 1933 [Harold\n",
    "Hotelling](http://en.wikipedia.org/wiki/Harold_Hotelling) published on\n",
    "*principal component analysis* the first mention of this approach.\n",
    "Hotelling's inspiration was to provide mathematical foundation for\n",
    "factor analysis methods that were by then widely used within psychology\n",
    "and the social sciences. His model was a factor analysis model, but he\n",
    "considered the noiseless 'limit' of the model. In other words he took\n",
    "$\\noiseStd^2_i \\rightarrow 0$ so that he had $$\n",
    "\\dataVector_{i, :} \\sim \\lim_{\\noiseStd^2 \\rightarrow 0} \\gaussianSamp{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye}.\n",
    "$$ The paper had two unfortunate effects. Firstly, the resulting model\n",
    "is no longer valid probablistically, because the covariance of this\n",
    "Gaussian is 'degenerate'. Because $\\mappingMatrix\\mappingMatrix^\\top$\n",
    "has rank of at most $q$ where $q<p$ (due to the dimensionality\n",
    "reduction) the determinant of the covariance is zero, meaning the\n",
    "inverse doesn't exist so the density, $$\n",
    "p(\\dataVector_{i, :}|\\mappingMatrix) =\n",
    "\\lim_{\\noiseStd^2 \\rightarrow 0} \\frac{1}{(2\\pi)^\\frac{p}{2}\n",
    "|\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye|^{-1}}\n",
    "\\exp\\left(-\\frac{1}{2}\\dataVector_{i, :}\\left[\\mappingMatrix\\mappingMatrix^\\top+ \\noiseStd^2\n",
    "\\eye\\right]^{-1}\\dataVector_{i, :}\\right),\n",
    "$$ is *not* valid for $q<p$ (where $\\mappingMatrix\\in \\Re^{p\\times q}$).\n",
    "This mathematical consequence is a probability density which has no\n",
    "'support' in large regions of the space for $\\dataVector_{i, :}$. There\n",
    "are regions for which the probability of $\\dataVector_{i, :}$ is zero.\n",
    "These are any regions that lie off the hyperplane defined by mapping\n",
    "from $\\latentVector$ to $\\dataVector$ with the matrix $\\mappingMatrix$.\n",
    "In factor analysis the noise corruption, $\\noiseVector$, allows for\n",
    "points to be found away from the hyperplane. In Hotelling's PCA the\n",
    "noise variance is zero, so there is only support for points that fall\n",
    "precisely on the hyperplane. Secondly, Hotelling explicity chose to\n",
    "rename factor analysis as principal component analysis, arguing that the\n",
    "factors social scientist sought were different in nature to the concept\n",
    "of a mathematical factor. This was unfortunate because the factor\n",
    "loadings, $\\mappingMatrix$ can also be seen as factors in the\n",
    "mathematical sense because the model Hotelling defined is a Gaussian\n",
    "model with covariance given by\n",
    "$\\covarianceMatrix = \\mappingMatrix\\mappingMatrix^\\top$ so\n",
    "$\\mappingMatrix$ is a *factor* of the covariance in the mathematical\n",
    "sense, as well as a factor loading.\n",
    "\n",
    "However, the paper had one great advantage over standard approaches to\n",
    "factor analysis. Despite the fact that the model was a special case that\n",
    "is subsumed by the more general approach of factor analysis it is this\n",
    "special case that leads to a particular algorithm, namely that the\n",
    "factor loadings (or principal components as Hotelling referred to them)\n",
    "are given by an *eigenvalue decomposition* of the empirical covariance\n",
    "matrix.\n",
    "\n",
    "## \n",
    "\n",
    "Eigenvalue Decomposition\n",
    "\n",
    "Eigenvalue problems are widespreads in physics and mathematics, they are\n",
    "often written as a matrix/vector equation but we prefer to write them as\n",
    "a full matrix equation. In an eigenvalue problem you are looking to find\n",
    "a matrix of eigenvectors, $\\mathbf{U}$ and a *diagonal* matrix of\n",
    "eigenvalues, $\\boldsymbol{\\Lambda}$ that satisfy the *matrix* equation\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\boldsymbol{\\Lambda}.\n",
    "$$ where $\\mathbf{A}$ is your matrix of interest. This equation is not\n",
    "trivially solvable through matrix inverse because matrix multiplication\n",
    "is not [commutative](http://en.wikipedia.org/wiki/Commutative_property),\n",
    "so premultiplying by $\\mathbf{U}^{-1}$ gives $$\n",
    "\\mathbf{U}^{-1}\\mathbf{A}\\mathbf{U}\n",
    "= \\boldsymbol{\\Lambda}, \n",
    "$$ where we remember that $\\boldsymbol{\\Lambda}$ is a *diagonal* matrix,\n",
    "so the eigenvectors can be used to *diagonalise* the matrix. When\n",
    "performing the eigendecomposition on a Gaussian covariances,\n",
    "diagonalisation is very important because it returns the covariance to a\n",
    "form where there is no correlation between points.\n",
    "\n",
    "## Positive Definite\n",
    "\n",
    "We are interested in the case where $\\mathbf{A}$ is a covariance matrix,\n",
    "which implies it is *positive definite*. A positive definite matrix is\n",
    "one for which the inner product, $$\n",
    "\\weightVector^\\top \\covarianceMatrix\\weightVector\n",
    "$$ is positive for *all* values of the vector $\\weightVector$ other than\n",
    "the zero vector. One way of creating a positive definite matrix is to\n",
    "assume that the symmetric and positive definite matrix\n",
    "$\\covarianceMatrix\\in \\Re^{p\\times p}$ is factorised into,\n",
    "$\\mathbf{A}in \\Re^{p\\times p}$, a *full rank* matrix, so that $$\n",
    "\\covarianceMatrix = \\mathbf{A}^\\top\n",
    "\\mathbf{A}.\n",
    "$$ This ensures that $\\covarianceMatrix$ must be positive definite\n",
    "because $$\n",
    "\\weightVector^\\top \\covarianceMatrix\\weightVector=\\weightVector^\\top\n",
    "\\mathbf{A}^\\top\\mathbf{A}\\weightVector \n",
    "$$ and if we now define a new *vector* $\\mathbf{b}$ as $$\n",
    "\\mathbf{b} = \\mathbf{A}\\weightVector\n",
    "$$ we can now rewrite as $$\n",
    "\\weightVector^\\top \\covarianceMatrix\\weightVector = \\mathbf{b}^\\top\\mathbf{b} = \\sum_{i}\n",
    "b_i^2\n",
    "$$ which, since it is a sum of squares, is positive or zero. The\n",
    "constraint that $\\mathbf{A}$ must be *full rank* ensures that there is\n",
    "no vector $\\weightVector$, other than the zero vector, which causes the\n",
    "vector $\\mathbf{b}$ to be all zeros.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "If $\\covarianceMatrix=\\mathbf{A}^\\top \\mathbf{A}$ then express\n",
    "$c_{i,j}$, the value of the element at the $i$th row and the $j$th\n",
    "column of $\\covarianceMatrix$, in terms of the columns of $\\mathbf{A}$.\n",
    "Use this to show that (i) the matrix is symmetric and (ii) the matrix\n",
    "has positive elements along its diagonal.\n",
    "\n",
    "*15 marks*\n",
    "\n",
    "### Write your answer to Question 2 here\n",
    "\n",
    "# Eigenvectors of a Symmetric Matric\n",
    "\n",
    "Symmetric matrices have *orthonormal* eigenvectors. This means that\n",
    "$\\mathbf{U}$ is an [orthogonal\n",
    "matrix](http://en.wikipedia.org/wiki/Orthogonal_matrix),\n",
    "$\\mathbf{U}^\\top\\mathbf{U} = \\eye$. This implies that\n",
    "$\\mathbf{u}_{:, i} ^\\top \\mathbf{u}_{:, j}$ is equal to 0 if $i\\neq j$\n",
    "and 1 if $i=j$.\n",
    "\n",
    "# Probabilistic PCA\n",
    "\n",
    "In 1997 [Tipping and\n",
    "Bishop](http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf)\n",
    "and [Roweis](https://www.cs.nyu.edu/~roweis/papers/empca.pdf)\n",
    "independently revisited Hotelling's model and considered the case where\n",
    "the noise variance was finite, but *shared* across all output dimensons.\n",
    "Their model can be thought of as a factor analysis where $$\n",
    "\\boldsymbol{\\Sigma} = \\noiseStd^2 \\eye.\n",
    "$$ This leads to a marginal likelihood of the form $$\n",
    "p(\\dataMatrix|\\mappingMatrix, \\noiseStd^2)\n",
    "= \\prod_{i=1}^\\numData\\gaussianDist{\\dataVector_{i, :}}{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye}\n",
    "$$ where the limit of $\\noiseStd^2\\rightarrow 0$ is *not* taken. This\n",
    "defines a proper probabilistic model. Tippping and Bishop then went on\n",
    "to prove that the *maximum likelihood* solution of this model with\n",
    "respect to $\\mappingMatrix$ is given by an eigenvalue problem. In the\n",
    "probabilistic PCA case the eigenvalues and eigenvectors are given as\n",
    "follows. $$\n",
    "\\mappingMatrix = \\mathbf{U}\\mathbf{L} \\mathbf{R}^\\top\n",
    "$$ where $\\mathbf{U}$ is the eigenvectors of the empirical covariance\n",
    "matrix $$\n",
    "\\mathbf{S} = \\sum_{i=1}^\\numData (\\dataVector_{i, :} - \\meanVector)(\\dataVector_{i,\n",
    ":} - \\meanVector)^\\top,\n",
    "$$ which can be written\n",
    "$\\mathbf{S} = \\frac{1}{\\numData} \\dataMatrix^\\top\\dataMatrix$ if the\n",
    "data is zero mean. The matrix $\\mathbf{L}$ is diagonal and is dependent\n",
    "on the *eigenvalues* of $\\mathbf{S}$, $\\boldsymbol{\\Lambda}$. If the\n",
    "$i$th diagonal element of this matrix is given by $\\lambda_i$ then the\n",
    "corresponding element of $\\mathbf{L}$ is $$\n",
    "\\ell_i =\n",
    "\\sqrt{\\lambda_i - \\noiseStd^2}\n",
    "$$ where $\\noiseStd^2$ is the noise variance. Note that if $\\noiseStd^2$\n",
    "is larger than any particular eigenvalue, then that eigenvalue (along\n",
    "with its corresponding eigenvector) is *discarded* from the solution.\n",
    "\n",
    "## Python Implementation of Probabilistic PCA\n",
    "\n",
    "We will now implement this algorithm in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic PCA algorithm\n",
    "def ppca(Y, q):\n",
    "    # remove mean\n",
    "    Y_cent = Y - Y.mean(0)\n",
    "    import numpy as np\n",
    "\n",
    "    # Comute covariance\n",
    "    S = np.dot(Y_cent.T, Y_cent)/Y.shape[0]\n",
    "    lambd, U = np.linalg.eig(S)\n",
    "\n",
    "    # Choose number of eigenvectors\n",
    "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
    "    l = np.sqrt(lambd[:q]-sigma2)\n",
    "    W = U[:, :q]*l[None, :]\n",
    "    return W, sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we may not wish to compute the eigenvectors of the\n",
    "covariance matrix directly. This is because it requires us to estimate\n",
    "the covariance, which involves a sum of squares term, before estimating\n",
    "the eigenvectors. We can estimate the eigenvectors directly either\n",
    "through [QR\n",
    "decomposition](http://en.wikipedia.org/wiki/QR_decomposition) or\n",
    "[singular value\n",
    "decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
    "We saw a similar issue arise when [computing the weights in a regression\n",
    "problem](./week3.ipynb), where we also wished to avoid computation of\n",
    "$\\latentMatrix^\\top\\latentMatrix$ (or in the case of [nonlinear\n",
    "regression with basis functions](./week4.ipynb)\n",
    "$\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}$).\n",
    "\n",
    "# Posterior for Principal Component Analysis\n",
    "\n",
    "Under the latent variable model justification for principal component\n",
    "analysis, we are normally interested in inferring something about the\n",
    "latent variables given the data. This is the distribution, $$\n",
    "p(\\latentVector_{i, :} | \\dataVector_{i, :})\n",
    "$$ for any given data point. Determining this density turns out to be\n",
    "very similar to the approach for determining the Bayesian posterior of\n",
    "$\\weightVector$ in Bayesian linear regression, only this time we place\n",
    "the prior density over $\\latentVector_{i, :}$ instead of\n",
    "$\\weightVector$. The posterior is proportional to the joint density as\n",
    "follows, $$\n",
    "p(\\latentVector_{i, :} | \\dataVector_{i, :}) \\propto p(\\dataVector_{i,\n",
    ":}|\\mappingMatrix, \\latentVector_{i, :}, \\noiseStd^2) p(\\latentVector_{i, :})\n",
    "$$ And as in the Bayesian linear regression case we first consider the\n",
    "log posterior, $$\n",
    "\\log\n",
    "p(\\latentVector_{i, :} | \\dataVector_{i, :}) = \\log p(\\dataVector_{i, :}|\\mappingMatrix,\n",
    "\\latentVector_{i, :}, \\noiseStd^2) + \\log p(\\latentVector_{i, :}) + \\text{const}\n",
    "$$ where the constant is not dependent on $\\latentVector$. As before we\n",
    "collect the quadratic terms in $\\latentVector_{i, :}$ and we assemble\n",
    "them into a Gaussian density over $\\latentVector$. $$\n",
    "\\log p(\\latentVector_{i, :} | \\dataVector_{i, :}) =\n",
    "-\\frac{1}{2\\noiseStd^2} (\\dataVector_{i, :} - \\mappingMatrix\\latentVector_{i,\n",
    ":})^\\top(\\dataVector_{i, :} - \\mappingMatrix\\latentVector_{i, :}) - \\frac{1}{2}\n",
    "\\latentVector_{i, :}^\\top \\latentVector_{i, :} + \\text{const}\n",
    "$$\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Multiply out the terms in the brackets. Then collect the quadratic term\n",
    "and the linear terms together. Show that the posterior has the form $$\n",
    "\\latentVector_{i, :} | \\mappingMatrix \\sim \\gaussianSamp{\\meanVector_x}{\\covarianceMatrix_x}\n",
    "$$ where $$\n",
    "\\covarianceMatrix_x = \\left(\\noiseStd^{-2}\n",
    "\\mappingMatrix^\\top\\mappingMatrix + \\eye\\right)^{-1}\n",
    "$$ and $$\n",
    "\\meanVector_x\n",
    "= \\covarianceMatrix_x \\noiseStd^{-2}\\mappingMatrix^\\top \\dataVector_{i, :} \n",
    "$$ Compare this to the posterior for the Bayesian linear regression from\n",
    "last week, do they have similar forms? What matches and what differs?\n",
    "\n",
    "*30 marks*\n",
    "\n",
    "### Write your answer to Question 3 here\n",
    "\n",
    "## Python Implementation of the Posterior\n",
    "\n",
    "Now let's implement the system in code.\n",
    "\n",
    "### Question\n",
    "\n",
    "Use the values for $\\mappingMatrix$ and $\\noiseStd^2$ you have computed,\n",
    "along with the data set $\\dataMatrix$ to compute the posterior density\n",
    "over $\\latentMatrix$. Write a function of the form\n",
    "\n",
    "-   marks\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for your answer to Question  in this box\n",
    "# provide the answers so that the code runs correctly otherwise you will loose marks!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python mu\\_x, C\\_x = posterior(Y, W, sigma2)} where `mu_x` and `C_x` are\n",
    "the posterior mean and posterior covariance for the given $\\dataMatrix$.\n",
    "\n",
    "Don't forget to subtract the mean of the data `Y` inside your function\n",
    "before computing the posterior: remember we assumed at the beginning of\n",
    "our analysis that the data had been centred (i.e. the mean was\n",
    "removed).}{4}{20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 Answer Code\n",
    "# Write code for you answer to this question in this box\n",
    "# Do not delete these comments, otherwise you will get zero for this answer.\n",
    "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "def posterior(Y, W, sigma2):\n",
    "    Y_cent = Y - Y.mean(0)\n",
    "    # Compute posterior over X\n",
    "    C_x = \n",
    "    mu_x = \n",
    "    return mu_x, C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerically Stable and Efficient Version\n",
    "\n",
    "Just as we saw for [linear regression](./week3.ipynb) and [regression\n",
    "with basis functions](./week4) computation of a matrix such as\n",
    "$\\dataMatrix^\\top\\dataMatrix$ (or its centred version) can be a bad idea\n",
    "in terms of loss of numerical accuracy. Fortunately, we can find the\n",
    "eigenvalues and eigenvectors of the matrix $\\dataMatrix^\\top\\dataMatrix$\n",
    "without direct computation of the matrix. This can be done with the\n",
    "[*singular value\n",
    "decomposition*](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
    "The singular value decompsition takes a matrix, $\\mathbf{Z}$ and\n",
    "represents it in the form, $$\n",
    "\\mathbf{Z} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\n",
    "$$ where $\\mathbf{U}$ is a matrix of orthogonal vectors in the columns,\n",
    "meaning $\\mathbf{U}^\\top\\mathbf{U} = \\eye$. It has the same number of\n",
    "rows and columns as $\\mathbf{Z}$. The matrices $\\mathbf{\\Lambda}$ and\n",
    "$\\mathbf{V}$ are both square with dimensionality given by the number of\n",
    "columns of $\\mathbf{Z}$. The matrix $\\mathbf{\\Lambda}$ is *diagonal* and\n",
    "$\\mathbf{V}$ is an orthogonal matrix so\n",
    "$\\mathbf{V}^\\top\\mathbf{V} = \\mathbf{V}\\mathbf{V}^\\top = \\eye$. The\n",
    "eigenvalues of the matrix $\\dataMatrix^\\top\\dataMatrix$ are then given\n",
    "by the singular values of the matrix $\\dataMatrix^\\top$ squared and the\n",
    "eigenvectors are given by $\\mathbf{U}$.\n",
    "\n",
    "## Solution for $\\mappingMatrix$\n",
    "\n",
    "Given the singular value decomposition of $\\dataMatrix$ then we have $$\n",
    "\\mappingMatrix =\n",
    "\\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top\n",
    "$$ where $\\mathbf{R}$ is an arbitrary rotation matrix. This implies that\n",
    "the posterior is given by $$\n",
    "\\covarianceMatrix_x =\n",
    "\\left[\\noiseStd^{-2}\\mathbf{R}\\mathbf{L}^2\\mathbf{R}^\\top + \\eye\\right]^{-1}\n",
    "$$ because $\\mathbf{U}^\\top \\mathbf{U} = \\eye$. Since, by convention, we\n",
    "normally take $\\mathbf{R} = \\eye$ to ensure that the principal\n",
    "components are orthonormal we can write $$\n",
    "\\covarianceMatrix_x = \\left[\\noiseStd^{-2}\\mathbf{L}^2 +\n",
    "\\eye\\right]^{-1}\n",
    "$$ which implies that $\\covarianceMatrix_x$ is actually diagonal with\n",
    "elements given by $$\n",
    "c_i = \\frac{\\noiseStd^2}{\\noiseStd^2 + \\ell^2_i}\n",
    "$$ and allows us to write $$\n",
    "\\meanVector_x = [\\mathbf{L}^2 + \\noiseStd^2\n",
    "\\eye]^{-1} \\mathbf{L} \\mathbf{U}^\\top \\dataVector_{i, :}\n",
    "$$ $$\n",
    "\\meanVector_x = \\mathbf{D}\\mathbf{U}^\\top \\dataVector_{i, :}\n",
    "$$ where $\\mathbf{D}$ is a diagonal matrix with diagonal elements given\n",
    "by $d_{i} = \\frac{\\ell_i}{\\noiseStd^2 + \\ell_i^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic PCA algorithm using SVD\n",
    "def ppca(Y, q, center=True):\n",
    "    \"\"\"Probabilistic PCA through singular value decomposition\"\"\"\n",
    "    # remove mean\n",
    "    if center:\n",
    "        Y_cent = Y - Y.mean(0)\n",
    "    else:\n",
    "        Y_cent = Y\n",
    "        \n",
    "    # Comute singluar values, discard 'R' as we will assume orthogonal\n",
    "    U, sqlambd, _ = sp.linalg.svd(Y_cent.T,full_matrices=False)\n",
    "    lambd = (sqlambd**2)/Y.shape[0]\n",
    "    # Compute residual and extract eigenvectors\n",
    "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
    "    ell = np.sqrt(lambd[:q]-sigma2)\n",
    "    return U[:, :q], ell, sigma2\n",
    "\n",
    "def posterior(Y, U, ell, sigma2, center=True):\n",
    "    \"\"\"Posterior computation for the latent variables given the eigendecomposition.\"\"\"\n",
    "    if center:\n",
    "        Y_cent = Y - Y.mean(0)\n",
    "    else:\n",
    "        Y_cent = Y\n",
    "    C_x = np.diag(sigma2/(sigma2+ell**2))\n",
    "    d = ell/(sigma2+ell**2)\n",
    "    mu_x = np.dot(Y_cent, U)*d[None, :]\n",
    "    return mu_x, C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "For our first example we'll consider some motion capture data of a man\n",
    "breaking into a run. [Motion capture\n",
    "data](http://en.wikipedia.org/wiki/Motion_capture) involves capturing a\n",
    "3-d point cloud to represent a character, often by an underlying\n",
    "skeleton. For this data set, from Ohio State University, we have 54\n",
    "frame of motion capture, each frame containing 102 values, which are the\n",
    "3-d locations of 34 different points from the subjects skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.osu_run1()\n",
    "Y = data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded in we can examine the first two principal\n",
    "components as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2\n",
    "U, ell, sigma2 = ppca(Y, q)\n",
    "mu_x, C_x = posterior(Y, U, ell, sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here because the data is a time course, we have connected points that\n",
    "are neighbouring in time. This highlights the form of the run, which\n",
    "involves 3 paces. This projects in our low dimensional space to 3 loops.\n",
    "We can examin how much residual variance there is in the system by\n",
    "looking at `sigma2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot Navigation Example\n",
    "\n",
    "In the next example we will load in data from a robot navigation\n",
    "problem. The data consists of wireless access point strengths as\n",
    "recorded by a robot performing a loop around the University of\n",
    "Washington's Computer Science department in Seattle. The robot records\n",
    "all the wireless access points it can cache and stores their signal\n",
    "strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pods.datasets.robot_wireless()\n",
    "Y = data['Y']\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 215 observations of 30 different access points. In this case\n",
    "the model is suggesting that the access point signal strength should be\n",
    "linearly dependent on the location in the map. In other words we are\n",
    "expecting the access point strength for the $j$th access point at robot\n",
    "position $x_{i, :}$ to be represented by\n",
    "$y_{i, j} = \\weightVector_{j, :}^\\top \\latentVector_{i, :} + \\epsilon_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2\n",
    "U, ell, sigma2 = ppca(Y, q)\n",
    "mu_x, C_x = posterior(Y, U, ell, sigma2)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.plot(mu_x[:, 0], mu_x[:, 1], 'rx-')\n",
    "ax.set_title('Latent Variable: Robot Inferred Locations')\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "W = U*ell[None, :]\n",
    "ax.plot(W[:, 0], W[:, 1], 'bo')\n",
    "ax.set_title('Access Point Inferred Locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, ell, sigma2 = ppca(Y.T, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship to Matrix Factorization\n",
    "\n",
    "We can use the robot naviation example to realise that PCA (and factor\n",
    "analysis) are very reminiscient of the [*matrix factorization*\n",
    "example](./week2.ipynb) that we used for introducing objective\n",
    "functions. In that system we used slightly different notation,\n",
    "$\\mathbf{u}_{i, :}$ for *user* location in our metaphorical library and\n",
    "$\\mathbf{v}_{j, :}$ for *item* location in our metaphorical library. To\n",
    "see how these systems are somewhat analagous, now let us think about the\n",
    "user as the robot and the items as the wifi access points. We can plot\n",
    "the relative location of both. This process is known as \"SLAM\":\n",
    "simultaneous *localisation* and *mapping*. A latent variable model of\n",
    "the type we have developed is one way of performing SLAM. We have an\n",
    "estimate of the *landmarks* in the system (in this case WIFI access\n",
    "points) and we have an estimate of the robot position. These are\n",
    "analagous to the estimate of the user's position and the estimate of the\n",
    "items positions in the library. In the matrix factorisation example\n",
    "users are informing us what items they are 'close' to by expressing\n",
    "their preferences, in the robot localization example the robot is\n",
    "informing us what access point it is close to by measuring signal\n",
    "strength.\n",
    "\n",
    "From a personal perspective, I find this analogy quite comforting. I\n",
    "think it is very arguable that one of the mechanisms through which we\n",
    "(as humans) may have developed higher reasoning is through the need to\n",
    "navigate around our environment, identifying landmarks and associating\n",
    "them with our search for food. If such a system were to exist, the idea\n",
    "that it could be readily adapted to other domains such as categorising\n",
    "the nature of the different foodstuffs we were able to forage is\n",
    "intriguing.\n",
    "\n",
    "From an algorithmic perspective, we also can now realise that matrix\n",
    "factorization and latent variable modelling are effectively the same\n",
    "thing. The only difference is the objective function and our\n",
    "probabilistic (or lack of probabilistic) treatment of the variables. But\n",
    "the prediction functoin for both systems, $$\n",
    "f_{i, j} =\n",
    "\\mathbf{u}_{i, :}^\\top \\mathbf{v}_{j, :} \n",
    "$$ for matrix factorization or $$\n",
    "f_{i, j} = \\latentVector_{i, :}^\\top \\weightVector_{j, :} \n",
    "$$ for probabilistic PCA and factor analysis are the same.\n",
    "\n",
    "# Other Interpretations of PCA: Separating Model and Algorithm\n",
    "\n",
    "Since Hotelling first introduced his perspective on factor analysis as\n",
    "PCA there has been somewhat of a conflation of the idea of the model\n",
    "underlying PCA (for which it was very clear that Hotelling was inspired\n",
    "by Factor Analysis) and the algorithm that is used to fit that model:\n",
    "the eigenvalues and eigenvectors of the covariance matrix. The\n",
    "eigenvectors of an ellipsoid have been known since the middle of the\n",
    "19th century as the principal axes of the elipsoid, and they arise\n",
    "through the following additional ideas: seeking the orthogonal\n",
    "directions of *maximum variance* in a dataset. Pearson in 1901 arrived\n",
    "at the same algorithm driven by a desire to seek a *symmetric\n",
    "regression* between two covariate/response variables $x$ and $y$. He is,\n",
    "therefore, often credited with the invention of principal component\n",
    "analysis, but to me this seems disengenous. His aim was very different\n",
    "from Hotellings, it was just happened that the optimal solution for his\n",
    "model was coincident with that of Hotelling. The approach is also known\n",
    "as the [Karhunen Loeve\n",
    "Transform](http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem)\n",
    "in stochastic process theory and in classical multidimensional scaling\n",
    "the same operation can be shown to be minimising a particular objective\n",
    "function based on interpoint distances in the data and the latent space\n",
    "(see the section on Classical Multidimensional Scaling in [Mardia, Kent\n",
    "and\n",
    "Bibby](http://store.elsevier.com/Multivariate-Analysis/Kanti-%20Mardia/isbn-9780124712522/)).\n",
    "One of my own contributions to machine learning was deriving yet another\n",
    "model whose linear variant was solved by finding the principal subspace\n",
    "of the covariance matrix (an approach I termed dual probabilistic PCA or\n",
    "probabilistic principal coordinate analysis). Finally, the approach is\n",
    "sometimes referred to simply as singular value decomposition (SVD). The\n",
    "singular value decomposition of a data set has the following form, $$\n",
    "\\dataMatrix = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{U}^\\top\n",
    "$$ where $\\mathbf{V}\\in\\Re^{n\\times n}$ and\n",
    "$\\mathbf{U}^\\in \\Re^{p\\times p}$ are square orthogonal matrices and\n",
    "$\\mathbf{\\Lambda}^{n \\times p}$ is zero apart from its first $p$\n",
    "diagonal entries. Singularvalue decomposition gives a diagonalisation of\n",
    "the covariance matrix, because under the SVD we have $$\n",
    "\\dataMatrix^\\top\\dataMatrix =\n",
    "\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\\mathbf{V} \\boldsymbol{\\Lambda}\n",
    "\\mathbf{U}^\\top = \\mathbf{U}\\boldsymbol{\\Lambda}^2 \\mathbf{U}^\\top\n",
    "$$ where $\\boldsymbol{\\Lambda}^2$ is now the eigenvalues of the\n",
    "covariane matrix and $\\mathbf{U}$ are the eigenvectors. So performing\n",
    "the SVD can simply be seen as another approach to determining the\n",
    "principal components.\n",
    "\n",
    "## Separating Model and Algorithm\n",
    "\n",
    "I've given a fair amount of personal thought to this situation and my\n",
    "own opinion that this confusion about method arises because of a\n",
    "conflation of model and algorithm. The model of Hotelling, that which he\n",
    "termed principal component analysis, was really a variant of factor\n",
    "analysis, and it was unfortunate that he chose to rename it. However,\n",
    "the algorithm he derived was a very convenient way of optimising a\n",
    "(simplified) factor analysis, and it's therefore become very popular.\n",
    "The algorithm is also the optimal solution for many other models of the\n",
    "data, even some which might seem initally to be unrelated (e.g. seeking\n",
    "directions of maximum variance). It is only through the mathematics of\n",
    "this linear system (which also contains some intersting symmetries) that\n",
    "all these ides become related. However, as soon as we choose to\n",
    "non-linearise the system (e.g. through basis functions) we find that\n",
    "each of the non-linear intepretations we can derive for the different\n",
    "models each leads to a very different algorithm (if such an algorithm is\n",
    "possible). For example [principal\n",
    "curves](http://web.stanford.edu/~hastie/Papers/Principal_Curves.pdf) of\n",
    "Hastie and Stuezle attempt to non-linearise the maximum variance\n",
    "interpretation, [kernel\n",
    "PCA](http://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
    "of Schoelkopf, Smola and Mueller uses basis functions to form the\n",
    "eigenvalue problem in a nonlinear space, and my own work in this area\n",
    "[non-linearises the dual probabilistic\n",
    "PCA](http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf).\n",
    "\n",
    "My conclusion is that when you are doing machine learning you should\n",
    "always have it clear in your mind what your *model* is and what your\n",
    "*algorithm* is. You can recognise your model because it normally\n",
    "contains a prediction function and an objective function. The algorithm\n",
    "on the other hand is the sequence of steps you implement on the computer\n",
    "to solve for the parameters of this model. For efficient implementation,\n",
    "we often modify our model to allow for faster algorithms, and this is a\n",
    "perfectly valid pragmatist's approach, so conflation of model and\n",
    "algorithm is not always a bad thing. But for clarity of thinking and\n",
    "understanding it is necessary to maintain the separation and to maintain\n",
    "a handle on when and why we perform the conflation.\n",
    "\n",
    "# PCA in Practice\n",
    "\n",
    "Principal component analysis is so effective in practice that there has\n",
    "almost developed a mini-industry in renaming the method itself (which is\n",
    "ironic, given its origin). In particular [Latent Semantic\n",
    "Indexing](http://en.wikipedia.org/wiki/Latent_semantic_indexing) in text\n",
    "processing is simply PCA on a particular representation of the term\n",
    "frequencies of the document. There is a particular fad to rename the\n",
    "eigenvectors after the nature of the data you are examining, perhaps\n",
    "initially triggered by [Turk and\n",
    "Pentland's](http://www.face-rec.org/algorithms/PCA/jcn.pdf) paper on\n",
    "eigenfaces, but also with\n",
    "[eigenvoices](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1_2007_8/kuhn-%20junqua-eigenvoice-icslp1998.pdf)\n",
    "and [eigengenes](http://www.biomedcentral.com/1752-0509/1/54). This\n",
    "seems to be an instantiation of a wider, and hopefully subconcious,\n",
    "tendency in academia to attempt to differentiate one idea from the same\n",
    "idea in related fields in order to emphasise the novelty. The\n",
    "unfortunate result is somewhat of a confusing literature for relatively\n",
    "simple model. My recommendations would be as follows. If you have\n",
    "multivariate data, applying some form of principal component would seem\n",
    "to be a very good idea as a first step. Even if you intend to later\n",
    "perform classification or regression on your data, it can give you\n",
    "understanding of the structure of the underlying data and help you to\n",
    "develop your intuitions about the nature of your data. Intelligent\n",
    "plotting and interaction with your data is always a good think, and for\n",
    "high dimensional data that means that you need some way of\n",
    "visualisation, PCA is typically a good starting point.\n",
    "\n",
    "# Marginal Likelihood\n",
    "\n",
    "We have developed the posterior density over the latent variables given\n",
    "the data and the parameters, and due to symmetries in the underlying\n",
    "prediction function, it has a very similar form to its sister density,\n",
    "the posterior of the weights given the data from Bayesian regression.\n",
    "Two key differences are as follows. If we were to do a Bayesian multiple\n",
    "output regression we would find that the marginal likelihood of the data\n",
    "is independent across the features and correlated across the data, $$\n",
    "p(\\dataMatrix|\\latentMatrix)\n",
    "= \\prod_{j=1}^p \\gaussianDist{\\dataVector_{:, j}}{\\zerosVector}{\n",
    "\\alpha\\latentMatrix\\latentMatrix^\\top + \\noiseStd^2 \\eye}\n",
    "$$ where $\\dataVector_{:, j}$ is a column of the data matrix and the\n",
    "independence is across the *features*, in probabilistic PCA the marginal\n",
    "likelihood has the form, $$\n",
    "p(\\dataMatrix|\\mappingMatrix) = \\prod_{i=1}^\\numData \\gaussianDist{\\dataVector_{i,\n",
    ":}}{\\zerosVector}{\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye}\n",
    "$$ where $\\dataVector_{i, :}$ is a row of the data matrix $\\dataMatrix$\n",
    "and the independence is across the data points.\n",
    "\n",
    "# Computation of the Log Likelihood\n",
    "\n",
    "The quality of the model can be assessed using the log likelihood of\n",
    "this Gaussian form. $$\n",
    "\\log p(\\dataMatrix|\\mappingMatrix) = -\\frac{\\numData}{2} \\log \\left|\n",
    "\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye \\right| -\\frac{1}{2}\n",
    "\\sum_{i=1}^\\numData \\dataVector_{i, :}^\\top \\left(\\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2\n",
    "\\eye\\right)^{-1} \\dataVector_{i, :} +\\text{const}\n",
    "$$ but this can be computed more rapidly by exploiting the low rank form\n",
    "of the covariance covariance,\n",
    "$\\covarianceMatrix = \\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye$\n",
    "and the fact that\n",
    "$\\mappingMatrix = \\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top$. Specifically, we\n",
    "first use the decomposition of $\\mappingMatrix$ to write: $$\n",
    "-\\frac{\\numData}{2} \\log \\left| \\mappingMatrix\\mappingMatrix^\\top + \\noiseStd^2 \\eye \\right|\n",
    "= -\\frac{\\numData}{2} \\sum_{i=1}^q \\log (\\ell_i^2 + \\noiseStd^2) - \\frac{n(p-q)}{2}\\log\n",
    "\\noiseStd^2,\n",
    "$$ where $\\ell_i$ is the $i$th diagonal element of $\\mathbf{L}$. Next,\n",
    "we use the [Woodbury matrix\n",
    "identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) which\n",
    "allows us to write the inverse as a quantity which contains another\n",
    "inverse in a smaller matrix: $$\n",
    "(\\noiseStd^2 \\eye + \\mappingMatrix\\mappingMatrix^\\top)^{-1} =\n",
    "\\noiseStd^{-2}\\eye-\\noiseStd^{-4}\\mappingMatrix{\\underbrace{(\\eye+\\noiseStd^{-2}\\mappingMatrix^\\top\\mappingMatrix)}_{\\covarianceMatrix_x}}^{-1}\\mappingMatrix^\\top\n",
    "$$ So, it turns out that the original inversion of the $p \\times p$\n",
    "matrix can be done by forming a quantity which contains the inversion of\n",
    "a $q \\times q$ matrix which, moreover, turns out to be the quantity\n",
    "$\\covarianceMatrix_x$ of the posterior.\n",
    "\n",
    "Now, we put everything together to obtain: $$\n",
    "\\log p(\\dataMatrix|\\mappingMatrix) = -\\frac{\\numData}{2} \\sum_{i=1}^q\n",
    "\\log (\\ell_i^2 + \\noiseStd^2)\n",
    "- \\frac{n(p-q)}{2}\\log \\noiseStd^2 - \\frac{1}{2} \\trace{\\dataMatrix^\\top \\left(\n",
    "\\noiseStd^{-2}\\eye-\\noiseStd^{-4}\\mappingMatrix \\covarianceMatrix_x\n",
    "\\mappingMatrix^\\top \\right) \\dataMatrix} + \\text{const},\n",
    "$$ where we used the fact that a scalar sum can be written as\n",
    "$\\sum_{i=1}^\\numData \\dataVector_{i,:}^\\top \\kernelMatrix \\dataVector_{i,:} = \\trace{\\dataMatrix^\\top \\kernelMatrix \\dataMatrix}$,\n",
    "for any matrix $\\kernelMatrix$ of appropriate dimensions. We now use the\n",
    "properties of the trace\n",
    "$\\trace{\\mathbf{A}+\\mathbf{B}}=\\trace{\\mathbf{A}}+\\trace{\\mathbf{B}}$\n",
    "and $\\trace{c \\mathbf{A}} = c \\trace{\\mathbf{A}}$, where $c$ is a scalar\n",
    "and $\\mathbf{A},\\mathbf{B}$ matrices of compatible sizes. Therefore, the\n",
    "final log likelihood takes the form: $$\n",
    "\\log p(\\dataMatrix|\\mappingMatrix) = -\\frac{\\numData}{2}\n",
    "\\sum_{i=1}^q \\log (\\ell_i^2 + \\noiseStd^2) - \\frac{\\numData(p-q)}{2}\\log \\noiseStd^2 -\n",
    "\\frac{\\noiseStd^{-2}}{2} \\trace{\\dataMatrix^\\top \\dataMatrix}\n",
    "+\\frac{\\noiseStd^{-4}}{2} \\trace{\\mathbf{B}\\covarianceMatrix_x\\mathbf{B}^\\top} +\n",
    "\\text{const}\n",
    "$$ where we also defined $\\mathbf{B}=\\dataMatrix^\\top\\mappingMatrix$.\n",
    "Finally, notice that\n",
    "$\\trace{\\dataMatrix\\dataMatrix^\\top}=\\trace{\\dataMatrix^\\top\\dataMatrix}$\n",
    "can be computed faster as the sum of all the elements of\n",
    "$\\dataMatrix\\circ\\dataMatrix$, where $\\circ$ denotes the element-wise\n",
    "(or [Hadamard](http://en.wikipedia.org/wiki/Hadamard_product_(matrices))\n",
    "product.\n",
    "\n",
    "## Reconstruction of the Data\n",
    "\n",
    "Given any posterior projection of a data point, we can replot the\n",
    "original data as a function of the input space.\n",
    "\n",
    "We will now try to reconstruct the motion capture figure form some\n",
    "different places in the latent plot.\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Project the motion capture data onto its principal components, and then\n",
    "use the *mean posterior estimate* to reconstruct the data from the\n",
    "latent variables at the data points. Use two latent dimensions. What is\n",
    "the sum of squares error for the reconstruction?\n",
    "\n",
    "*25 marks*\n",
    "\n",
    "### Write your answer to Question 5 here\n",
    "\n",
    "## Other Data Sets to Explore\n",
    "\n",
    "Below there are a few other data sets from `pods` you might want to\n",
    "explore with PCA. Both of them have $p$\\>$n$ so you need to consider how\n",
    "to do the larger eigenvalue probleme efficiently without large demands\n",
    "on computer memory.\n",
    "\n",
    "The data is actually quite high dimensional, and solving the eigenvalue\n",
    "problem in the high dimensional space can take some time. At this point\n",
    "we turn to a neat trick, you don't have to solve the full eigenvalue\n",
    "problem in the $\\dataDim\\times \\dataDim$ covariance, you can choose\n",
    "instead to solve the related eigenvalue problem in the\n",
    "$\\numData \\times \\numData$ space, and in this case $\\numData=200$ which\n",
    "is much smaller than $\\dataDim$.\n",
    "\n",
    "The original eigenvalue problem has the form $$\n",
    "\\dataMatrix^\\top\\dataMatrix \\mathbf{U} = \\mathbf{U}\\boldsymbol{\\Lambda}\n",
    "$$ But if we premultiply by $\\dataMatrix$ then we can solve, $$\n",
    "\\dataMatrix\\dataMatrix^\\top\\dataMatrix \\mathbf{U} = \\dataMatrix\\mathbf{U}\\boldsymbol{\\Lambda}\n",
    "$$ but it turns out that we can write $$\n",
    "\\mathbf{U}^\\prime = \\dataMatrix \\mathbf{U} \\Lambda^{\\frac{1}{2}}\n",
    "$$ where $\\mathbf{U}^\\prime$ is an orthorormal matrix because $$\n",
    "\\left.\\mathbf{U}^\\prime\\right.^\\top\\mathbf{U}^\\prime = \\Lambda^{-\\frac{1}{2}}\\mathbf{U}\\dataMatrix^\\top\\dataMatrix \\mathbf{U} \\Lambda^{-\\frac{1}{2}}\n",
    "$$ and since $\\mathbf{U}$ diagonalises $\\dataMatrix^\\top\\dataMatrix$, $$\n",
    "\\mathbf{U}\\dataMatrix^\\top\\dataMatrix \\mathbf{U} = \\Lambda\n",
    "$$ then $$\n",
    "\\left.\\mathbf{U}^\\prime\\right.^\\top\\mathbf{U}^\\prime = \\eye\n",
    "$$\n",
    "\n",
    "## Olivetti Faces\n",
    "\n",
    "You too can create your own eigenfaces. In this example we load in the\n",
    "'Olivetti Face' data set, a small data set of 200 faces from the\n",
    "[Olivetti Research\n",
    "Laboratory](http://en.wikipedia.org/wiki/Olivetti_Research_Laboratory).\n",
    "Below we load in the data and display an image of the second face in the\n",
    "data set (i.e., indexed by 1).\n",
    "\n",
    "Note that to display the face we had to reshape the appropriate row of\n",
    "the data matrix. This is because the images are turned into vectors by\n",
    "stacking columns of the image on top of each other to form a vector. The\n",
    "operation\n",
    "\n",
    "`im = np.reshape(Y[1, :].flatten(), (64, 64)).T}`\n",
    "\n",
    "recovers the original image into a matrix `im` by using the `np.reshape`\n",
    "function to return the vector to a matrix.\n",
    "\n",
    "## Visualizing the Eigenvectors\n",
    "\n",
    "Each retained eigenvector is stored in the $j$th column of $\\mathbf{U}$.\n",
    "Each of these eigenvectors is associated with particular directions of\n",
    "variation in the original data. Principal component analysis implies\n",
    "that we can reconstruct any face by using a weighted sum of these\n",
    "eigenvectors where the weights for each face are given by the relevant\n",
    "vector of the latent variables, $\\latentVector_{i, :}$ and the diagonal\n",
    "elements of the matrix $\\mathbf{L}$. We can visualize the eigenvectors\n",
    "$\\mathbf{U}$ as images by performing the same reshape operation we used\n",
    "to recover the image associated with a data point above. Below we do\n",
    "this for the first nine eigenvectors of the Olivetti Faces data.\n",
    "\n",
    "## Reconstruction\n",
    "\n",
    "We can now attempt to reconstruct a given training point from these\n",
    "eigenvectors. As we mentioned above, the reconstruction is dependent on\n",
    "the value of the latent variable and the weights from the matrix\n",
    "$\\mathbf{L}$. First let's compute the value of the latent variables for\n",
    "the point we want to construct. Then we'll use them to compute the\n",
    "weightings of each of the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x, C_x = posterior(Y, U, ell, sigma2)\n",
    "reconstruction_weights = mu_x[display_index, :]*ell\n",
    "print(reconstruction_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector of reconstruction weights is applied to the 'template\n",
    "images' given by the eigenvectors to give us our reconstruction. Below\n",
    "we weight these templates and combine to form the reconstructed image,\n",
    "and show the comparison to the original image.\n",
    "\n",
    "The quality of the reconstruction is a bit blurry, it can be improved by\n",
    "increasing the number of template images used (i.e. increasing the\n",
    "*latent dimensionality*).\n",
    "\n",
    "## Gene Expression\n",
    "\n",
    "Each of the cells in your body stores your entire genetic code in your\n",
    "DNA, but at any one moment it is only 'expressing' a small portion of\n",
    "that code. Your cells are mainly constructed of protein, and these\n",
    "proteins are manufactured by first transcribing the DNA to RNA and then\n",
    "translating the RNA to protein. If the DNA is the cells hard drive, then\n",
    "one role of the RNA is to act like a cache of data that is being read\n",
    "from the hard drive at any one time. Gene expression arrays allow us to\n",
    "measure the quantity of different types of RNA in the cell, effectively\n",
    "analyzing what's in the cache (although we have to destroy the cell or\n",
    "the tissue to access it). A gene expression experiment often consists of\n",
    "a time course or a series of experiments that characterise the gene\n",
    "expression of cells at any given time.\n",
    "\n",
    "We will now load in one of the earliest gene expression data sets from a\n",
    "[1998 paper by Spellman et\n",
    "al.](http://www.ncbi.nlm.nih.gov/pubmed/9843569), it consists of gene\n",
    "expression measurements of over six thousand genes in a range of\n",
    "conditions for brewer's yeast. The experiment was designed for\n",
    "understanding the cell cycle of the genes. The expectation is that there\n",
    "should be oscillating signals inside the cell.\n",
    "\n",
    "First we extract the principale components of the gene expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data and replace missing values with zero\n",
    "data=pods.datasets.spellman_yeast_cdc15()\n",
    "Y = data['Y'].fillna(0)\n",
    "q = 5\n",
    "U, ell, sigma2 = ppca(Y, q)\n",
    "mu_x, C_x = posterior(Y, U, ell, sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking through, we find that there is indeed a latent variable\n",
    "that appears to oscilate at approximately the right frequency. The 4th\n",
    "latent dimension (`index=3`) can be plotted across the time course as\n",
    "follows.\n",
    "\n",
    "To reveal an oscillating shape. We can see which genes correspond to\n",
    "this shape by examining the associated column of $\\mathbf{U}$. Let's\n",
    "augment our data matrix with this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = Y.T\n",
    "gene_list['oscilation'] = np.sqrt(U[:, 3]**2)\n",
    "gene_list.sort(columns='oscilation', ascending=False).index[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look up the first three genes in this list which now ranks the\n",
    "genes according to how strongly they respond to the fourth latent\n",
    "dimension. [The NCBI gene database](http://www.ncbi.nlm.nih.gov/gene/)\n",
    "allows us to search for the function of these genes. Looking at the\n",
    "function of the four genes that respond most strongly to the third\n",
    "latent variable they are all genes that encode\n",
    "[histone](http://en.wikipedia.org/wiki/Histone) proteins. The histone is\n",
    "thesupport scaffold for the DNA that ensures it folds correctly within\n",
    "the cell creating the nucleosomes. It seems to make sense that\n",
    "production of histone proteins should be strongly correlated with the\n",
    "cell cycle, as when the cell divides it needs to create a large number\n",
    "of histone proteins for creating the duplicated nucleosomes. The\n",
    "relevant links giving the descriptions of each gene given here:\n",
    "[YDR224C](http://www.ncbi.nlm.nih.gov/gene/851810),\n",
    "[YDR225W](http://www.ncbi.nlm.nih.gov/gene/851811),\n",
    "[YBL003C](http://www.ncbi.nlm.nih.gov/gene/852283) and\n",
    "[YNL030W](http://www.ncbi.nlm.nih.gov/gene/855701)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
