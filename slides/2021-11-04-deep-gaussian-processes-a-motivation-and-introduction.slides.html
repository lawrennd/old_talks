<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2021-11-04">
  <title>Deep Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="../assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Deep Gaussian Processes</h1>
  <p class="subtitle" style="text-align:center">A Motivation and Introduction</p>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2021-11-04</time></p>
  <p class="venue" style="text-align:center">LG Distinguished Talk</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--https://twitter.com/demishassabis/status/1453794436056502274?s=20 -->
<!-- SECTION Introduction -->
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
</section>
<section id="the-fourth-industrial-revolution" class="slide level2">
<h2>The Fourth Industrial Revolution</h2>
<ul>
<li>The first to be named before it happened.</li>
</ul>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level2">
<h2>What is Machine Learning?</h2>
</section>
<section id="what-is-machine-learning-1" class="slide level2">
<h2>What is Machine Learning?</h2>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-2" class="slide level2">
<h2>What is Machine Learning?</h2>
<p><span class="math display">\[\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span class="math inline">\(f(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span class="math inline">\(E(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//gp/mackay-baby.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="structure-of-priors" class="slide level2">
<h2>Structure of Priors</h2>
<p>MacKay: NeurIPS Tutorial 1997 “Have we thrown out the baby with the bathwater?” <span class="citation" data-cites="MacKay:gpintroduction98">(Published as MacKay, n.d.)</span></p>
</section>
<section id="deep-neural-network" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="deep-neural-network-1" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="mathematically" class="slide level2">
<h2>Mathematically</h2>
<p><span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{ h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{ h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
</section>
<section id="overfitting" class="slide level2">
<h2>Overfitting</h2>
<ul>
<li><p>Potential problem: if number of nodes in two adjacent layers is big, corresponding <span class="math inline">\(\mathbf{W}\)</span> is also very big and there is the potential to overfit.</p></li>
<li><p>Proposed solution: “dropout.”</p></li>
<li><p>Alternative solution: parameterize <span class="math inline">\(\mathbf{W}\)</span> with its SVD. <span class="math display">\[
\mathbf{W}= \mathbf{U}\boldsymbol{ \Lambda}\mathbf{V}^\top
\]</span> or <span class="math display">\[
\mathbf{W}= \mathbf{U}\mathbf{V}^\top
\]</span> where if <span class="math inline">\(\mathbf{W}\in \Re^{k_1\times k_2}\)</span> then <span class="math inline">\(\mathbf{U}\in \Re^{k_1\times q}\)</span> and <span class="math inline">\(\mathbf{V}\in \Re^{k_2\times q}\)</span>, i.e. we have a low rank matrix factorization for the weights.</p></li>
</ul>
</section>
<section id="low-rank-approximation" class="slide level2">
<h2>Low Rank Approximation</h2>
<div class="figure">
<div id="low-rank-mapping-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//wisuvt.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pictorial representation of the low rank form of the matrix <span class="math inline">\(\mathbf{W}\)</span>.
</aside>
</section>
<section id="bottleneck-layers-in-deep-neural-networks" class="slide level2">
<h2>Bottleneck Layers in Deep Neural Networks</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-nn-bottleneck1.svg" width="60%" style=" ">
</object>
</section>
<section id="deep-neural-network-2" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-nn-bottleneck2.svg" width="60%" style=" ">
</object>
</section>
<section id="mathematically-1" class="slide level2">
<h2>Mathematically</h2>
<p>The network can now be written mathematically as <span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{ h}_{1}\\
  \mathbf{ h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{ h}_{2}\\
  \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{ z}_{3}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4^\top\mathbf{ h}_{3}.
\end{align}
\]</span></p>
</section>
<section id="a-cascade-of-neural-networks" class="slide level2">
<h2>A Cascade of Neural Networks</h2>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4 ^\top \mathbf{ z}_{3}
\end{align}
\]</span></p>
</section>
<section id="cascade-of-gaussian-processes" class="slide level2">
<h2>Cascade of Gaussian Processes</h2>
<ul>
<li><p>Replace each neural network with a Gaussian process <span class="math display">\[
\begin{align}
\mathbf{ z}_{1} &amp;= \mathbf{ f}_1\left(\mathbf{ x}\right)\\
\mathbf{ z}_{2} &amp;= \mathbf{ f}_2\left(\mathbf{ z}_{1}\right)\\
\mathbf{ z}_{3} &amp;= \mathbf{ f}_3\left(\mathbf{ z}_{2}\right)\\
\mathbf{ y}&amp;= \mathbf{ f}_4\left(\mathbf{ z}_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to infinity.</p></li>
</ul>
<!-- SECTION Deep Learning -->
</section>
<section id="deep-learning" class="slide level2">
<h2>Deep Learning</h2>
<!-- No slide titles in this context -->
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p><span class="fragment fade-in"><small>Outline of the DeepFace architecture. A front-end of a single convolution-pooling-convolution filtering on the rectified input, followed by three locally-connected layers and two fully-connected layers. Color illustrates feature maps produced at each layer. The net includes more than 120 million parameters, where more than 95% come from the local and fully connected.</small></span></p>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The DeepFace architecture <span class="citation" data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>, visualized through colors to represent the functional mappings at each layer. There are 120 million parameters in the model.
</aside>
<div style="text-align:right">
<small>Source: DeepFace <span class="citation" data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small>
</div>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Deep learning models are composition of simple functions. We can think of a pinball machine as an analogy. Each layer of pins corresponds to one of the layers of functions in the model. Input data is represented by the location of the ball from left to right when it is dropped in from the top. Output class comes from the position of the ball as it leaves the pins at the bottom.
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
At initialization, the pins, which represent the parameters of the function, aren’t in the right place to bring the balls to the correct decisions.
</aside>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
After learning the pins are now in the right place to bring the balls to the correct decisions.
</aside>
</section>
<section id="mathematically-2" class="slide level2">
<h2>Mathematically</h2>
<ul>
<li><p>Composite <em>multivariate</em> function</p>
<p><span class="math display">\[
\mathbf{g}(\mathbf{ x})=\mathbf{ f}_5(\mathbf{ f}_4(\mathbf{ f}_3(\mathbf{ f}_2(\mathbf{ f}_1(\mathbf{ x}))))).
\]</span></p></li>
</ul>
</section>
<section id="equivalent-to-markov-chain" class="slide level2">
<h2>Equivalent to Markov Chain</h2>
<ul>
<li>Composite <em>multivariate</em> function <span class="math display">\[
p(\mathbf{ y}|\mathbf{ x})= p(\mathbf{ y}|\mathbf{ f}_5)p(\mathbf{ f}_5|\mathbf{ f}_4)p(\mathbf{ f}_4|\mathbf{ f}_3)p(\mathbf{ f}_3|\mathbf{ f}_2)p(\mathbf{ f}_2|\mathbf{ f}_1)p(\mathbf{ f}_1|\mathbf{ x})
\]</span></li>
</ul>
<div class="figure">
<div id="deep-markov-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-markov.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Probabilistically the deep Gaussian process can be represented as a Markov chain. Indeed they can even be analyzed in this way <span class="citation" data-cites="Dunlop:deep2017">(Dunlop et al., n.d.)</span>.
</aside>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-markov-vertical.svg" width="7%" style=" ">
</object>
</div>
</div>
<aside class="notes">
More usually deep probabilistic models are written vertically rather than horizontally as in the Markov chain.
</aside>
</section>
<section id="why-composition" class="slide level2">
<h2>Why Composition?</h2>
<ul>
<li><p>Gaussian processes give priors over functions.</p></li>
<li><p>Elegant properties:</p>
<ul>
<li>e.g. <em>Derivatives</em> of process are also Gaussian distributed (if they exist).</li>
</ul></li>
<li><p>For particular covariance functions they are ‘universal approximators,’ i.e. all functions can have support under the prior.</p></li>
<li><p>Gaussian derivatives might ring alarm bells.</p></li>
<li><p>E.g. a priori they don’t believe in function ‘jumps.’</p></li>
</ul>
</section>
<section id="stochastic-process-composition" class="slide level2">
<h2>Stochastic Process Composition</h2>
<ul>
<li><p>From a process perspective: <em>process composition</em>.</p></li>
<li><p>A (new?) way of constructing more complex <em>processes</em> based on simpler components.</p></li>
</ul>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-markov-vertical.svg" width style=" ">
</object>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-side-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/deep-markov-vertical-side.svg" width="15%" style=" ">
</object>
</div>
</div>
<aside class="notes">
More generally we aren’t constrained by the Markov chain. We can design structures that respect our belief about the underlying conditional dependencies. Here we are adding a side note from the chain.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//dimred/nonlinear-mapping-3d-plot.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two dimensional manifold.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//dimred/nonlinear-mapping-2d-plot.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate independent functions. Each point can be mapped exactly through the mappings.
</aside>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a very non Gaussian output. Here the output is multimodal.
</aside>
</section>
<section id="standard-variational-approach-fails" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Standard variational bound has the form: <span class="math display">\[
\mathcal{L}= \left\langle\log p(\mathbf{ y}|\mathbf{Z})\right\rangle_{q(\mathbf{Z})} + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
</section>
<section id="standard-variational-approach-fails-1" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Requires expectation of <span class="math inline">\(\log p(\mathbf{ y}|\mathbf{Z})\)</span> under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span class="math display">\[
\begin{align}
\log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{ y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log \det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}} -\frac{n}{2}\log 2\pi
\end{align}
\]</span> <span class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}\)</span> is dependent on <span class="math inline">\(\mathbf{Z}\)</span> and it appears in the inverse.</li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Consider collapsed variational bound, <span class="fragment" data-fragment-index="1"><small><span class="math display">\[
  p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{ f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment" data-fragment-index="2"><small><span class="math display">\[
  p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment" data-fragment-index="3"><small><span class="math display">\[
    \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right) p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span></li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm-1" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Apply variational lower bound to the inner integral. <small><span class="math display">\[
  \begin{align}
  \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right) p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp; \left\langle\sum_{i=1}^n\log  c_i\right\rangle_{q(\mathbf{Z})}\\ &amp; +\left\langle\log\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u}, \mathbf{Z})},\sigma^2\mathbf{I}\right)\right\rangle_{q(\mathbf{Z})}\\&amp; + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
  \end{align}
\]</span></small></li>
<li>Which is analytically tractable for Gaussian <span class="math inline">\(q(\mathbf{Z})\)</span> and some covariance functions.</li>
</ul>
</section>
<section id="required-expectations" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>Need expectations under <span class="math inline">\(q(\mathbf{Z})\)</span> of: <small><span class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{ u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i, \mathbf{ u}}\right]
\]</span></small> and <small><span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log 2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f}, \mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{ u}\right)^2
\]</span></small></li>
</ul>
</section>
<section id="required-expectations-1" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>This requires the expectations <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\right\rangle_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{ f}}\right\rangle_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance functions <span class="citation" data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or through sampling <span class="citation" data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015; Salimbeni and Deisenroth, 2017)</span>.</li>
</ul>
</section>
<section id="see-also" class="slide level2">
<h2>See also …</h2>
<ul>
<li>MAP approach <span class="citation" data-cites="Lawrence:hgplvm07">(Lawrence and Moore, 2007)</span>.</li>
<li>Hamiltonian Monte Carlo approach <span class="citation" data-cites="Havasi:deepgp18">(Havasi et al., 2018)</span>.</li>
<li>Expectation Propagation approach <span class="citation" data-cites="Bui:deep16">(Bui et al., 2016)</span>.</li>
</ul>
</section>
<section id="neural-networks" class="slide level2">
<h2>Neural Networks</h2>
<div class="figure">
<div id="neural-network-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//deepgp/neural-network-uncertainty.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Even the latest work on Bayesian neural networks has severe problems handling uncertainty. In this example, <span class="citation" data-cites="Izmailov:subspace19">(Izmailov et al., 2019)</span>, methods even fail to interpolate through the data correctly or provide well calibrated error bars in regions where data is observed.
</aside>
<div style="text-align:right">
<span class="citation" data-cites="Izmailov:subspace19">Izmailov et al. (2019)</span>
</div>
</section>
<section id="deep-gaussian-processes" class="slide level2">
<h2>Deep Gaussian Processes</h2>
<ul>
<li>Deep architectures allow abstraction of features <span class="citation" data-cites="Bengio:deep09 Hinton:fast06 Salakhutdinov:quantitative08">(Bengio, 2009; Hinton and Osindero, 2006; Salakhutdinov and Murray, n.d.)</span></li>
<li>We use variational approach to stack GP models.</li>
</ul>
</section>
<section id="stacked-pca" class="slide level2">
<h2>Stacked PCA</h2>
<script>
showDivs(0, 'stack-pca-sample');
</script>
<p><small></small> <input id="range-stack-pca-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-pca-sample')" oninput="setDivs('stack-pca-sample')"> <button onclick="plusDivs(-1, 'stack-pca-sample')">❮</button> <button onclick="plusDivs(1, 'stack-pca-sample')">❯</button></p>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-pca-sample-0.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-pca-sample-1.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-pca-sample-2.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-pca-sample-3.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-pca-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="stacked-gp" class="slide level2">
<h2>Stacked GP</h2>
<script>
showDivs(0, 'stack-gp-sample');
</script>
<p><small></small> <input id="range-stack-gp-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-gp-sample')" oninput="setDivs('stack-gp-sample')"> <button onclick="plusDivs(-1, 'stack-gp-sample')">❮</button> <button onclick="plusDivs(1, 'stack-gp-sample')">❯</button></p>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-gp-sample-0.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-gp-sample-1.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-gp-sample-2.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-gp-sample-3.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//stack-gp-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="analysis-of-deep-gps" class="slide level2">
<h2>Analysis of Deep GPs</h2>
<ul>
<li><p><em>Avoiding pathologies in very deep networks</em> <span class="citation" data-cites="Duvenaud:pathologies14">Duvenaud et al. (2014)</span> show that the derivative distribution of the process becomes more <em>heavy tailed</em> as number of layers increase.</p></li>
<li><p><em>How Deep Are Deep Gaussian Processes?</em> <span class="citation" data-cites="Dunlop:deep2017">Dunlop et al. (n.d.)</span> perform a theoretical analysis possible through conditional Gaussian Markov property.</p></li>
</ul>
</section>
<section id="stacked-gps-video-by-david-duvenaud" class="slide level2">
<h2>Stacked GPs (video by David Duvenaud)</h2>
<div class="figure">
<div id="visualization-deep-gp-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Visualization of mapping of a two dimensional space through a deep Gaussian process.
</aside>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the GitHub repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="alan-turing" class="slide level2">
<h2>Alan Turing</h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="probability-winning-olympics" class="slide level2">
<h2>Probability Winning Olympics?</h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="gaussian-process-fit" class="slide level2">
<h2>Gaussian Process Fit</h2>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="deep-gp-fit" class="slide level2">
<h2>Deep GP Fit</h2>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="olympic-marathon-data-deep-gp" class="slide level2">
<h2>Olympic Marathon Data Deep GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep GP fit to the Olympic marathon data. Error bars now change as the prediction evolves.
</aside>
</section>
<section id="olympic-marathon-data-deep-gp-1" class="slide level2">
<h2>Olympic Marathon Data Deep GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Point samples run through the deep Gaussian process show the distribution of output locations.
</aside>
</section>
<section id="olympic-marathon-data-latent-1" class="slide level2">
<h2>Olympic Marathon Data Latent 1</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from input to the latent layer is broadly, with some flattening as time goes on. Variance is high across the input range.
</aside>
</section>
<section id="olympic-marathon-data-latent-2" class="slide level2">
<h2>Olympic Marathon Data Latent 2</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from the latent layer to the output layer.
</aside>
</section>
<section id="olympic-marathon-pinball-plot" class="slide level2">
<h2>Olympic Marathon Pinball Plot</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through each layer of the Gaussian processes. Mean directions of movement are shown by lines. Shading gives one standard deviation of movement position. At each layer, the uncertainty is reset. The overal uncertainty is the cumulative uncertainty from all the layers. There is some grouping of later points towards the right in the first layer, which also injects a large amount of uncertainty. Due to flattening of the curve in the second layer towards the right the uncertainty is reduced in final output.
</aside>
</section>
<section id="della-gatta-gene-data" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gene expression levels over time for a gene from data provided by <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>. We would like to understand whether there is signal in the data, or we are only observing noise.
</aside>
</section>
<section id="gene-expression-example" class="slide level2">
<h2>Gene Expression Example</h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Freddie Kalaitzis
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/./slides/diagrams//people/freddie-kalaitzis.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/./slides/diagrams//health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The example is taken from the paper “A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression.” <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.
</aside>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
</section>
<section id="tp53-gene-data-gp" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale parameter initialized to 50 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-1" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale parameter initialized to 2000 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-2" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the noise initialized low (standard deviation 0.1) and the time scale parameter initialized to 20 minutes.
</aside>
</section>
<section id="multiple-optima" class="slide level2">
<h2>Multiple Optima</h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<!--
## Multiple Optima  {}



<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="della-gatta-gene-data-deep-gp" class="slide level2">
<h2>Della Gatta Gene Data Deep GP</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/della-gatta-gene-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the Della Gatta gene expression data.
</aside>
</section>
<section id="della-gatta-gene-data-deep-gp-1" class="slide level2">
<h2>Della Gatta Gene Data Deep GP</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/della-gatta-gene-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process samples fitted to the Della Gatta gene expression data.
</aside>
</section>
<section id="della-gatta-gene-data-latent-1" class="slide level2">
<h2>Della Gatta Gene Data Latent 1</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-0.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process mapping from input to latent layer for the della Gatta gene expression data.
</aside>
</section>
<section id="della-gatta-gene-data-latent-2" class="slide level2">
<h2>Della Gatta Gene Data Latent 2</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-1.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process mapping from latent to output layer for the della Gatta gene expression data.
</aside>
</section>
<section id="tp53-gene-pinball-plot" class="slide level2">
<h2>TP53 Gene Pinball Plot</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/della-gatta-gene-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through each layer of the Gaussian processes. Mean directions of movement are shown by lines. Shading gives one standard deviation of movement position. At each layer, the uncertainty is reset. The overal uncertainty is the cumulative uncertainty from all the layers. Pinball plot of the della Gatta gene expression data.
</aside>
</section>
<section id="step-function-data" class="slide level2">
<h2>Step Function Data</h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Simulation study of step function data artificially generated. Here there is a small overlap between the two lines.
</aside>
</section>
<section id="step-function-data-gp" class="slide level2">
<h2>Step Function Data GP</h2>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the step function data. Note the large error bars and the over-smoothing of the discontinuity. Error bars are shown at two standard deviations.
</aside>
</section>
<section id="step-function-data-deep-gp" class="slide level2">
<h2>Step Function Data Deep GP</h2>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the step function data.
</aside>
</section>
<section id="step-function-data-deep-gp-1" class="slide level2">
<h2>Step Function Data Deep GP</h2>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process model for the step function fit.
</aside>
</section>
<section id="step-function-data-latent-1" class="slide level2">
<h2>Step Function Data Latent 1</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-2" class="slide level2">
<h2>Step Function Data Latent 2</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-3" class="slide level2">
<h2>Step Function Data Latent 3</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-4" class="slide level2">
<h2>Step Function Data Latent 4</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg" width style=" ">
</object>
</section>
<section id="step-function-pinball-plot" class="slide level2">
<h2>Step Function Pinball Plot</h2>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot of the deep GP fitted to the step function data. Each layer of the model pushes the ‘ball’ towards the left or right, saturating at 1 and 0. This causes the final density to be be peaked at 0 and 1. Transitions occur driven by the uncertainty of the mapping in each layer.
</aside>
</section>
<section id="motorcycle-helmet-data" class="slide level2">
<h2>Motorcycle Helmet Data</h2>
<div class="figure">
<div id="motorcycle-helment-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//datasets/motorcycle-helmet.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Motorcycle helmet data. The data consists of acceleration readings on a motorcycle helmet undergoing a collision. The data exhibits heteroschedastic (time varying) noise levles and non-stationarity.
</aside>
</section>
<section id="motorcycle-helmet-data-gp" class="slide level2">
<h2>Motorcycle Helmet Data GP</h2>
<div class="figure">
<div id="motorcycle-helmet-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/motorcycle-helmet-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="motorcycle-helmet-data-deep-gp" class="slide level2">
<h2>Motorcycle Helmet Data Deep GP</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/motorcycle-helmet-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="motorcycle-helmet-data-deep-gp-1" class="slide level2">
<h2>Motorcycle Helmet Data Deep GP</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/motorcycle-helmet-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process as fitted to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="motorcycle-helmet-data-latent-1" class="slide level2">
<h2>Motorcycle Helmet Data Latent 1</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mappings from the input to the latent layer for the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="motorcycle-helmet-data-latent-2" class="slide level2">
<h2>Motorcycle Helmet Data Latent 2</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mappings from the latent layer to the output layer for the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="motorcycle-helmet-pinball-plot" class="slide level2">
<h2>Motorcycle Helmet Pinball Plot</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/motorcycle-helmet-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot for the mapping from input to output layer for the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="robot-wireless-data" class="slide level2">
<h2>Robot Wireless Data</h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//datasets/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Ground truth movement for the position taken while recording the multivariate time-course of wireless access point signal strengths.
</aside>
</section>
<section id="robot-wifi-data" class="slide level2">
<h2>Robot WiFi Data</h2>
<div class="figure">
<div id="robot-wireless-data-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//datasets/robot-wireless-dim-1.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Output dimension 1 from the robot wireless data. This plot shows signal strength changing over time.
</aside>
</section>
<section id="gaussian-process-fit-to-robot-wireless-data" class="slide level2">
<h2>Gaussian Process Fit to Robot Wireless Data</h2>
</section>
<section id="robot-wifi-data-gp" class="slide level2">
<h2>Robot WiFi Data GP</h2>
<div class="figure">
<div id="robot-wireless-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//gp/robot-wireless-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Robot Wireless dimension 1.
</aside>
</section>
<section id="robot-wifi-data-deep-gp" class="slide level2">
<h2>Robot WiFi Data Deep GP</h2>
<div class="figure">
<div id="robot-wireless-deep-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/robot-wireless-deep-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of the deep Gaussian process to dimension 1 of the robot wireless data.
</aside>
</section>
<section id="robot-wifi-data-deep-gp-1" class="slide level2">
<h2>Robot WiFi Data Deep GP</h2>
<div class="figure">
<div id="robot-wireless-deep-gp-samples-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/robot-wireless-deep-gp-samples-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process fit to dimension 1 of the robot wireless data.
</aside>
</section>
<section id="robot-wifi-data-latent-space" class="slide level2">
<h2>Robot WiFi Data Latent Space</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</section>
<section id="robot-wifi-data-latent-space-1" class="slide level2">
<h2>Robot WiFi Data Latent Space</h2>
<div class="figure">
<div id="robot-wireless-latent-space-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/robot-wireless-latent-space.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Inferred two dimensional latent space of the model for the robot wireless data.
</aside>
</section>
<section id="high-five-motion-capture-data" class="slide level2">
<h2>‘High Five’ Motion Capture Data</h2>
<ul>
<li>‘High five’ data.</li>
<li>From CMU Mocap Database <span class="citation" data-cites="CMU-mocap03">(CMU Motion Capture Lab, 2003)</span>.</li>
<li>Model learns structure between two interacting subjects.</li>
</ul>
</section>
<section id="shared-lvm" class="slide level2">
<h2>Shared LVM</h2>
<div class="figure">
<div id="shared-latent-variable-model-graph-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//shared.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Shared latent variable model structure. Here two related data sets are brought together with a set of latent variables that are partially shared and partially specific to one of the data sets.
</aside>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-gp-high-five-figure" class="figure-frame">
<p><img class="" src="https://inverseprobability.com/talks/./slides/diagrams//deep-gp-high-five2.png" width="80%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</div>
</div>
<aside class="notes">
Latent spaces of the ‘high five’ data. The structure of the model is automatically learnt. One of the latent spaces is coordinating how the two figures walk together, the other latent spaces contain latent variables that are specific to each of the figures separately.
</aside>
</section>
<section id="subsample-of-the-mnist-data" class="slide level2">
<h2>Subsample of the MNIST Data</h2>
</section>
<section id="fitting-a-deep-gp-to-a-the-mnist-digits-subsample" class="slide level2">
<h2>Fitting a Deep GP to a the MNIST Digits Subsample</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Zhenwen Dai
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/./slides/diagrams//people/zhenwen-dai.jpg" clip-path="url(#clip1)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/./slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip2)"/>
</svg>
</div>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-latent-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/mnist-digits-subsample-latent.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Latent space for the deep Gaussian process learned through unsupervised learning and fitted to a subset of the MNIST digits subsample.
</aside>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/mnist-digits-subsample-hidden-1-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs dimension 0.
</aside>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/mnist-digits-subsample-hidden-2-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs dimension 0.
</aside>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/mnist-digits-subsample-hidden-3-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs dimension 0.
</aside>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/mnist-digits-subsample-hidden-4-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs dimension 0.
</aside>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="figure">
<div id="digit-samples-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deepgp/digit-samples-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
These digits are produced by taking a tour of the two dimensional latent space (as described by a Gaussian process sample) and mapping the tour into the data space. We visualize the mean of the mapping in the images.
</aside>
</section>
<section id="deep-health" class="slide level2">
<h2>Deep Health</h2>
<div class="figure">
<div id="deep-health-model-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/./slides/diagrams//deep-health.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The deep health model uses different layers of abstraction in the deep Gaussian process to represent information about diagnostics and treatment to model interelationships between a patients different data modalities.
</aside>
</section>
<section id="uncertainty" class="slide level2">
<h2>Uncertainty</h2>
<ul>
<li>Uncertainty in prediction arises from:</li>
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all possible prediction functions.</li>
<li>Also uncertainties in objective, leave those for another day.</li>
</ul>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/./slides/diagrams//gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions defined by our covarariance
</aside>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/./slides/diagrams//gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/./slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in Bayesian inference we discard all samples that are distant from the data.
</aside>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/./slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/./slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed analytically using linear algebra.
</aside>
</section>
<section id="stochastic-process-composition-1" class="slide level2">
<h2>Stochastic Process Composition</h2>
<p><span class="math display">\[\mathbf{ y}= \mathbf{ f}_4\left(\mathbf{ f}_3\left(\mathbf{ f}_2\left(\mathbf{ f}_1\left(\mathbf{ x}\right)\right)\right)\right)\]</span></p>
<!-- SECTION Deep Gaussian Processes -->
</section>
<section id="deep-gaussian-processes-1" class="slide level2">
<h2>Deep Gaussian Processes</h2>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="figure">
<div id="cosmic-microwave-background-figure" class="figure-frame">
<div class="centered" style="">
<img class="vertical-align:middle" src="https://inverseprobability.com/talks/./slides/diagrams//Planck_CMB.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The cosmic microwave background is, to a very high degree of precision, a Gaussian process. The parameters of its covariance function are given by fundamental parameters of the universe, such as the amount of dark matter and mass.
</aside>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mollweide-sample-cmb-figure" class="figure-frame">
<div class="centered" style="">
<img class="vertical-align:middle" src="https://inverseprobability.com/talks/./slides/diagrams//physics/mollweide-sample-cmb.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A simulation of the Cosmic Microwave Background obtained through sampling from the relevant Gaussian process covariance (in polar co-ordinates).
</aside>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="figure">
<div id="modern-universe-non-linear-function-figure" class="figure-frame">
<div style="fontsize:120px;vertical-align:middle">
<img src="https://inverseprobability.com/talks/./slides/diagrams//earth_PNG37.png" width="20%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(=f\Bigg(\)</span><img src="https://inverseprobability.com/talks/./slides/diagrams//Planck_CMB.png"  width="50%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(\Bigg)\)</span>
</div>
</div>
</div>
<aside class="notes">
What we observe today is some non-linear function of the cosmic microwave background.
</aside>
</section>
<section id="modern-review" class="slide level2">
<h2>Modern Review</h2>
<ul>
<li><em>A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</em> <span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span>
<div class="centered " style="">
</li>
</ul>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/./slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip3)"/>
</svg>
</div>
<ul>
<li><em>Deep Gaussian Processes and Variational Propagation of Uncertainty</em> <span class="citation" data-cites="Damianou:thesis2015">Damianou (2015)</span></li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What is Machine Learning?</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Bengio:deep09" class="csl-entry" role="doc-biblioentry">
Bengio, Y., 2009. <span class="nocase">Learning Deep Architectures for AI</span>. Found. Trends Mach. Learn. 2, 1–127. <a href="https://doi.org/10.1561/2200000006">https://doi.org/10.1561/2200000006</a>
</div>
<div id="ref-Thang:unifying17" class="csl-entry" role="doc-biblioentry">
Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for <span>G</span>aussian process pseudo-point approximations using power expectation propagation. Journal of Machine Learning Research 18, 1–72.
</div>
<div id="ref-Bui:deep16" class="csl-entry" role="doc-biblioentry">
Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R., 2016. Deep <span>G</span>aussian processes for regression using approximate expectation propagation, in: Balcan, M.F., Weinberger, K.Q. (Eds.), Proceedings of the 33rd International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, New York, New York, USA, pp. 1472–1481.
</div>
<div id="ref-CMU-mocap03" class="csl-entry" role="doc-biblioentry">
CMU Motion Capture Lab, 2003. The <span>CMU</span> mocap database.
</div>
<div id="ref-Damianou:thesis2015" class="csl-entry" role="doc-biblioentry">
Damianou, A., 2015. Deep <span>G</span>aussian processes and variational propagation of uncertainty (PhD thesis). University of Sheffield.
</div>
<div id="ref-Damianou:variational15" class="csl-entry" role="doc-biblioentry">
Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference for latent variables and uncertain inputs in <span>G</span>aussian processes. Journal of Machine Learning Research 17.
</div>
<div id="ref-DellaGatta:direct08" class="csl-entry" role="doc-biblioentry">
Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a>
</div>
<div id="ref-Dunlop:deep2017" class="csl-entry" role="doc-biblioentry">
Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. How deep are deep <span>G</span>aussian processes? Journal of Machine Learning Research 19, 1–46.
</div>
<div id="ref-Duvenaud:pathologies14" class="csl-entry" role="doc-biblioentry">
Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding pathologies in very deep networks.
</div>
<div id="ref-Havasi:deepgp18" class="csl-entry" role="doc-biblioentry">
Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018. Inference in deep <span>G</span>aussian processes using stochastic gradient <span>H</span>amiltonian <span>M</span>onte <span>C</span>arlo, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 31. Curran Associates, Inc., pp. 7506–7516.
</div>
<div id="ref-Hinton:fast06" class="csl-entry" role="doc-biblioentry">
Hinton, G.E., Osindero, S., 2006. A fast learning algorithm for deep belief nets. Neural Computation 18, 2006.
</div>
<div id="ref-Izmailov:subspace19" class="csl-entry" role="doc-biblioentry">
Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P., Wilson, A.G., 2019. Subspace inference for bayesian deep learning. CoRR abs/1907.07504.
</div>
<div id="ref-Kalaitzis:simple11" class="csl-entry" role="doc-biblioentry">
Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through <span>Gaussian</span> process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a>
</div>
<div id="ref-Lawrence:hgplvm07" class="csl-entry" role="doc-biblioentry">
Lawrence, N.D., Moore, A.J., 2007. Hierarchical <span>G</span>aussian process latent variable models. pp. 481–488.
</div>
<div id="ref-MacKay:gpintroduction98" class="csl-entry" role="doc-biblioentry">
MacKay, D.J.C., n.d. Introduction to <span>G</span>aussian processes. pp. 133–166.
</div>
<div id="ref-Salakhutdinov:quantitative08" class="csl-entry" role="doc-biblioentry">
Salakhutdinov, R., Murray, I., n.d. On the quantitative analysis of deep belief networks. pp. 872–879.
</div>
<div id="ref-Salimbeni:doubly2017" class="csl-entry" role="doc-biblioentry">
Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational inference for deep <span>G</span>aussian processes, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="doc-biblioentry">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. <span>DeepFace</span>: Closing the gap to human-level performance in face verification, in: Proceedings of the <span>IEEE</span> Computer Society Conference on Computer Vision and Pattern Recognition. <a href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
