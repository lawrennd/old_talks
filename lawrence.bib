%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  File: Lawrence.bib
%
%  Bibliography file
%
%  Neil Lawrence, 8 August, 1999.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@string{phdthesis = {PhD Theses}}

@string{article = {Journal Papers}}
@string{book = {Books}}
@string{techreport = {Technical Reports}}
@string{unpublished = {Submitted Papers}}
@string{inproceedings = {Refereed Conference Papers}}
@string{proceedings = {Proceedings}}
@string{incollection = {In Collected Volumes}}
@string{collection = {Volumes of Collected Papers}}
@string{misc = {Miscellaneous}}
@string{patent = {Patents}}
@string{talk = {Talks}}
@string{poster = {Posters}}
@string{mainheading = {Machine Learning Publications}}
@String{bioinf = {Bioinformatics}}
@String{bmcbioinf = {BMC Bioinformatics}}

@string{RMP =      {Reviews of Modern Physics}}
@string{ieeecomp = {IEEE Computer Society Press}}
@string{pCVPR =    {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}}
@string{jasa = {Journal of the American Statistical Association}}
@string{icml =     {Proceedings of the International Conference in
                   Machine Learning}}
@string{auai =      {AUAI Press}}
@string{uai =      {Uncertainty in Artificial Intelligence}}
@string{icann =    {International Conference on Artificial Neural Networks}}
@String{jmbcell = {Mol. Biol. Cell.}}
@String{pnasusa = {Proc. Natl. Acad. Sci. USA}}
@String{jair = {Journal of Artificial Intelligence Research}}
@String{jmlr = {Journal of Machine Learning Research}}
@String{lncs = {Lecture Notes in Computer Science}}
@string{nips =     {Advances in Neural Information Processing Systems}}
@string{NC =       {Neural Computation}}
@string{ML =       {Machine Learning}}
@string{NN =       {Neural Networks}}
@string{NW =       {Network: Computation in Neural Systems}}
@string{IJNS =     {International Journal of Neural Systems}}
@string{PRa =      {Physical Review A}}
@string{PRL =      {Physical Review Letters}}
@string{EPL =      {Europhysics Letters}}
@string{icassp =   {International Conference on Acoustics, Speech and Signal Processing}}
@string{IEEE =     {IEEE Transactions on Neural Networks}}
@string{TIT =      {IEEE Transactions on Information Theory}}
@string{TKDE =     {IEEE Transactions on Knowledge and Data Engineering}}
@string{AMS =      {Annals of Mathematical Statistics}}
@string{PAMI =     {IEEE Transactions on Pattern Analysis and
                   Machine Intelligence}}
@string{DOKLADY =  {Doklady Akademiia Nauk SSSR}}
@string{network =  {Network: Computation in Neural Systems}}
@string{ijcnn =    {Proceedings of the International Joint Conference on
                   Neural Networks}}

@string{addison =  {Addison-Wesley}}
@string{mcgraw =   {McGraw-Hill}}
@string{nholland = {North Holland}}
@string{ams = {AMS}}
@string{springer = {Springer-Verlag}}
@string{harvard =      {Harvard University Press}}
@string{mit =      {MIT Press}}
@string{cup =      {Cambridge University Press}}
@string{mk =       {Morgan Kauffman}}
@string{wiley =    {John Wiley and Sons}}
@string{JRSSb =    {Journal of the Royal Statistical Society, B}}
@string{JMB =    {Journal of Molecular Biology}}

@string{myftp =    {http://www.thelawrences.net/neil/}}
@string{SheffieldML =    {https://github.com/SheffieldML/}}
@string{shefnbrepo_nohttp = {github/SheffieldML/notebook/blob/master/}}
@string{shefmlrepo_nohttp = {github.com/SheffieldML}}
@string{shefftp =    {ftp://ftp.dcs.shef.ac.uk/home/neil/}}
@string{shefhttp_nohttp =    {staffwww.dcs.shef.ac.uk/people/N.Lawrence/talks/}}
@string{shefhttp =    {http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/talks/}}
@string{manftp =    {ftp://ftp.cs.man.ac.uk/pub/ai/neill/}}
@string{sheftech =    {The University of Sheffield, Department of Computer Science}}
@string{joabpubs = {http://www.dcs.shef.ac.uk/~joab/Publications/}}
@string{softwarehttp = {http://inverseprobability.com/}}
@string{sheffieldgit = {https://github.com/SheffieldML/}}

@string{gplvmTitle1 = {Probabilistic Non-linear Component Analysis through {G}aussian Process Latent Variable Models}}
@string{gplvmAbstract1 = {It is known that Principal Component Analysis has an
                 underlying probabilistic representation based on a
                 latent variable model. Principal component analysis
                 (PCA) is recovered when the latent variables are
                 integrated out and the parameters of the model are
                 optimised by maximum likelihood. It is less well
                 known that the dual approach of integrating out the
                 parameters and optimising with respect to the latent
                 variables also leads to PCA. The marginalised
                 likelihood in this case takes the form of Gaussian
                 process mappings, with linear Covariance functions,
                 from a latent space to an observed space, which we
                 refer to as a Gaussian Process Latent Variable Model
                 (GPLVM). This dual probabilistic PCA is still a
                 linear latent variable model, but by looking beyond
                 the inner product kernel as a covariance function we
                 can develop a non-linear probabilistic PCA.

                 In the talk we will introduce the GPLVM and
                 illustrate its application on a range of high
                 dimensional data sets including motion capture data,
                 hand written digits, a medical diagnosis data set and
                 images.}}

@string{gplvmTitle2 = {High Dimensional Probabilistic Modelling through Manifolds}}
@string{gplvmAbstract2 = {Density modelling in high dimensions is a
                 very difficult problem. Traditional approaches, such
                 as mixtures of Gaussians, typically fail to capture
                 the structure of data sets in high dimensional
                 spaces. In this talk we will argue that for many data
                 sets of interest, the data can be represented as a
                 lower dimensional manifold immersed in the higher
                 dimensional space. We will then present the Gaussian
                 Process Latent Variable Model (GP-LVM), a non-linear
                 probabilistic variant of principal component analysis
                 (PCA) which implicitly assumes that the data lies on
                 a lower dimensional space.

                 We will demonstrate the application of the model to a
                 range of data sets, but with a particular focus on
                 human motion data. We will show some preliminary work
                 on facial animation and make use of a skeletal motion
                 capture data set to illustrate differences between
                 our model and traditional manifold techniques.}}

@string{gplvmTitle3 = {Computer Vision Reading Group: The {G}aussian
                 Process Latent Variable Model}}

@string{gplvmAbstract3 = {The Gaussian process latent variable model
                 (GP-LVM) is a recently proposed probabilistic
                 approach to obtaining a reduced dimension
                 representation of a data set. In this tutorial we
                 motivate and describe the GP-LVM, giving a review of
                 the model itself and some of the concepts behind it.}
}

@string{gplvmTitle4 = {Probabilistic Dimensional Reduction with the {G}aussian Process Latent Variable Model}}
@string{gplvmAbstract4 = {Density modelling in high dimensions is a
                 very difficult problem. Traditional approaches, such
                 as mixtures of Gaussians, typically fail to capture
                 the structure of data sets in high dimensional
                 spaces. In this talk we will argue that for many data
                 sets of interest, the data can be represented as a
                 lower dimensional manifold immersed in the higher
                 dimensional space. We will then present the Gaussian
                 Process Latent Variable Model (GP-LVM), a non-linear
                 probabilistic variant of principal component analysis
                 (PCA) which implicitly assumes that the data lies on
                 a lower dimensional space.

                 Having introduced the GP-LVM we will review
                 extensions to the algorithm, including dynamics,
                 learning of large data sets and back constraints.  We
                 will demonstrate the application of the model and its
                 extensions to a range of data sets, including human
                 motion data, a vowel data set and a robot mapping
                 problem.}
}
@string{gplvmTitle5 = {Probabilistic Dimensional Reduction with the {G}aussian Process Latent Variable Model}}
@string{gplvmAbstract5 = {Density modelling in high dimensions is a
                 very difficult problem. Traditional approaches, such
                 as mixtures of Gaussians, typically fail to capture
                 the structure of data sets in high dimensional
                 spaces. In this talk we will argue that for many data
                 sets of interest, the data can be represented as a
                 lower dimensional manifold immersed in the higher
                 dimensional space. We will then present the Gaussian
                 Process Latent Variable Model (GP-LVM), a non-linear
                 probabilistic variant of principal component analysis
                 (PCA) which implicitly assumes that the data lies on
                 a lower dimensional space.


                 Having introduced the GP-LVM we will review
                 extensions to the algorithm. Given time we will review dynamical extensions, Bayesian approaches to dimensionality determination,
                 learning of large data sets. We
                 will demonstrate the application of the model and its
                 extensions to a range of data sets, including human
                 motion data, speech data and video.}
}

@string{gpTitle1 = {Learning and Inference with {G}aussian Processes}}

@string{gpAbstract1 = {Many application domains of machine learning can be
                 reduced to inference about the values of a
                 function. Gaussian processes are powerful, flexible,
                 probabilistic models that enable us to efficiently
                 perform inference about functions in the presence of
                 uncertainty.

                 In this talk I will introduce Gaussian processes and
                 review a few standard applications of these models. I
                 will then show how Gaussian processes can be used to
                 solve important and diverse real-world problems,
                 including inference of the concentration of
                 transcription factors which regulate gene expression
                 and creating probabilistic models of human motion for
                 animation and tracking.}
}

@String{pumaTitle1 = {{PUMA}: Propagation of Uncertainty in Microarray Analysis}}
@String{pumaAbstract1 = {}}

@Article{Sarkka:lfmcontrol18,
  author = 	 {Simo Sarkka and Mauricio A. Alvarez and Neil D. Lawrence},
  journal =	 {IEEE Transactions on Automatic Control}, 
  title = 	 {Gaussian Process Latent Force Models for Learning and Stochastic Control of Physical Systems}, 
  year =	 {2018}, 
  volume =	 {}, 
  number =	 {}, 
  abstract =	 {This article is concerned with learning and stochastic control in physical systems which contain unknown input signals. These unknown signals are modeled as Gaussian processes (GP) with certain parametrized covariance structures. The resulting latent force models (LFMs) can be seen as hybrid models that contain a first-principles physical model part and a non-parametric GP model part. We briefly review the statistical inference and learning methods for this kind of models, introduce stochastic control methodology for the models, and provide new theoretical observability and controllability results for them.}, 
  doi =	 	 {10.1109/TAC.2018.2874749}, 
  ISSN =	 {0018-9286}, 
  month =	 {}
}

@misc{Dai:gpu14,
Author = {Zhenwen Dai and Andreas Damianou and James Hensman and Neil D. Lawrence},
Title = {Gaussian Process Models with Parallelization and {GPU} acceleration},
Year = {2014},
Eprint = {arXiv:1410.4984},
}

@Article{Gambardella:reverse15,
  author = 	 {Gennaro Gambardella and Ivana Peluso and Sandro Montefusco and Mukesh Bansal and Diego L. Medina and Neil D. Lawrence and Diego di Bernardo},
  title = 	 {A reverse-engineering approach to dissect post-translational modulators of transcription factor's activity from transcriptional data},
  journal = 	 {BMC Bioinformatics},
  year = 	 {2015},
  OPTkey = 	 {},
  volume =	 {16},
  number =	 {279},
  OPTpages = 	 {},
  day = {3},
  month =	 {9},
  note =	 {In press},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1186/s12859-015-0700-3},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Background

Transcription factors (TFs) act downstream of the major signalling pathways functioning as master regulators of cell fate. Their activity is tightly regulated at the transcriptional, post-transcriptional and post-translational level. Proteins modifying TF activity are not easily identified by experimental high-throughput methods.

Results

We developed a computational strategy, called Differential Multi-Information (DMI), to infer post-translational modulators of a transcription factor from a compendium of gene expression profiles (GEPs). DMI is built on the hypothesis that the modulator of a TF (i.e. kinase/phosphatases), when expressed in the cell, will cause the TF target genes to be co-expressed. On the contrary, when the modulator is not expressed, the TF will be inactive resulting in a loss of co-regulation across its target genes. DMI detects the occurrence of changes in target gene co-regulation for each candidate modulator, using a measure called Multi-Information. We validated the DMI approach on a compendium of 5,372 GEPs showing its predictive ability in correctly identifying kinases regulating the activity of 14 different transcription factors.

Conclusions

DMI can be used in combination with experimental approaches as high-throughput screening to efficiently improve both pathway and target discovery. An on-line web-tool enabling the user to use DMI to identify post-transcriptional modulators of a transcription factor of interest che be found at http://dmi.tigem.it.},
  OPTgroup = 	 {}
}

@Article{Honkela:genome15,
  author = 	 {Antti Honkela and Jaakko Peltonen and Hande Topa and Iryna Charapitsa and Filomena Matarese and Korbinian Grote and Hendrik G. Stunnenberg and George Reid and Neil D. Lawrence and Magnus Rattray},
  title = 	 {Genome-wide modeling of transcription kinetics reveals patterns of {RNA} production delays},
  journal = 	 pnasusa,
  year = 	 {2015},
  OPTkey = 	 {},
  volume =	 {112},
  number =	 {42},
  pages =	 {13115--13120},
  day = {5},
  month =	 {10},
  note =	 {In press},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {10.1073/pnas.1420404112},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract = {Genes with similar transcriptional activation kinetics can display very different temporal mRNA profiles because of differences in transcrip tion time, degradation rate, and RNA-processing kinetics. Recent studies have shown that a splicing-associated RNA production delay can be significant. To investigate this issue more generally, it is useful to develop methods applicable to genome-wide datasets. We introduce a joint model of transcriptional activation and mRNA accumulation that can be used for inference of transcription rate, RNA production delay, and degradation rate given data from high-throughput sequencing time course experiments. We combine a mechanistic differential equation model with a nonparametric statistical modeling approach allowing us to capture a broad range of activation kinetics, and we use Bayesian parameter estimation to quantify the uncertainty in estimates of the kinetic parameters. We apply the model to data from estrogen receptor Î± activation in the MCF-7 breast cancer cell line. We use RNA polymerase II ChIP-Seq time course data to characterize transcriptional activation and mRNA-Seq time course data to quantify mature transcripts. We find that 11\% of genes with a good signal in the data display a delay of more than 20 min between completing transcription and mature mRNA production. The genes displaying these long delays are significantly more likely to be short. We also find a statistical association between high delay and late intron retention in pre-mRNA data, indicating significant splicing-associated production delays in many genes.},

  OPTgroup = 	 {}
}

@Talk{Lawrence:deep-summit16b,
  author = {Neil D. Lawrence},
  title = {The Data Delusion: Challenges for Democratising Deep Learning},
  abstract = {The widespread success of deep learning in a variety of domains is being hailed as a new revolution in artificial intelligence. It has taken 20 years to go from defeating Kasparov at Chess to Lee Sedol at Go. But what have the real advances been across this time? The fundamental change has been in terms of data availability and compute availability. The underlying technology has not changed much in the last 20 years. So what does that mean for areas like medicine and health? Significant challenges remain, improving the data efficiency of these algorithms and retaining the balance between individual privacy and predictive power of the models. In this talk we will review these challenges and propose some ways forward.

Bio:

Neil Lawrence is a Professor of Machine Learning and Computational Biology at the University of Sheffield. His main research interest is machine learning through probabilistic models. He focuses on both the algorithmic side of these models and their application. He has a particular focus on applications in personalized health and applications in the developing world. He is well known for his work with Gaussian processes, and has proposed Gaussian process variants of many of the succesful deep learning architectures. He is highly active in the machine learning community, most recently Program Chairing the NIPS conference in 2014 and General Chairing (alongside Corinna Cortes) in 2015.

},
  day = 22,
  month = 9,
  year = 2016,
  youtube = {BI9PMvuolqc},
  blog = {2016-03-04-deep-learning-and-uncertainty.md},
  ppt = {2016-09-22-deepLearningSummit.pptx},  
  reveal = {2016-09-22-the-data-delusion.slides.html},  
  reveal-md = {2016-09-22-the-data-delusion.md},  
  venue = {Deep Learning Summit, London, UK}
}

@Talk{Lawrence:brightonseo17,
  author = {Neil D. Lawrence},
  title = {Lies Damned Lies and Big Data},
  abstract = {There are three types of lies, lies damned lies and big data. We're entering a new era of misrepresentation, but these challenges are not new. Humans are easily misled, but some basic skepticism and attention to detail can keep us on top of the new challenges we face. In this talk will give some hints and tips on how to progress.},
  month = 4,
  year = 2017,
  day = 7,
  venue = {BrightonSEO},
  blog = {2016-11-19-lies-damned-lies-big-data.md}
}

@Talk{Lawrence:osdc16,
  author = {Neil D. Lawrence},
  title = {Three Challenges for Open Data Science},
  abstract = {Data science presents new opportunities but also new challenges. In this talk we will focus on three separate challenges for data science: 1. Paradoxes of the Data Society, 2. Quantifying the Value of Data, 3. Privacy, loss of control, marginalization. 

Each of these challenges has particular implications for data science. The paradoxes relate to our evolving relationship with data and our changing expectations. Quantifying value is vital for accounting for the influence of data in our new digital economies and issues of privacy  and loss of control are fundamental to how our pre-existing rights evolve as the digital world encroaches more closely on the physical. 

One of the goals of open data science should be to address these challenges to ensure that we can avoid the pitfalls of the data driven society, allowing us to reap the benefits of data science in applications from personalized health to the developing world.},
  reveal = {2016-10-08-data-science-challenges.slides.html},
  reveal-md = {2016-10-08-data-science-challenges.md},
  venue = {Open Data Science Conference},
  month = 10,
  year = 2016,
  day = 8
}

@Talk{Lawrence:gpss16b,
  author =  	 {Neil D. Lawrence},
  title =        {Fitting Covariance and Multioutput Gaussian Processes},
  abstract =     {In this second session we will talk about fitting covariance matrices and look at multiple output processes.},
  venue =  	 {GPSS, Sheffield},
  pdf =		 {gp_gpss16_session2.pdf},
  year =  	 2016,
  month =  	 9,
  day =  	 13
}
@Talk{Lawrence:gpss16a,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to Gaussian Processes},
  abstract =     {In this first session we will introduce Gaussian process models, non parametric Bayesian models that allow for principled propagation of uncertainty in regression analysis. We will assume a background in parametric models, linear algebra and probability.},
  venue =  	 {GPSS, Sheffield},
  pdf =		 {gp_gpss16_session1.pdf},
  year =  	 2016,
  month =  	 9,
  day =  	 12
}

@Talk{Lawrence:enbis16,
  author = {Neil D. Lawrence},
  title = {The Challenges of Data Science},
  abstract = {Data science presents new opportunities but also new challenges. In this talk we will focus on three separate challenges for data science: 1. Paradoxes of the Data Society, 2. Quantifying the Value of Data, 3. Privacy, loss of control, marginalization. 

Each of these challenges has particular implications for data science. The paradoxes relate to our evolving relationship with data and our changing expectations. Quantifying value is vital for accounting for the influence of data in our new digital economies and issues of privacy  and loss of control are fundamental to how our pre-existing rights evolve as the digital world encroaches more closely on the physical. 

By addressing these challenges now we can ensure that the pitfalls of the data driven society are overcome allowing us to reap the benefits of data science in applications from personalized health to the developing world.},
  reveal = {2016-09-14-data-science-challenges.slides.html},
  reveal-md = {2016-09-14-data-science-challenges.md},
  venue = {European Network for Business and Industrial Statistics (ENBIS) 2016, Sheffield},
  month = 9,
  year = 2016,
  day = 14
}

@Talk{Lawrence:mlss16bI,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to Gaussian Processes},
  abstract =     {In this first session we will introduce Gaussian process models, non parametric Bayesian models that allow for principled propagation of uncertainty in regression analysis. We will assume a background in parametric models, linear algebra and probability.},
  venue =  	 {MLSS, Arequipa},
  pdf =		 {gp_mlss16b.pdf},
  year =  	 2016,
  month =  	 8,
  day =  	 2
}
@Talk{Lawrence:mlss16bII,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to Gaussian Processes II},
  abstract =     {In the second session we will look at how Gaussian process models are related to Kalman filters and how they may be extended to deal with multiple outputs and mechanistic models.},
  venue =  	 {MLSS, Arequipa},
  pdf =		 {gp_mlss16b.pdf},
  OPTyoutube = {xeP5Sh5VMoM},
  year =  	 2016,
  month =  	 8,
  day =  	 2
}
@Talk{Lawrence:mlss16bIII,
  author =  	 {Neil D. Lawrence},
  title =        {Probabilistic Dimensionality Reduction with Gaussian Processes},
  abstract =     {In the third session we will look at latent variable models from a Gaussian process perspective with a particular focus on dimensionality reduction.},
  venue =  	 {MLSS, Arequipa},
  pdf =		 {gp_mlss16b.pdf},
  OPTyoutube = {xeP5Sh5VMoM},
  year =  	 2016,
  month =  	 8,
  day =  	 3
}
@Talk{Lawrence:edinburgh16,
  author =  	 {Neil D. Lawrence},
  title =        {Communicating Machine Learning},
  abstract =     {As machine learning approaches become more widely adopted their societal impact is increasing. This raises issues in public understanding of science. In this talk I will give an overview of my own approach to addressing this challenge, mixing thoughts and experience into an approach to communicating machine learning.},
  venue =  	 {Symposium on Communicating Machine Learning, Edinburgh},
  reveal = {2016-08-31-communicating-machine-learning.slides.html},
  year =  	 2016,
  month =  	 8,
  day =  	 31
}
@Talk{Lawrence:mlss16bIV,
  author =  	 {Neil D. Lawrence},
  title =        {Variational Compression and Deep {G}aussian Processes},
  abstract =     {In this fourth sesssion we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {MLSS, Arequipa},
  pdf =		 {gp_mlss16b.pdf},
  OPTyoutube = {xeP5Sh5VMoM},
  demo = {demo_2016_08_04_mlss16.m},
  year =  	 2016,
  month =  	 8,
  day =  	 4
}
@Talk{Lawrence:security16,
  author =  	 {Neil D. Lawrence},
  title =        {Privacy and Learning},
  abstract =	 {Absolute security of information locks it down and exposes it to only those who are granted access. Social privacy can be seen as a continuum where we expose different information to different parties according to levels of trust. In this talk we will briefly introduce our efforts on integrating privacy into learning algorithms to ensure a more equitable and free data society.}, 
  venue =  	 {Workshop on Security, Workroom 2, Diamond Building, Sheffield},
  reveal = {2016-07-14-privacy-and-learning.slides.html},
  reveal-md = {2016-07-14-privacy-and-learning.md},
  year =  	 2016,
  month = 7,
  day =  	 14
}
@Talk{Lawrence:professions16,
  author =  	 {Neil D. Lawrence},
  title =        {Machine Learning and the Professions},
  abstract =	 {As part of the Royal Society Working Group on Machine Learning this talk is a short introduction to machine learning for members of the professions followed by a provocation on what machine learning might mean for the future of the professions.}, 
  venue =  	 {Royal Society, London},
  reveal = {2016-07-13-machine-learning-professions.slides.html},
  reveal-md = {2016-07-13-machine-learning-professions.md},
  year =  	 2016,
  month = 7,
  day =  	 13
}

@Talk{Lawrence:dsa16a,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to Data Science and Machine Learning},
  OPTabstract = {}, 
  venue =  	 {Data Science in Africa Summer School, Makerere University},
  ipynb = {2016-06-27-data-science-intro.ipynb},
  reveal = {2016-06-27-data-science-intro.slides.html},
  youtube = {TJRK1_U2skw},
  link1 = {http://inverseprobability.com/mlai2015},
  label1 = {Neil's machine learning course (with video and notes)},
  year =  	 2016,
  month = 6,
  day =  	 27
}
@Talk{Lawrence:dsa16b,
  author =  	 {Neil D. Lawrence},
  title =        {New Directions in Data Science},
  venue =  	 {Data Science in Africa Workshop, UN Global Pulse, Kampala, Uganda},
  reveal-md = {2016-07-01-data-science-challenges.md},
  reveal = {2016-07-01-data-science-challenges.slides.html},
  blog = {2016-07-01-data-science-challenges.md},
  guardian = {https://www.theguardian.com/media-network/2015/aug/25/africa-benefit-data-science-information},
  youtube =	 {_GSLvu6B7Bw},
  abstract = {Data science presents new opportunities for Africa but also new challenges. In this talk we will focus on three separate challenges for data science: 1. Paradoxes of the Data Society, 2. Quantifying the Value of Data, 3. Privacy, loss of control, marginalization. Each of these challenges has particular implications for data science in the developing world. By addressing these challenges now we can ensure that the pitfalls of the data driven society are overcome allowing to reap the benefits.},
  year =  	 2016,
  month = 7,
  day =  	 1
}



@Talk{Lawrence:futureofhumanity16,
  author =  	 {Neil D. Lawrence},
  title =        {System Zero: What Kind of AI Have We Created?},
  abstract = {Machine learning technologies have evolved to the extent that they are now considered the principle underlying technology for our advances in artificial intelligence. Artificial intelligence is an emotive term, given the implications for replacing qualities that humans consider specific to ourselves. In this talk we'll consider what kind of artificial intelligence we've created and what possible implications are for our society.}, 
  venue =  	 {Future of Humanity Institute, Oxford Martin School},
  blog = {2015-12-04-what-kind-of-ai.md},
  blog1 = {2016-05-09-machine-learning-futures-6.md},
  blog2 = {2016-02-29-future-debates-ai.md},
  pdf = {2016-06-09-future-of-ai.pdf},
  ppt = {2016-06-09-future-of-ai.pptx},
  youtube = {8R4t7d7o6ew},
  year =  	 2016,
  month = 6,
  day =  	 9
}
 

@Talk{Lawrence:futureofwork16,
  author =  	 {Neil D. Lawrence},
  title =        {Machine Learning and the Future of Work},
  abstract =	 {Machine learning technologies have evolved to the extent that they are now considered the principle underlying technology for our advances in artificial intelligence. Artificial intelligence is an emotive term, given the implications for replacing qualities that humans consider specific to ourselves. As always new technology has a significant disruptive effect on existing markets, jobs and economies. In this talk we'll explore where the advances are coming from and speculate about how our machine learning future is likely to pan out with a particular focus on work.},
  venue =  	 {Cambridge Centre for Science and Policy},
  pdf = {2016-05-27-future-of-work.pdf},
  ppt = {2016-05-27-future-of-work.pptx},
  blog = {2016-03-09-quora-session.md},
  year =  	 2016,
  month = 5,
  day =  	 27
}

@Talk{Lawrence:pintofscience16,
  author =  	 {Neil D. Lawrence},
  title =        {What Kind of AI Have We Created?},
  abstract =	 {There have been fears voiced by Elon Musk and Stephen Hawking about the direction of artificial intelligent research. They worry about the creation of a sentient AI, one that might outwit us. However, the nature of the AI we have actually created is a long way distant from this. In this talk we will try and relate our models of artificial intelligence to models that have been proposed for the way humans think. The AI that Hawking and Musk fear is not yet here, but is the AI we have actually developed more or less disturbing than the vision they project?},
  venue =  	 {A Pint of Science},
  ipynb = {2016-05-24-what-kind-of-ai.ipynb},
  reveal = {2016-05-24-what-kind-of-ai.slides.html},
  blog = {2015-12-04-what-kind-of-ai.md},
  year =  	 2016,
  month = 5,
  day =  	 24
}

@Talk{Lawrence:entropyday16,
  author =  	 {Neil D. Lawrence},
  title =        {Data Efficiency and Machine Learning},
  abstract =	 {Entropy is a key component of information and probability, and may provide the key to \emph{data efficient} learning. While we've seen great success with the AlphaGo computer program and strides forward in image and speech recognition our current machine learning systems are incredibly data inefficient. Better understanding of entropy with in these systems may provide the key to data efficient learning.},
  ppt = {2016-05-23-entropyDay.pptx},
  pdf = {2016-05-23-entropyDay.pdf},
  venue =  	 {Entropy Day, University of Sheffield},
  blog = {2016-03-04-deep-learning-and-uncertainty.md},
  year =  	 2016,
  month = 5,
  day =  	 23
}


@Talk{Lawrence:iclr16,
  author = {Neil D. Lawrence},
  title = {Beyond Backpropagation: Uncertainty Propagation},
  abstract = {Deep learning is founded on composable functions that are structured to capture regularities in data and can have their parameters optimized by backpropagation (differentiation via the chain rule). Their recent success is founded on the increased availability of data and computational power. However, they are not very data efficient. In low data regimes parameters are not well determined and severe overfitting can occur. The solution is to explicitly handle the indeterminacy by converting it to parameter uncertainty and propagating it through the model. Uncertainty propagation is more involved than backpropagation because it involves convolving the composite functions with probability distributions and integration is more challenging than differentiation. 

We will present one approach to fitting such models using Gaussian processes. The resulting models perform very well in both supervised and unsupervised learning on small data sets. The remaining challenge is to scale the algorithms to much larger data.},
  bio = {Neil Lawrence is Professor of Machine Learning at the University of Sheffield. His expertise is in probabilistic modelling with a particular focus on Gaussian processes and a strong interest in bridging the worlds of mechanistic and empirical models.},
day = 3,
month = 5,
year = 2016,
  ppt = {2016-05-03-UncertaintyPropagationICLR.pptx},
  pdf = {2016-05-03-UncertaintyPropagationICLR.pdf},
  demo = {demo_2016_05_03_iclr.m},
  blog = {2016-03-04-deep-learning-and-uncertainty.md},
venue = {ICLR 2016, San Jaun, Puerto Rico}
}

@Talk{Lawrence:amazon16,
  author = {Neil D. Lawrence},
  title = {Machine Learning with Gaussian Processes},
  abstract = {Gaussian processes (GPs) provide a principled probabilistic approach to prior probability distributions for functions. In this talk we will give an overview of some uses of GPs and their extensions. In particular we will introduce mechanistic models alongside GPs and also use GPs within a structured framework of latent variable models.},
  day = 28,
  month = 4,
  year = 2016,
  demo = {demo_2016_04_28_amazon.m},
  ppt = {2016-04-28-MLGPsAmazon.pptx},
  pdf = {2016-04-28-MLGPsAmazon.pdf},
  venue = {Amazon Machine Learning Conference, Seattle}
}

@Talk{Lawrence:msrne16b,
  author = {Neil D. Lawrence},
  title = {Beyond Backpropagation: Uncertainty Propagation},
  abstract = {Deep learning is founded on composable functions that are structured to capture regularities in data and can have their parameters optimized by backpropagation (differentiation via the chain rule). Their recent success is founded on the increased availability of data and computational power. However, they are not very data efficient. In low data regimes parameters are not well determined and severe overfitting can occur. The solution is to explicitly handle the indeterminacy by converting it to parameter uncertainty and propagating it through the model. Uncertainty propagation is more involved than backpropagation because it involves convolving the composite functions with probability distributions and integration is more challenging than differentiation. We will present one approach to fitting such models using Gaussian processes. The resulting models perform very well in both supervised and unsupervised learning on small data sets. The remaining challenge is to scale the algorithms to much larger data.},
  year = 2016,
  month = 4,
  day = 26,
  demo = {demo_2016_04_26_msr.m},
  blog = {2016-03-04-deep-learning-and-uncertainty.md},
  venue = {Microsoft Research, New England, USA},
  pdf = {2016-04-26-UncertaintyPropagation.pdf},
  ppt = {2016-04-26-UncertaintyPropagation.pptx},
  }

@Talk{Lawrence:msrne16a,
  author = {Neil D. Lawrence},
  title = {Variational Inference in Deep GPs},
  year = 2016,
  month = 4,
  day = 21,
  venue = {Microsoft Research, New England, USA},
  pdf = {msr16_deepgp.pdf}
  }

@Talk{Lawrence:facebook16,
  author = {Neil D. Lawrence},
  title = {Probabilistic Dimensionality Reduction},
  abstract = {In this talk I give a quick overview of probabilistic interpretations of dimensionality reduction, starting with probabilistic principal component analysis and generalising to non-linear approaches such as the Gaussian Process Latent variable model.},
  venue = {Facebook London, UK},
  pdf = {probdim_facebook16.pdf},
  day = 14,
  month = 4,
  year = 2016
}
@Talk{Lawrence:deepSummit16,
  author = {Neil D. Lawrence},
  title = {The Data Delusion: Challenges for Democratising Deep Learning},
  abstract = {The widespread success of deep learning in a variety of domains is being hailed as a new revolution in artificial intelligence. It has taken 20 years to go from defeating Kasparov at Chess to Lee Sedol at Go. But what have the real advances been across this time? The fundamental change has been in terms of data availability and compute availability. The underlying technology has not changed much in the last 20 years. So what does that mean for areas like medicine and health? Significant challenges remain, improving the data efficiency of these algorithms and retaining the balance between individual privacy and predictive power of the models. In this talk we will review these challenges and propose some ways forward.

  Bio:

  Neil Lawrence is a Professor of Machine Learning and Computational Biology at the University of Sheffield. His main research interest is machine learning through probabilistic models. He focuses on both the algorithmic side of these models and their application. He has a particular focus on applications in personalized health and applications in the developing world. 

  He is well known for his work with Gaussian processes, and has proposed Gaussian process variants of many of the succesful deep learning architectures. He is highly active in the machine learning community, most recently Program Chairing the NIPS conference in 2014 and General Chairing (alongside Corinna Cortes) in  2015.},
  day = 7,
  month = 4,
  year = 2016,
  blog = {2016-03-04-deep-learning-and-uncertainty.md},
  ppt = {2016-04-07-deepLearningSummit.pptx},  
  pdf = {2016-04-07-deepLearningSummit.pdf},  
venue = {Deep Learning Summit, London, UK}
}


@Talk{Lawrence:mars16,
  author = {Neil D. Lawrence},
  title = {The Data Delusion},
  abstract = {The race on to develop the next generation of artificially intelligent algorithms, recent successes in hitherto unmanageable problems have somewhat blinded us to our own capabilities. Despite the commercial success of the current generation of learning algorithms, the time has come for the academic community to take stock. Have we really got the tools in place to solve the next generation of learning problems? Or is our current confidence in our toolsets misplaced? In this talk we'll develop at least one direction where our capabilities are lacking.},
  venue = {MARS Conference, Parker Palm Springs, Palm Springs, CA},
  blog = {2016-03-04-deep-learning-and-uncertainty.md},
  day = 21,
  month = 3,
  year = 2016,
  pdf = {2016-03-21-amazonMars.pdf},  
  ppt = {2016-03-21-amazonMars.pptx}  
}
@Talk{Lawrence:futuredebates16,
  author = {Sheffield Debating Society and Tony Dodd and Neil D. Lawrence},
  title = {Future Debates: This House Believes an Artificial Intelligence will Benefit Society},
  abstract = {The British Science Association hosts a series of debates to encourage constructive debate about science's role in people's lives, economy and the UK's future. This debate was hosted by the Sheffield association and was focussed on artificial intelligence. The debate was led by two speakers from Sheffield's Debating Society with Tony Dodd supporting the 'against' and myself supporting the 'for'.},
  day = 29,
  month = 2,
  year = 2016,
  blog = {2016-02-29-future-debates-ai.md},
  venue = {British Science Association Future Debates, Coffee Revolution, University of Sheffield}
}
@Talk{Lawrence:oxwasp16,
  author = {Neil D. Lawrence},
  title = {Machine Learning with {Gaussian} Processes},
  abstract = {Gaussian processes (GPs) provide a principled probabilistic approach to prior probability distributions for functions. In this talk we will give an overview of some uses of GPs and their extensions. In particular we will introduce mechanistic models alongside GPs and also use GPs within the framework of latent variable models.},
  day = 29,
  month = 1,
  year = 2016,
  demo = {demo_2016_01_29_OxWaSP.m},
  ppt = {2016-01-29-OxWaSPGPTalk.pptx},  
  pdf = {2016-01-29-OxWaSPGPTalk.pdf},  
  venue = {OxWaSP Symposium, University of Warwick}
}


@Talk{Lawrence:phd16,
  author = {Neil D. Lawrence},
  title = {What kind of AI have we created?},
  abstract = {The media is full of concerns about our data and how algorithms are affecting us. We worry about personal information becoming public, we worry about what intelligent machines have in store for us. This talk will be about the state of the art in terms of Artificial Intelligence. It will consider what it can do and what it can't do. We are a long way away  from implementing a 'sentient intelligence', but what do we have in its place? This talk will explore current technology and speculate on what futures it may lead to.},
  blog = {2015-12-04-what-kind-of-ai.md},
  ipynb = {2016-01-26-what-kind-of-ai.ipynb},
  reveal = {2016-01-26-what-kind-of-ai.slides.html},
  day = 26,
  month = 1,
  year = 2016,
  venue = {Sheffield Computer Science Welcome Event}
  }


@Talk{Lawrence:odsi15,
  author = {Neil D. Lawrence},
  title = {The Open Data Science Initiative},
  venue = {data@sheffield},
  pdf = {odsi_data_at_sheffield15.pdf},
  abstract = {The Open Data Science Initiative is founded on the idea that there are a set of core principles that are restricting our ability, as a society, to exploit the large quantity of data we are now generating. In this talk we identify the challenges across the range of industry, science, health and the developing world. We then review the principles of open data science which we hope will address these challenges.},
  blog = {2014-07-01-open-data-science.md},
  year = 2015,
  month = 12,
  day = 16
}
@Talk{Lawrence:mechanistic15,
  author = {Neil D. Lawrence},
  title = {The Mechanistic Fallacy and Modelling How We Think},
  abstract = {In this talk we will discuss how our current set of modelling solutions relates to dual process models from psychology. By analogising with layered models of networks we first address the danger of focussing purely on mechanism (or biological plausibility) when discussion modelling in the brain. We term this idea the mechanistic fallacy. In an attempt to operate at a higher level of abstraction, we then take a conceptual approach and attempt to map the broader domain of mechanistic and phenomological models to dual process ideas from psychology. it seems that System 1 is closer to phenomological and System 2 is closer to mechanistic ideas. We will draw connections to surrogate modelling (also known as emmulation) and speculate that one role of System 2 may be to provide additional simulation data for System 1.},
  venue = {NIPS Workshop on Statistical Methods for Understanding Neural Systems},
  ipynb = {2015-12-11-mechanistic-fallacy.ipynb},
  reveal = {2015-12-11-mechanistic-fallacy.slides.html},
  blog = {2015-12-04-what-kind-of-ai.md},
  blog1 = {2015-11-00-artificial-stupidity.md},
  year = 2015,
  month = 12,
  day = 11
}

@Talk{Lawrence:atiscope15,
  author = {Neil D. Lawrence},
  title = {Information Infrastructure for Health},
  abstract = {In this talk we will address challenges in information infrastructure for health. Personalized health care is one of the promises of the information revolution. However, there are major challenges in the curation, collection and management of the data. These are not currently being properly addressed. The care.data fiasco demonstrated the high sensitivity of the public to this data regime. Data leaks from the Pentagon, TalkTalk, Carphone Warehouse have demonstrated the inability of major institutions to keep our data secure. healthcare data is purportedly worth ten times credit card information on international black markets. Machine learning techniques are currently part of the problem, not the solution, they require centralised assimilation of data in a repository that can be easily accessed. A more robust information infrastructure would distribute data and contain afar greater degree of patient control over access. Such user-centric models may offer greater opportunity in terms of obtaining the necessary data-liquidity to fulfill the full potential of personalized health in effecting individuals' health outcomes.},
  venue = {ATI Scoping Workshop on the Data Analytics Pipeline, Edinburgh},
  ipynb = {2015-11-18-health-infrastructure.ipynb},
  reveal = {2015-11-18-health-infrastructure.slides.html},
  blog = {2015-11-17-alan-turing-information-infrastructures.md},
  year = 2015,
  month = 11,
  day = 18
}
@Talk{Lawrence:benevolent15,
  author =  	 {Neil D. Lawrence},
  title =  	 {Personalised Health and {Gaussian} Processes},
  abstract = {},
  venue = {Stratified Medical, 40 Churchway, London, NW1},
  year = 2015,
  month = 10,
  day = 14
}

@Talk{Lawrence:rise15a,
  author =  	 {Neil D. Lawrence},
  title =  	 {What Kind of Artificial Intelligence are we Creating?},
  abstract = {The media is full of concerns about our data and how algorithms are affecting us. We worry about personal information becoming public, we worry about what intelligent machines have in store for us. This talk will be about the state of the art in terms of Artificial Intelligence. It will consider what it can do and what it can't do. We are a long way away  from implementing a 'sentient intelligence', but what do we have in its place? This talk will explore current technology and speculate on what futures it may lead to.},
  venue = {Cognitive Science work in Progress Meeting, Sheffield},
  blog = {2015-12-04-what-kind-of-ai.md},
  ipynb = {2015-10-23-what-kind-of-ai.ipynb},
  reveal = {2015-10-23-what-kind-of-ai.slides.html},
  year = 2015,
  month = 10,
  day = 23
}
@Talk{Lawrence:rise15,
  author =  	 {Neil D. Lawrence},
  title =  	 {What Kind of Artificial Intelligence have we Created?},
  abstract = {The media is full of concerns about our data and how algorithms are affecting us. We worry about personal information becoming public, we worry about what intelligent machines have in store for us. This talk will be about the state of the art in terms of Artificial Intelligence. It will consider what it can do and what it can't do. We are a long way away  from implementing a 'sentient intelligence', but what do we have in its place? This talk will explore current technology and speculate on what futures it may lead to.},
  venue = {The Data Hide, The Hide, Scotland Street, Sheffield},
  blog = {2015-12-04-what-kind-of-ai.md},
  ipynb = {2015-10-20-what-kind-of-ai.ipynb},
  reveal = {2015-10-20-what-kind-of-ai.slides.html},
  year = 2015,
  month = 10,
  day = 20
}

@Talk{Lawrence:peer15,
  author =  	 {Neil D. Lawrence},
  title =  	 {Peer Review and The NIPS Experiment},
  abstract =   {The peer review process can be difficult to navigate for newcomers. In this informal talk we will review the results of the NIPS experiment, an experiment on the repeatability of peer review conducted for the 2014 conference. We will try to keep the presentation information to ensure questions can be asked. With luck it will give more insight into the processes that a program committee goes through when selecting papers.},
  venue =  	 {MLPM Summer School, Museum of Science and Industry, Manchester, UK},
  reveal = {nips_mlpm15.slides.html},
  ipynb = {nips_mlpm15.ipynb},
  blog = {2014-12-16-the-nips-experiment.md},
  label1 =	 {Software},
  link1 =	 {https://github.com/sods/conference/},
  year =  	 2015,
  month =  	 9,
  day =  	 21
}

@Talk{Lawrence:nyeri15a,
  author =  	 {Neil D. Lawrence},
  title =  	 {Introduction to Machine Learning and Data Science},
  abstract =   {},
  venue =  	 {Data Science Africa School, Dedan Kimathi University, Nyeri, Kenya},
  pdf = {motivation_dss15.pdf},
  OPTblog = {},
  OPTlabel1 =	 {},
  OPTlink1 =	 {},
  year =  	 2015,
  month =  	 6,
  day =  	 15
}

@Talk{Lawrence:nyeri15b,
  author =  	 {Neil D. Lawrence},
  title =  	 {Regression},
  abstract =   {},
  venue =  	 {Data Science Africa School, Dedan Kimathi University, Nyeri, Kenya},
  pdf = {regression_dss15.pdf},
  OPTblog = {},
  OPTlabel1 =	 {},
  OPTlink1 =	 {},
  year =  	 2015,
  month =  	 6,
  day =  	 15
}

@Talk{Lawrence:nyeri15c,
  author =  	 {Neil D. Lawrence},
  title =  	 {Personalized Health},
  abstract =   {},
  venue =  	 {Dedan Kimathi University, Nyeri, Kenya},
  pdf = {personalized_dsa15.pdf},
  OPTblog = {},
  OPTlabel1 =	 {},
  OPTlink1 =	 {},
  year =  	 2015,
  month =  	 6,
  day =  	 18
}


@article{Damianou:variational14,
  title={Variational Inference for Uncertainty on the Inputs of {G}aussian Process Models},
  author={Andreas Damianou and Michalis K. Titsias and Neil D. Lawrence},
  journal={ar{X}iv preprint ar{X}iv:1409.2287},
  year={2014}
}

@Article{Damianou:variational15,
  author = 	 {Andreas Damianou and Michalis K. Titsias and Neil D. Lawrence},
  title = 	 {Variational Inference for Latent Variables and Uncertain Inputs in {G}aussian Processes},
  journal = 	 jmlr,
  year = 	 2016,
  OPTkey = 	 {},
  volume =	 {17},
  OPTnumber = 	 {42},
  OPTpages = 	 {1--62},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  html = 	 {http://jmlr.org/papers/v17/damianou16a.html},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to over-fitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets,
including high resolution video data.},
  OPTgroup = 	 {}
}

@Article{Hensman:fast14,
  author = 	 {James Hensman and Magnus Rattray and Neil D. Lawrence},
  title = 	 {Fast nonparametric clustering of structured time-series},
  journal = 	 PAMI,
  year = 	 {2014},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1109/TPAMI.2014.2318711},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {In this publication, we combine two Bayesian nonparametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variational approximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a significant speed-up over EM-based variational inference.},
  OPTgroup = 	 {}
}

@Article{Maina:inference14,
  author = 	 {Ciira wa Maina and  Antti Honkela and Filomena Matarese and Korbinian Grote and Hendrik G. Stunnenberg and George Reid and Neil D. Lawrence and Magnus Rattray},
  title = 	 {Inference of {RNA} Polymerase {II} Transcription Dynamics from Chromatin Immunoprecipitation Time Course Data},
  journal = 	 {PLoS Computat Biol},
  year = 	 {2014},
  OPTkey = 	 {},
  volume =	 {10},
  number =	 {5},
  pages =	 {e1003598},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1371/journal.pcbi.1003598},
  Optlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {Gene transcription mediated by RNA polymerase II (pol-II) is a key step in gene expression. The dynamics of pol-II moving along the transcribed region influence the rate and timing of gene expression. In this work, we present a probabilistic model of transcription dynamics which is fitted to pol-II occupancy time course data measured using ChIP-Seq. The model can be used to estimate transcription speed and to infer the temporal pol-II activity profile at the gene promoter. Model parameters are estimated using either maximum likelihood estimation or via Bayesian inference using Markov chain Monte Carlo sampling. The Bayesian approach provides confidence intervals for parameter estimates and allows the use of priors that capture domain knowledge, e.g. the expected range of transcription speeds, based on previous experiments. The model describes the movement of pol-II down the gene body and can be used to identify the time of induction for transcriptionally engaged genes. By clustering the inferred promoter activity time profiles, we are able to determine which genes respond quickly to stimuli and group genes that share activity profiles and may therefore be co-regulated. We apply our methodology to biological data obtained using ChIP-seq to measure pol-II occupancy genome-wide when MCF-7 human breast cancer cells are treated with estradiol (E2). The transcription speeds we obtain agree with those obtained previously for smaller numbers of genes with the advantage that our approach can be applied genome-wide. We validate the biological significance of the pol-II promoter activity clusters by investigating cluster-specific transcription factor binding patterns and determining canonical pathway enrichment. We find that rapidly induced genes are enriched for both estrogen receptor alpha (ER) and FOXA1 binding in their proximal promoter regions.},
  OPTgroup = 	 {}
}

@InProceedings{Gonzalez:batch16,
  author = 	 {Javier Gonzalez and Zhenwen Dai and Philipp Hennig and Neil D. Lawrence},
  title = 	 {Batch {B}ayesian Optimization via Local Penalization},
  crossref =	 {Gretton:aistats16},
  pages =	 {648--657},
  linkpdf =	 {http://jmlr.org/proceedings/papers/v51/gonzalez16a.pdf},
  abstract =	 {The popularity of Bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These could either be computational or physical facets of the process being optimized. Batch methods, however, require the modeling of the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. We investigate this issue and propose a highly effective heuristic based on an estimate of the functionâs Lipschitz constant that captures the most important aspect of this interactionâlocal repulsionâat negligible computational overhead. A penalized acquisition function is used to collect batches of points minimizing the non-parallelizable computational effort. The resulting algorithm compares very well, in run-time, with much more elaborate alternatives.},
  OPTgroup = 	 {}
}

@InProceedings{Kalaitzis:bigraphical13,
  author = 	 {Alfredo A. Kalaitzis and John Lafferty and Neil D. Lawrence and Shuheng Zhou},
  title = 	 {The Bigraphical Lasso},
  crossref =	 {icml13},
  pages =	 {1229-â1237},
  linkpdf =	 {http://jmlr.org/proceedings/papers/v51/gonzalez16a.pdf},
  abstract =	 {The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce L1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions.},
  linkpdf = {http://jmlr.org/proceedings/papers/v28/kalaitzis13.pdf}
}

@InProceedings{Gonzalez:glasses16,
  author = 	 {Javier Gonzalez and Michael Osborne and Neil D. Lawrence},
  title = 	 {{GLASSES}: Relieving The Myopia Of {B}ayesian Optimisation},
  crossref =	 {Gretton:aistats16},
  pages =	 {790--799},
  linkpdf =	 {http://jmlr.org/proceedings/papers/v51/gonzalez16b.pdf},
  abstract =	 {We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss. We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
  OPTgroup = 	 {}
}



@TechReport{Lawrence:drl17,
  author = 	 {Neil D. Lawrence},
  title = 	 {Data Readiness Levels},
  institution =  {arXiv},
  year = 	 {2017},
  month = {5},
  day = {5},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {https://arxiv.org/pdf/1705.02245.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  arxiv = {1705.02245},
  abstract =	 {Application of models to data is fraught. Data-generating collaborators often only have a very basic understanding of the complications of collating, processing and curating data. Challenges include: poor data collection practices, missing values, inconvenient storage mechanisms, intellectual property, security and privacy. All these aspects obstruct the sharing and interconnection of data, and the eventual interpretation of data through machine learning or other approaches. In project reporting, a major challenge is in encapsulating these problems and enabling goals to be built around the processing of data. Project overruns can occur due to failure to account for the amount of time required to curate and collate. But to understand these failures we need to have a common language for assessing the readiness of a particular data set. This position paper proposes the use of data readiness levels: it gives a rough outline of three stages of data preparedness and speculates on how formalisation of these levels into a common language for data readiness could facilitate project management.},
  OPTgroup = 	 {}
}
@TechReport{Damianou:ibfa16,
  author = 	 {Andreas Damianou and Neil D. Lawrence and Carl Henrik Ek},
  title = 	 {Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis},
  institution =  {arXiv},
  year = 	 {2016},
  month = {4},
  day = {17},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1604.04939v1.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  arxiv = {1604.04939},
  abstract =	 {Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification.},
  OPTgroup = 	 {}
}

@InProceedings{Damianou:semi15,
  author = 	 {Andreas Damianou and Neil D. Lawrence},
  title = 	 {Semi-described and semi-supervised learning with {G}aussian processes},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle =	 {31st Conference on Uncertainty in Artificial Intelligence (UAI)},
  OPTpages = 	 {},
  year =	 {2015},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1509.01168v1.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as "semi-described learning". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.},
  OPTgroup = 	 {}
}

@InProceedings{Saul:chained16,
  author = 	 {Alan D. Saul and James Hensman and Aki Vehtari and Neil D. Lawrence},
  title = 	 {Chained {G}aussian Processes},
  crossref =	 {Gretton:aistats16},
  pages =	 {1431--1440},
  linkpdf =	 {http://jmlr.org/proceedings/papers/v51/saul16.pdf},
  abstract =	 {Gaussian process models are flexible, Bayesian non-parametric approaches to regression. Properties of multivariate Gaussians mean that they can be combined linearly in the manner of additive models and via a link function (like in generalized linear models) to handle non-Gaussian data. However, the link function formalism is restrictive, link functions are always invertible and must convert a parameter of interest to an linear combination of the underlying processes. There are many likelihoods and models where a non-linear combination is more appropriate. We term these more general models ``Chained Gaussian Processes'': the transformation of the GPs to the likelihood parameters will not generally be invertible, and that implies that linearisation would only be possible with multiple (localized) links, i.e a chain. We develop an approximate inference procedure for Chained GPs that is scalable and applicable to any factorized likelihood. We demonstrate the approximation on a range of likelihood functions.},
  OPTgroup = 	 {}
}
@InProceedings{Mattos:recurrent16,
  title = 	 {Recurrent {G}aussian Processes},
  author = 	 {C\'esar Lincoln C. Mattos and Zhenwen Dai and Andreas Damianou and Jeremy Forth and Guilherme A. Barreto and Neil D. Lawrence},
  crossref =	 {Larochelle:iclr16},
  OPTpages = 	 {},
  linkpdf =	 {http://arxiv.org/abs/1511.06644},
  OPTlinksoftware = {},
  abstract =	 {We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.},
  OPTgroup = 	 {}
}
@InProceedings{Dai:variationally16,
  title = 	 {Variationally Auto-Encoded Deep {G}aussian Processes},
  author = {Zhenwen Dai and Andreas Damianou and Javier Gonzalez and Neil D. Lawrence},
  crossref =	 {Larochelle:iclr16},
  OPTpages = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1511.06455v2},
  OPTlinksoftware = {},
  abstract =	 {We develop a scalable deep non-parametric generative model by augmenting deep Gaussian processes with a recognition model. Inference is performed in a novel scalable variational framework where the variational posterior distributions are reparametrized through a multilayer perceptron. The key aspect of this reformulation is that it prevents the proliferation of variational parameters which otherwise grow linearly in proportion to the sample size. We derive a new formulation of the variational lower bound that allows us to distribute most of the computation in a way that enables to handle datasets of the size of mainstream deep learning tasks. We show the efficacy of the method on a variety of challenges including deep unsupervised learning and deep Bayesian optimization.},
  OPTgroup = 	 {}
}

@InProceedings{Andrade:hybrid14,
  author =	 {Ricardo Andrade-Pacheco and James Hensman and Neil D. Lawrence},
  title =	 {Hybrid Discriminative-Generative Approaches with {G}aussian Processes},
  crossref =	 {Kaski:aistats14},
  pages =	 {47--56},
  year =	 2014,
  OPTeditor =	 {},
  volume =	 33,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 {http://jmlr.org/proceedings/papers/v33/andradepacheco14.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = SheffieldML # {GPy},
  abstract =	 {Machine learning practitioners are often faced with
                 a choice between a discriminative and a generative
                 approach to modelling. Here, we present a model based
                 on a hybrid approach that breaks down some of the
                 barriers between the discriminative and generative
                 points of view, allowing continuous dimensionality
                 reduction of hybrid discrete-continous data,
                 discriminative classification with missing inputs and
                 manifold learning informed by class labels.},
  OPTgroup =	 {}
}
@InProceedings{Hensman:tvb14,
  author =	 {James Hensman and Max Zwiessele and Neil D. Lawrence},
  title =	 {Tilted Variational {B}ayes},
  crossref =	 {Kaski:aistats14},
  pages =	 {356--364},
  year =	 2014,
  OPTeditor =	 {},
  volume =	 33,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 {http://jmlr.org/proceedings/papers/v33/hensman14.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = SheffieldML # {GPy},
  abstract =	 {We present a novel method for approximate inference. Using some of the constructs from expectation propagation (EP), we derive a lower bound of the marginal likelihood in a similar fashion to variational Bayes (VB). The method combines some of the benefits of VB and EP: it can be used with light-tailed likelihoods (where traditional VB fails), and it provides a lower bound on the marginal likelihood. We apply the method to Gaussian process classification, a situation where the Kullback-Leibler divergence minimized in traditional VB can be infinite, and to robust Gaussian process regression, where the inference process is dramatically simplified in comparison to EP.

\\\\Code to reproduce all the experiments can be found at \url{http://github.com/SheffieldML/TVB}.},
  OPTgroup =	 {}
}



@Article{Hensman:hierarchical13,
  author = 	 {James Hensman and Neil D. Lawrence and Magnus Rattray},
  title = 	 {Hierarchical {B}ayesian modelling of gene expression time series across irregularly sampled replicates and clusters},
  journal = 	 {BMC Bioinformatics},
  year = 	 2013,
  OPTkey = 	 {},
  volume =	 14,
  number =	 252,
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {doi:10.1186/1471-2105-14-252},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {\textbf{Background}\\\\
Time course data from microarrays and high-throughput sequencing experiments require simple, computationally efficient and powerful statistical models to extract meaningful biological signal, and for tasks such as data fusion and clustering. Existing methodologies fail to capture either the temporal or replicated nature of the experiments, and often impose constraints on the data collection process, such as regularly spaced samples, or similar sampling schema across replications.\\\\

\textbf{Results}\\\\
We propose hierarchical Gaussian processes as a general model of gene expression time-series, with application to a variety of problems. In particular, we illustrate the method's capacity for missing data imputation, data fusion and clustering.The method can impute data which is missing both systematically and at random: in a hold-out test on real data, performance is significantly better than commonly used imputation methods. The method's ability to model inter- and intra-cluster variance leads to more biologically meaningful clusters. The approach removes the necessity for evenly spaced samples, an advantage illustrated on a developmental Drosophila dataset with irregular replications.\\\\

\textbf{Conclusion}\\\\
The hierarchical Gaussian process model provides an excellent statistical basis for several gene-expression time-series tasks. It has only a few additional parameters over a regular GP, has negligible additional complexity, is easily implemented and can be integrated into several existing algorithms. Our experiments were implemented in python, and are available from the authors' website: \url{http://staffwww.dcs.shef.ac.uk/people/J.Hensman/}.},
  OPTgroup = 	 {}
}

@Article{Alvarez:llfm13,
  author = 	 {Mauricio A. \'Alvarez and David Luengo and Neil D. Lawrence},
  title = 	 {Linear Latent Force Models Using {G}aussian Processes},
  journal = 	 PAMI,
  year = 	 2013,
  month = 	 5,
  day =  	 13,
  volume =	 {35},
  number =	 {11},
  pages =	 {2693--2705},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  note =	 {},
  OPTannote = 	 {},
  pubmedid =	 {24051729},
  doi =		 {10.1109/TPAMI.2013.86},
  linkpdf =	 {http://arxiv.org/pdf/1107.2699},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {multigp},
  OPTlinksoftware = {},
  abstract =	 {Purely data driven approaches for machine learning present
                 difficulties when data is scarce relative to the
                 complexity of the model or when the model is forced
                 to extrapolate. On the other hand, purely mechanistic
                 approaches need to identify and specify all the
                 interactions in the problem at hand (which may not be
                 feasible) and still leave the issue of how to
                 parameterize the system. In this paper, we present a
                 hybrid approach using Gaussian processes and
                 differential equations to combine data driven
                 modelling with a physical model of the system. We
                 show how different, physically-inspired, kernel
                 functions can be developed through sensible, simple,
                 mechanistic assumptions about the underlying
                 system. The versatility of our approach is
                 illustrated with three case studies from motion
                 capture, computational biology and geostatistics.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1107.2699},
  primaryClass = {stat.ML}
}


@Article{Fusi:warped14,
  author = 	 {Nicol\'o Fusi and Christoph Lippert and Neil D. Lawrence and Oliver Stegle},
  title = 	 {Warped linear mixed models for the genetic analysis of transformed phenotypes},
  journal = 	 {Nature Communications},
  year = 	 {2014},
  OPTkey = 	 {},
  volume =	 {5},
  number =	 {4890},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1038/ncomms5890},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = {http://github.com/pmbio/warpedLMM},
  abstract =	 {Linear mixed models (LMMs) are a powerful and established tool for studying genotypeâphenotype relationships. A limitation of the LMM is that the model assumes Gaussian distributed residuals, a requirement that rarely holds in practice. Violations of this assumption can lead to false conclusions and loss in power. To mitigate this problem, it is common practice to pre-process the phenotypic values to make them as Gaussian as possible, for instance by applying logarithmic or other nonlinear transformations. Unfortunately, different phenotypes require different transformations, and choosing an appropriate transformation is challenging and subjective. Here we present an extension of the LMM that estimates an optimal transformation from the observed data. In simulations and applications to real data from human, mouse and yeast, we show that using transformations inferred by our model increases power in genome-wide association studies and increases the accuracy of heritability estimation and phenotype prediction.},
  OPTgroup = 	 {}
}

@Article{Fusi:detecting13,
  author = 	 {Nicol\'o Fusi and Christoph Lippert and Karsten Borgwardt and Neil D. Lawrence and Oliver Stegle},
  title = 	 {Detecting Regulatory Gene-Environment Interactions with Unmeasured Environmental Factors},
  journal = 	 {Bioinformatics},
  year = 	 {2013},
  OPTkey = 	 {},
  volume =	 {29},
  number =	 {11},
  pages =	 {1382--1389},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1093/bioinformatics/btt148},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {\textbf{Motivation}: Genomic studies have revealed a substantial

                 heritable component of the transcriptional state of
                 the cell. To fully understand the genetic regulation
                 of gene expression variability, it is important to
                 study the effect of genotype in the context of
                 external factors such as alternative environmental
                 conditions. In model systems, explicit environmental
                 perturbations have been considered for this purpose,
                 allowing to directly test for environment-specific
                 genetic effects. However, such experiments are
                 limited to species that can be profiled in controlled
                 environments, hampering their use in important
                 systems such as human. Moreover, even in seemingly
                 tightly regulated experimental conditions, subtle
                 environmental perturbations cannot be ruled out, and
                 hence unknown environmental influences are
                 frequent. Here, we propose a model-based approach to
                 simultaneously infer unmeasured environmental factors
                 from gene expression profiles and use them in genetic
                 analyses, identifying environment-specific
                 associations between polymorphic loci and individual
                 gene expression traits.\\\\

                 \textbf{Results}: In extensive simulation studies, we show
                 that our method is able to accurately reconstruct
                 environmental factors and their interactions with
                 genotype in a variety of settings. We further
                 illustrate the use of our model in a real-world
                 dataset in which one environmental factor has been
                 explicitly experimentally controlled. Our method is
                 able to accurately reconstruct the true underlying
                 environmental factor even if it's not given as an
                 input, allowing to detect genuine
                 genotype-environment interactions. In addition to the
                 known environmental factor, we find unmeasured
                 factors involved in novel genotype-environment
                 interactions. Our results suggest that interactions
                 with both known and unknown environmental factors
                 significantly contribute to gene expression
                 variability.\\\\

                 \textbf{Availability}: Software available at \url{http://ml.sheffield.ac.uk/qtl/limmi}\\\\

                  \textbf{Contact}: \url{oliver.stegle@ebi.ac.uk}, \url{nicolo.fusi@sheffield.ac.uk}},
  OPTgroup = 	 {}
}
@InProceedings{Hensman:fast12,
  title = {Fast variational inference in the Conjugate Exponential family},
  author = {James Hensman and Magnus Rattray and Neil D. Lawrence},
  crossref = {Bartlett:nips12},
  linkpdf = {http://books.nips.cc/papers/files/nips25/NIPS2012_1314.pdf},
  year = {2012}
}

@Article{Andrade:consistent14,
  author = 	 {Ricardo Andrade-Pacheco and Martin Mubangizi and John Quinn and Neil D. Lawrence},
  title = 	 {Consistent mapping of government malaria records across a changing territory delimitation},
  journal = 	 {Malaria Journal},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {5},
  year =	 {2014},
  month = 9,
  day = 22,
  OPTeditor = 	 {},
  volume =	 {13},
  number =	 {Suppl 1},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  note =	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1186/1475-2875-13-S1-P5},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}


@InProceedings{Mubangizi:malaria14,
  title = 	 {Malaria surveillance with multiple data sources
using {Gaussian} process models},
  author = 	 {Martin Mubangizi and Ricardo Andrade-Pacheco and Michael Thomas Smith and John Quinn and Neil D. Lawrence},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle =	 {1st International Conference on the Use of Mobile ICT in Africa},
  OPTpages = 	 {},
  year =	 {2014},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  month =	 {9--10 Dec},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://air.ug/papers/MubangiziUMICTA2014.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@InProceedings{Damianou:deepgp13,
  author =	 {Andreas Damianou and Neil D. Lawrence},
  title =	 {Deep {G}aussian Processes},
  crossref =	 {Carvalho:aistats13},
  pages =	 {207--215},
  year =	 2013,
  comments = {yes},
  OPTeditor =	 {},
  volume =	 31,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      shefftp # {deepGPsAISTATS.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {deepGP},
  abstract =	 {In this paper we introduce deep Gaussian process (GP)
                 models. Deep GPs are a deep belief network based on
                 Gaussian process mappings. The data is modeled as the
                 output of a multivariate GP. The inputs to that
                 Gaussian process are then governed by another GP. A
                 single layer model is equivalent to a standard GP or
                 the GP latent variable model (GP-LVM). We perform
                 inference in the model by approximate variational
                 marginalization. This results in a strict lower bound
                 on the marginal likelihood of the model which we use
                 for model selection (number of layers and nodes per
                 layer). Deep belief networks are typically applied to
                 relatively large data sets using stochastic
                 gradient descent for optimization. Our fully Bayesian
                 treatment allows for the application of deep models
                 even when data is scarce. Model selection by our
                 variational bound shows that a five layer hierarchy
                 is justified even when modelling a digit data set
                 containing only 150 examples.},
  OPTgroup =	 {}
}


@Article{Durrande:periodicities16,
  author = 	 {Nicolas Durrande and James Hensman and Magnus Rattray and Neil D. Lawrence},
  title = 	 {Detecting periodicities with {Gaussian} processes},
  journal = 	 {PeerJ Computer Science},
  year = 	 {2016},
  OPTkey = 	 {},
  volume =	 {2},
  OPTnumber = 	 {},
  pages =	 {e50},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi = {10.7717/peerj-cs.t50},
  label1 = {Preprint},
  link1 = {https://dx.doi.org/10.7287/peerj.preprints.1743v1},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {We consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. Our approach is based on Gaussian process regression which provides a flexible non-parametric framework for modelling periodic data. We introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. This decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. To quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. Although the method can be applied to many kernels, we give a special emphasis to the MatÃ©rn family, from the expression of the reproducing kernel Hilbert space inner product to the implementation of the associated periodic kernels in a Gaussian process toolkit. The proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.},
  OPTgroup = 	 {}
}

@TechReport{Zwiessele:topslam16,
  author = 	 {Max Zwiessele and Neil D. Lawrence},
  title = 	 {Topslam: Waddington Landscape Recovery for Single Cell Experiments},
  institution =  {},
  day = {20},
  month = {6},
  year = 	 {2016},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1101/057778},
  linkpdf =	 {http://biorxiv.org/content/early/2016/06/08/057778.full.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {We present an approach to estimating the nature of the Waddington (or epigenetic) landscape that underlies a population of individual cells. Through exploiting high resolution single cell transcription experiments we show that cells can be located on a landscape that reflects their differentiated nature. Our approach makes use of probabilistic non-linear dimensionality reduction that respects the topology of our estimated epigenetic landscape. In simulation studies and analyses of real data we show that the approach, known as \manifold, outperforms previous attempts to understand the differentiation landscape. Hereby, the novelty of our approach lies in the correction of distances \emph{before} extracting ordering information. This gives the advantage over other attempts, which have to correct for extracted time lines by post processing or additional data.},
  OPTgroup = 	 {}
}

@TechReport{Smith:dpgp16,
  author = 	 {Michael Thomas Smith and Max Zwiessele and Neil D. Lawrence},
  title = 	 {Differentially Private Gaussian Processes},
  institution =  {},
  day = {2},
  month = {6},
  year = 	 {2016},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {https://arxiv.org/abs/1606.00720},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {A major challenge for machine learning is increasing the availability of data while respecting the privacy of individuals. Differential privacy is a framework which allows algorithms to have provable privacy guarantees. Gaussian processes are a widely used approach for dealing with uncertainty in functions. This paper explores differentially private mechanisms for Gaussian processes. We compare binning and adding noise before regression with adding noise post-regression. For the former we develop a new kernel for use with binned data. For the latter we show that using inducing inputs allows us to reduce the scale of the added perturbation. We find that, for the datasets used, adding noise to a binned dataset has superior accuracy. Together these methods provide a starter toolkit for combining differential privacy and Gaussian processes.},
  OPTgroup = 	 {}
}

@TechReport{Hensman:nested14,
  author = 	 {James Hensman and Neil D. Lawrence},
  title = 	 {Nested Variational Compression in Deep {G}aussian Processes},
  institution =  {University of Sheffield},
  year = 	 {2014},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Brockington:unravelling13,
  title = 	 {Unravelling the enigma of selective vulnerability in neurodegeneration: motor neurons resistant to degeneration in {ALS} show distinct gene expression characteristics and decreased susceptibility to excitotoxicity},
  author = 	 {Alice Brockington and Ke Ning and Paul R. Heath and Elizabeth Wood and Janine Kirby and Nicol\'o Fusi and Neil D. Lawrence and Stephen B. Wharton and Paul G. Ince and Pamela J. Shaw},
  journal = 	 {Acta Neuropathologica},
  year = 	 {2013},
  OPTkey = 	 {},
  volume =	 {125},
  number =	 {1},
  OPTpages = 	 {95--109},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1007/s00401-012-1058-5},
  linkpdf =	 {http://link.springer.com/content/pdf/10.1007%2Fs00401-012-1058-5},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {A consistent clinical feature of amyotrophic lateral sclerosis (ALS) is the sparing of eye movements and the function of external sphincters, with corresponding preservation of motor neurons in the brainstem oculomotor nuclei, and of Onufâs nucleus in the sacral spinal cord. Studying the differences in properties of neurons that are vulnerable and resistant to the disease process in ALS may provide insights into the mechanisms of neuronal degeneration, and identify targets for therapeutic manipulation. We used microarray analysis to determine the differences in gene expression between oculomotor and spinal motor neurons, isolated by laser capture microdissection from the midbrain and spinal cord of neurologically normal human controls. We compared these to transcriptional profiles of oculomotor nuclei and spinal cord from rat and mouse, obtained from the GEO omnibus database. We show that oculomotor neurons have a distinct transcriptional profile, with significant differential expression of 1,757 named genes (q < 0.001). Differentially expressed genes are enriched for the functional categories of synaptic transmission, ubiquitin-dependent proteolysis, mitochondrial function, transcriptional regulation, immune system functions, and the extracellular matrix. Marked differences are seen, across the three species, in genes with a function in synaptic transmission, including several glutamate and GABA receptor subunits. Using patch clamp recording in acute spinal and brainstem slices, we show that resistant oculomotor neurons show a reduced AMPA-mediated inward calcium current, and a higher GABA-mediated chloride current, than vulnerable spinal motor neurons. The findings suggest that reduced susceptibility to excitotoxicity, mediated in part through enhanced GABAergic transmission, is an important determinant of the relative resistance of oculomotor neurons to degeneration in ALS.},
  OPTgroup = 	 {}
}

@InCollection{Honkela:mining12,
  author = 	 {Antti Honkela and Magnus Rattray and Neil D. Lawrence},
  title = 	 {Mining Regulatory Network Connections by Ranking Transcription Factor Target Genes Using Time Series Expression Data},
  booktitle = 	 {Data Mining for Systems Biology},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTpages = 	 {59--67},
  publisher =	 springer,
  year =	 {2013},
  editor =	 {Hiroshi Mamitsuka and Charles DeLisi and Minoru Kanehisa},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  ISBN = 	 {978-1-62703-107-3},
  OPTchapter = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@InProceedings{Tosi:metrics14,
  author = 	 {Alessandra Tosi and SÃ¸ren Hauberg and Alfredo Vellido and Neil D. Lawrence},
  title = 	 {Metrics for Probabilistic Geometries},
  crossref =	 {Zhang:uai14},
  OPTkey = 	 {},
  OPTbooktitle = {},
  pages =	 {800--808},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://auai.org/uai2014/proceedings/individuals/171.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {We investigate the geometrical structure of probabilistic generative dimensionality reduction models using the tools of Riemannian geometry. We explicitly define a distribution over the natural metric given by the models. We provide the necessary algorithms to compute expected metric tensors where the distribution over mappings is given by a Gaussian process. We treat the corresponding latent variable model as a Riemannian manifold and we use the expectation of the metric under the Gaussian process prior to define interpolating paths and measure distance between latent points. We show how distances that respect the expected metric lead to more appropriate generation of new data.},
  OPTgroup = 	 {}
}

@InProceedings{Hensman:bigdata13,
  author = 	 {James Hensman and Nicol\'o Fusi and Neil D. Lawrence},
  title = 	 {{G}aussian Processes for Big Data},
  crossref =	 {Nicholson:uai13},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://auai.org/uai2013/prints/papers/244.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Titsias:identifying12,
  author = 	 {Michalis K. Titsias and Antti Honkela and Neil D. Lawrence and Magnus Rattray},
  title = 	 {Identifying Targets of Multiple Co-regulated Transcription Factors from Expression Time-series by {B}ayesian Model Comparison},
  journal = 	 {BMC Systems Biology},
  year = 	 {2012},
  OPTkey = 	 {},
  volume =	 {6},
  number =	 {53},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  note =	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1186/1752-0509-6-53},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTgroup = 	 {},
  abstract =	 {\textbf{Background}\\\\

                  Complete transcriptional regulatory network
                  inference is a huge challenge because of the
                  complexity of the network and sparsity of available
                  data. One approach to make it more manageable is to
                  focus on the inference of context-speciï¬c networks
                  involving a few interacting transcription factors
                  (TFs) and all of their target genes.

                  \textbf{Results}\\\\

                  We present a computational framework for Bayesian
                  statistical inference of target genes of multiple
                  interacting TFs from high-throughput gene expression
                  time-series data. We use ordinary differential
                  equation models that describe transcription of
                  target genes taking into account combinatorial
                  regulation. The method consists of a training and a
                  prediction phase. During the training phase we infer
                  the unobserved TF protein concentrations on a
                  subnetwork of approximately known regulatory
                  structure. During the prediction phase we apply
                  Bayesian model selection on a genome-wide scale and
                  score all alternative regulatory structures for each
                  target gene. We use our methodology to identify
                  targets of ï¬ve TFs regulating Drosophila
                  melanogaster mesoderm development. We ï¬nd that
                  conï¬dent predicted links between TFs and targets are
                  signiï¬cantly enriched for supporting ChIP-chip
                  binding events and annotated TF-gene
                  interations. Our method statistically signiï¬cantly
                  outperforms existing alternatives.

                  \textbf{Conclusions}\\\\

                  Our results show that it is possible to infer
                  regulatory links between multiple interacting TFs
                  and their target genes even from a single relatively
                  short time series and in presence of unmodelled
                  confounders and unreliable prior knowledge on
                  training network connectivity. Introducing data from
                  several different experimental perturbations
                  signiï¬cantly increases the accuracy.}
}

@techreport{Hensman:bigdata12,
  title={{G}aussian Processes for Big Data with Stochastic Variational Inference},
  author={James Hensman and Neil D. Lawrence},
  journal={Submitted to NIPS 2012 Workshop},
  year={2012}
}

@Article{Donaldson:genome12,
  author = 	 {Ian J. Donaldson and Shilu Amin and James Hensman and Eva Kutejova and Magnus Rattray and Neil D. Lawrence and Andrew Hayes and Christopher M. Ward and Nicoletta Bobola},
  title = 	 {Genome-wide occupancy links Hoxa2 to Wnt-$\beta$-catenin signaling in mouse embryonic development},
  journal = 	 {Nucleaic Acids Research},
  year = 	 2012,
  OPTkey = 	 {},
  volume =	 {40},
  number =	 {9},
  pages =	 {3390--4001},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1093/nar/gkr1240},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {The regulation of gene expression is central to
                 developmental programs and largely depends on the
                 binding of sequence-specific transcription factors
                 with cis-regulatory elements in the genome. Hox
                 transcription factors specify the spatial coordinates
                 of the body axis in all animals with bilateral
                 symmetry, but a detailed knowledge of their molecular
                 function in instructing cell fates is lacking. Here,
                 we used chromatin immunoprecipitation with massively
                 parallel sequencing (ChIP-seq) to identify Hoxa2
                 genomic locations in a time and space when it is
                 actively instructing embryonic development in
                 mouse. Our data reveals that Hoxa2 has large genome
                 coverage and potentially regulates thousands of
                 genes. Sequence analysis of Hoxa2-bound regions
                 identifies high occurrence of two main classes of
                 motifs, corresponding to Hox and Pbx--Hox recognition
                 sequences. Examination of the binding targets of
                 Hoxa2 faithfully captures the processes regulated by
                 Hoxa2 during embryonic development; in addition, it
                 uncovers a large cluster of potential targets
                 involved in the Wnt-signaling pathway. In vivo
                 examination of canonical Wnt--$\beta$-catenin
                 signaling reveals activity specifically in Hoxa2
                 domain of expression, and this is undetectable in
                 Hoxa2 mutant embryos. The comprehensive mapping of
                 Hoxa2-binding sites provides a framework to study Hox
                 regulatory networks in vertebrate developmental
                 processes.},
  OPTgroup = 	 {}
}

@InCollection{Lawrence:gpinference11,
  author = 	 {Neil D. Lawrence and Magnus Rattray and Antti Honkela and Michalis K. Titsias},
  title = 	 {{G}aussian Process Inference for Differential Equation Models of Transcriptional Regulation},
  booktitle = 	 {Handbook of Statistical Systems Biology},
  OPTcrossref =  {},
  OPTkey = 	 {},
  pages =	 {376--394},
  publisher =	 wiley,
  year =	 2011,
  editor =	 {Michael P. H. Stumpf and David J. Balding and Mark Girolami},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  chapter =	 19,
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1002/9781119970606.ch19},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Lazaro:overlapping11,
  author = 	 {Miguel {L\'azaro Gredilla} and Steven {Van Vaerenbergh} and Neil D. Lawrence},
  title = 	 {Overlapping Mixtures of {G}aussian Processes for the Data Association Problem},
  journal = 	 {Pattern Recognition},
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 10,
  number =	 4,
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1016/j.patcog.2011.10.004},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {In this work we introduce a mixture of GPs to address the data association problem, i.e., to label a group of observations according to the sources that generated them. Unlike several previously proposed GP mixtures, the novel mixture has the distinct characteristic of using no gating function to determine the association of samples and mixture components. Instead, all the GPs in the mixture are global and samples are clustered following ``trajectories'' across input space. We use a non-standard variational Bayesian algorithm to efficiently recover sample labels and learn the hyperparameters. We show how multi-object tracking problems can be disambiguated and also explore the characteristics of the model in traditional regression settings.},
  OPTgroup = 	 {}
}

@Article{Honkela:tigre11,
  author = 	 {Antti Honkela and Pei Gao and Jonatan Ropponen and Magnus Rattray and Neil D. Lawrence},
  title = 	 {tigre: Transcription factor inference through {Gaussian} process reconstruction of expression for Bioconductor},
  journal = 	 {Bioinformatics},
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 {27},
  linkpdf = 	 {http://bioinformatics.oxfordjournals.org/content/27/7/1026.full.pdf+html},
  pages =	 {1026--1027},
  doi =		 {10.1093/bioinformatics/btr057},
  masid = 	 {39311794},
  abstract = 	 {\textbf{Summary}: tigre is an R/Bioconductor package for
                 inference of transcription factor activity and
                 ranking candidate target genes from gene expression
                 time series. The underlying methodology is based on
                 Gaussian process inference on a differential equation
                 model that allows the use of short, unevenly sampled,
                 time series. The method has been designed with
                 efficient parallel implementation in mind, and the
                 package supports parallel operation even without
                 additional software.\\\\

                 \textbf{Availability}: The tigre package is included
                 in Bioconductor since release 2.6 for R 2.11. The
                 package and a user's guide are available at
                 http://www.bioconductor.org.\\\\

                 \textbf{Contact}: antti.honkela@hiit.fi;
                 m.rattray@sheffield.ac.uk; n.lawrence@dcs.shef.ac.uk}
}

@Article{Alvarez:vector12,
  author = 	 {Mauricio A. \'Alvarez and Lorenzo Rosasco and Neil D. Lawrence},
  title = 	 {Kernels for Vector-Valued Functions: A Review},
  journal = 	 {Foundations and Trends in Machine Learning},
  year = 	 {2012},
  OPTkey = 	 {},
  volume =	 4,
  number =	 3,
  pages =	 {195--266},
  OPTmonth = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1561/2200000036},
  linkpdf =	 shefftp # {2200000036.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Kernel methods are among the most popular techniques in machine
learning. From a regularization perspective they play a central role
in regularization theory as they provide a natural choice for the
hypotheses space and the regularization functional through the notion
of reproducing kernel Hilbert spaces. From a probabilistic perspec-
tive they are the key in the context of Gaussian processes, where
the kernel function is known as the covariance function. Traditionally,
kernel methods have been used in supervised learning problems with
scalar outputs and indeed there has been a considerable amount of
work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple
outputs, motivated partially by frameworks like multitask learning. In
this monograph, we review different methods to design or learn valid
kernel functions for multiple outputs, paying particular attention to the
connection between probabilistic and functional methods.

},
  OPTgroup = 	 {}
}

@Article{Alvarez:computationally11,
  author = 	 {Mauricio A. \'Alvarez and Neil D. Lawrence},
  title = 	 {Computationally Efficient Convolved Multiple Output {Gaussian} Processes},
  journal = 	 jmlr,
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 12,
  OPTnumber = 	 {},
  pages =	 {1425--1466},
  month =	 {May},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://www.jmlr.org/papers/volume12/alvarez11a/alvarez11a.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {multigp},

  abstract =	 {Recently there has been an increasing interest in regression
                 methods that deal with multiple outputs. This has
                 been motivated partly by frameworks like multitask
                 learning, multisensor networks or structured output
                 data. From a Gaussian processes perspective, the
                 problem reduces to specifying an appropriate
                 covariance function that, whilst being positive
                 semi-definite, captures the dependencies between all
                 the data points and across all the outputs. One
                 approach to account for non-trivial correlations
                 between outputs employs convolution processes. Under
                 a latent function interpretation of the convolution
                 transform we establish dependencies between output
                 variables. The main drawbacks of this approach are
                 the associated computational and storage demands. In
                 this paper we address these issues. We present
                 different efficient approximations for dependent
                 output Gaussian processes constructed through the
                 convolution formalism. We exploit the conditional
                 independencies present naturally in the model. This
                 leads to a form of the covariance similar in spirit
                 to the so called PITC and FITC approximations for a
                 single output. We show experimental results with
                 synthetic and real data, in particular, we show
                 results in school exams score prediction, pollution
                 prediction and gene expression data},
  OPTgroup = 	 {}
}


@Article{Kalaitzis:simple11,
  author = 	 {Alfredo A. Kalaitzis and Neil D. Lawrence},
  title = 	 {A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through {Gaussian} Process Regression},
  journal = 	 {BMC Bioinformatics},
  year = 	 2011,
  OPTkey = 	 {},
  volume =	 12,
  number =	 180,
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  pubmedid =	 {21599902},
  doi =		 {10.1186/1471-2105-12-180},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = {https://www.bioconductor.org/packages/release/bioc/html/gprege.html},
  link1 =        {https://github.com/SheffieldML/gprege},
  label1 = {Github Repo},
  link2 = {https://cran.r-project.org/web/packages/gptk/gptk.pdf},
  label2 = {GPTK R Toolkit},
  abstract =	 {{\bf Background}\\\\
The analysis of gene expression from time series underpins many biological studies. Two basic forms of analysis recur for data of this type: removing inactive (quiet) genes from the study and determining which genes are differentially expressed. Often these analysis stages are applied disregarding the fact that the data is drawn from a time series. In this paper we propose a simple model for accounting for the underlying temporal nature of the data based on a Gaussian process.\\\\

{\bf Results}\\\\
We review Gaussian process (GP) regression for estimating the continuous trajectories underlying in gene expression time-series. We present a simple approach which can be used to filter quiet genes, or for the case of time series in the form of expression ratios, quantify differential expression. We assess via ROC curves the rankings produced by our regression framework and compare them to a recently proposed hierarchical Bayesian model for the analysis of gene expression time-series (BATS). We compare on both simulated and experimental data showing that the proposed approach significantly outperforms the current state of the art.\\\\

{\bf Conclusions}\\\\
Gaussian processes offer an attractive trade-off between efficiency and usability for the analysis of micro-array time series. The Gaussian process framework offers a natural way of handling biological replicates and missing values and provides confidence intervals along the estimated curves of gene expression. Therefore, we believe Gaussian processes should be a standard tool in the analysis of gene expression time series.},
  OPTgroup = 	 {}
}

@Article{Asif:tfinfer10,
  author = 	 {H. M. Shahzad Asif and Matthew D. Rolfe and Jeff Green and Neil D. Lawrence and Magnus Rattray and Guido Sanguinetti},
  title = 	 {TFInfer: a tool for probabilistic inference of transcription factor activities},
  journal = 	 bioinf,
  year = 	 2010,
  OPTkey = 	 {},
  volume =	 {26},
  OPTnumber = 	 {},
  pages =	 {2635--2636},
  linkpdf = 	 {http://bioinformatics.oxfordjournals.org/content/26/20/2635.full.pdf+html},
  abstract =	 {\textbf{Summary}: TFInfer is a novel open access, standalone tool for genome-wide inference of transcription factor activities from gene expression data. Based on an earlier MATLAB version, the software has now been extended in a number of ways. It has been significantly optimised in terms of performance, and it was given novel functionality, by allowing the user to model both time series and data from multiple independent conditions. With a full documentation and intuitive graphical user interface, together with an in-built data base of yeast and Escherichia coli transcription factors, the software does not require any mathematical or computational expertise to be used effectively.\\

\textbf{Availability}: \url{http://homepages.inf.ed.ac.uk/gsanguin/TFInfer.html}

\textbf{Contact}: \url{gsanguin@staffmail.ed.ac.uk}}
}

@InProceedings{Kalaitzis:rca12,
  author = 	 {Alfredo A. Kalaitzis and Neil D. Lawrence},
  title = 	 {Residual Component Analysis},
  crossref =	 {Langford:icml12},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://icml.cc/2012/papers/114.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Probabilistic principal component analysis (PPCA) seeks a
                 low dimensional representation of a data set in the
                 presence of independent spherical Gaussian noise,
                 $\Sigma = \sigma^2\mathbf{I}$. The maximum likelihood
                 solution for the model is an eigenvalue problem on
                 the sample covariance matrix. In this paper we
                 consider the situation where the data variance is
                 already partially explained by other factors,
                 e.g. conditional dependencies between the covariates,
                 or temporal correlations leaving some residual
                 variance. We decompose the residual variance into its
                 components through a generalised eigenvalue problem,
                 which we call residual component analysis (RCA). We
                 explore a range of new algorithms that arise from the
                 framework, including one that factorises the
                 covariance of a Gaussian density into a low-rank and
                 a sparse-inverse component. We illustrate the ideas
                 on the recovery of a protein-signaling network, a
                 gene expression time-series data set and the recovery
                 of the human skeleton from motion capture 3-D cloud
                 data.},
  group =	 {}
}
@InProceedings{Damianou:manifold12,
  author = 	 {Andreas Damianou and Carl Henrik Ek and Michalis K. Titsias and Neil D. Lawrence},
  title = 	 {Manifold Relevance Determination},
  crossref =	 {Langford:icml12},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 shefftp # {mrdICML2012.pdf},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {In this paper we present a fully Bayesian latent
                 variable model which exploits conditional nonlinear
                 (in)-dependence structures to learn an efficient
                 latent representation. The latent space is factorized
                 to represent shared and private information from
                 multiple views of the data. In contrast to previous
                 approaches, we introduce a relaxation to the discrete
                 segmentation and allow for a ``softly'' shared latent
                 space. Further, Bayesian techniques allow us to
                 automatically estimate the dimensionality of the
                 latent spaces. The model is capable of capturing
                 structure underlying extremely high dimensional
                 spaces. This is illustrated by modelling unprocessed
                 images with tenths of thousands of pixels.  This also
                 allows us to directly generate novel images from the
                 trained model by sampling from the discovered latent
                 spaces. We also demonstrate the model by prediction
                 of human pose in an ambiguous setting. Our Bayesian
                 framework allows us to perform disambiguation in a
                 principled manner by including latent space priors
                 which incorporate the dynamic nature of the data.},
  group = {}
}

@InCollection{Titsias:mcmcgp11,
  author =	 {Michalis K. Titsias and Magnus Rattray and Neil D. Lawrence},
  title = 	 {Markov chain {M}onte {C}arlo algorithms for {G}aussian processes},
  chapter = 	 {14},
  year = 	 {2011},
  crossref =	 {Barber:bayestime11},
  OPTkey = 	 {},
  OPTpages = 	 {},
  OPTpublisher = {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  OPTchapter = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Honkela:modelbased10,
  author =	 {Antti Honkela and Charles Girardot and E. Hilary Gustafson and Ya-Hsin Liu and Eileen E. M. Furlong and Neil D. Lawrence and Magnus Rattray},
  title =	 {Model-based Method for Transcription Factor Target
                  Identification with Limited Data},
  journal =	 pnasusa,
  year =	 2010,
  linksoftware = softwarehttp # {disimrank},
  volume =	 {107},
  pages =	 {7793--7798},
  number =	 {17},
  month =	 {Apr},
  day = 27,
  doi =		 {10.1073/pnas.0914285107},
  abstract =	 {We present a computational method for identifying potential targets of a transcription factor (TF) using wild-type gene expression time series data. For each putative target gene we fit a simple differential equation model of transcriptional regulation, and the model likelihood serves as a score to rank targets. The expression profile of the TF is modeled as a sample from a Gaussian process prior distribution that is integrated out using a nonparametric Bayesian procedure. This results in a parsimonious model with relatively few parameters that can be applied to short time series datasets without noticeable overfitting. We assess our method using genome-wide chromatin immunoprecipitation (ChIP-chip) and loss-of-function mutant expression data for two TFs, Twist, and Mef2, controlling mesoderm development in Drosophila. Lists of top-ranked genes identified by our method are significantly enriched for genes close to bound regions identified in the ChIP-chip data and for genes that are differentially expressed in loss-of-function mutants. Targets of Twist display diverse expression profiles, and in this case a model-based approach performs significantly better than scoring based on correlation with TF expression. Our approach is found to be comparable or superior to ranking based on mutant differential expression scores. Also, we show how integrating complementary wild-type spatial expression data can further improve target ranking performance.}
}

@InProceedings{Damianou:vgpds11,
  author = 	 {Andreas Damianou and Michalis K. Titsias and Neil D. Lawrence},
  title = 	 {Variational {Gaussian} Process Dynamical Systems},
  crossref =	 {Bartlett:nips11},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 shefftp # {VGPDS_Nips11.pdf},
  label1 = {Supplementary Videos},
  link1 = {http://staffwww.dcs.shef.ac.uk/people/A.Damianou/varFiles/VGPDS/index.html},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {vargplvm},
  abstract =	 {High dimensional time series are endemic in applications
                 of machine learning such as robotics (sensor data),
                 computational biology (gene expression data), vision
                 (video sequences) and graphics (motion capture
                 data). Practical nonlinear probabilistic approaches
                 to this data are required. In this paper we introduce
                 the variational Gaussian process dynamical
                 system. Our work builds on recent variational
                 approximations for Gaussian process latent variable
                 models to allow for nonlinear dimensionality
                 reduction simultaneously with learning a dynamical
                 prior in the latent space. The approach also allows
                 for the appropriate dimensionality of the latent
                 space to be automatically determined. We demonstrate
                 the model on a human motion capture data set and a
                 series of high resolution video sequences.},
  OPTgroup = 	 {}
}

@InProceedings{Alvarez:switched10,
  author = 	 {Mauricio A. \'Alvarez and Jan Peters and Bernhard Sch\"olkopf and Neil D. Lawrence},
  title = 	 {Switched Latent Force Models for Movement Segmentation},
  crossref =	 {Lafferty:nips10},
  OPTkey = 	 {},
  pages =	 {55--63},
  pdf =	   	 {http://books.nips.cc/papers/files/nips23/NIPS2010_1222.pdf},
  linkvideo = 	 {http://videolectures.net/nips2010_alvarez_slf/},
  OPTorganization = {},
  OPTpublisher = {},
  abstract =	 {Latent force models encode the interaction between multiple
                 related dynamical systems in the form of a kernel or
                 covariance function. Each variable to be modeled is
                 represented as the output of a differential equation
                 and each differential equation is driven by a
                 weighted sum of latent functions with uncertainty
                 given by a Gaussian process prior. In this paper we
                 consider employing the latent force model framework
                 for the problem of determining robot motor
                 primitives. To deal with discontinuities in the
                 dynamical systems or the latent driving force we
                 intro- duce an extension of the basic latent force
                 model, that switches between different latent
                 functions and potentially different dynamical
                 systems. This creates a versatile representation
                 for robot movements that can capture discrete changes
                 and non-linearities in the dynamics. We give
                 illustrative examples on both synthetic data and for
                 striking movements recorded using a Barrett WAM robot
                 as haptic in- put device. Our inspiration is robot
                 motor primitives, but we expect our model to have
                 wide application for dynamical systems including
                 models for human motion capture data and systems
                 biology.}
}

@InProceedings{Alvarez:efficient10,
  author =	 {Mauricio A. \'Alvarez and David Luengo and Michalis K. Titsias and Neil D. Lawrence},
  title =	 {Efficient Multioutput {G}aussian Processes through
                  Variational Inducing Kernels},
  crossref =	 {Teh:aistats10},
  pages =	 {25--32},
  year =	 2010,
  OPTeditor =	 {},
  volume =	 9,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v9/alvarez10a/alvarez10a.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {multigp},
  abstract =	 {Interest in multioutput kernel methods is
                  increasing, whether under the guise of multitask
                  learning, multisensor networks or structured output
                  data. From the Gaussian process perspective a
                  multioutput Mercer kernel is a covariance function
                  over correlated output functions. One way of
                  constructing such kernels is based on convolution
                  processes (CP). A key problem for this approach is
                  efficient inference. \'Alvarez and Lawrence
                  \cite{Alvarez:convolved08} recently presented a
                  sparse approximation for CPs that enabled efficient
                  inference. In this paper, we extend this work in two
                  directions: we introduce the concept of variational
                  inducing functions to handle potential non-smooth
                  functions involved in the kernel CP construction and
                  we consider an alternative approach to approximate
                  inference based on variational methods, extending
                  the work by Titsias \cite{Titsias:variational09} to
                  the multiple output case. We demonstrate our
                  approaches on prediction of school marks, compiler
                  performance and financial time series.},
  OPTgroup =	 {}
}

@InProceedings{Lawrence:spectral11,
  author =	 {Neil D. Lawrence},
  title =	 {Spectral Dimensionality Reduction via Maximum Entropy},
  crossref =	 {Gordon:aistats11},
  errata1 =	 {Equation (4) on Page 55, the sum index outside the norm should be over $i$, not $j$.},
  errataCredit1 = {},
  pages =	 {51--59},
  year =	 2011,
  OPTeditor =	 {},
  volume =	 15,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v15/lawrence11a/lawrence11a.pdf},
  OPTlinkps =	 {},
  linkvideo =	 {http://videolectures.net/aistats2011_lawrence_spectral/},
  videolectures =	 {aistats2011_lawrence_spectral},
  linksoftware = sheffieldgit # {meu},
  abstract =	 {We introduce a new perspective on spectral dimensionality reduction
  which views these methods as Gaussian random fields (GRFs). Our
  unifying perspective is based on the maximum entropy principle which
  is in turn inspired by maximum variance unfolding. The resulting
  probabilistic models are based on GRFs. The resulting model is a
  nonlinear generalization of principal component analysis. We show
  that parameter fitting in the locally linear embedding is
  approximate maximum likelihood in these models. We directly maximize
  the likelihood and show results that are competitive with the
  leading spectral approaches on a robot navigation visualization and
  a human motion capture data set.},
  note =	 {Notable Paper}
}


@InProceedings{Titsias:bayesGPLVM10,
  author =	 {Michalis K. Titsias and Neil D. Lawrence},
  title =	 {Bayesian {G}aussian Process Latent Variable Model},
  crossref =	 {Teh:aistats10},
  pages =	 {844--851},
  year =	 2010,
  OPTeditor =	 {},
  volume =	 9,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v9/titsias10a/titsias10a.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {vargplvm},
  abstract =	 {We introduce a variational inference framework for
                  training the Gaussian process latent variable model
                  and thus performing Bayesian nonlinear
                  dimensionality reduction. This method allows us to
                  variationally integrate out the input variables of
                  the Gaussian process and compute a lower bound on
                  the exact marginal likelihood of the nonlinear
                  latent variable model. The maximization of the
                  variational lower bound provides a Bayesian training
                  procedure that is robust to overfitting and can
                  automatically select the dimensionality of the
                  nonlinear latent space. We demonstrate our method on
                  real world datasets. The focus in this paper is on
                  dimensionality reduction problems, but the
                  methodology is more general. For example, our
                  algorithm is immediately applicable for training
                  Gaussian process models in the presence of missing
                  or uncertain inputs.},
  OPTgroup =	 {}
}

@Article{Zampini:elementary09,
  author =	 {Valeria Zampini and Stuart Leigh Johnson and
                  Christoph Franz and Neil D. Lawrence and Stefan
                  Muenkner Jutta Engel and Marlies Knipper and Jacopo
                  Magistretti and Sergio Masetto and Walter Marcotti},
  title = 	 {Elementary properties of {CaV}1.3 {Ca}2+ channels expressed in mouse cochlear inner hair cells},
  journal = 	 {The Journal of Physiology},
  year = 	 2010,
  OPTkey = 	 {},
  volume =	 {588},
  OPTnumber = 	 {},
  pages =	 {187--189},
  OPTmonth = 	 {},
  OPTannote = 	 {},
  pmid =	 {19917569},
  doi =		 {10.1113/jphysiol.2009.181917},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Mammalian cochlear inner hair cells (IHCs) are specialized to process developmental signals during immature stages and sound stimuli in adult animals. These signals are conveyed onto auditory afferent nerve fibres. Neurotransmitter release at IHC ribbon synapses is controlled by L-type CaV1.3 Ca2+ channels, the biophysics of which are still unknown in native mammalian cells. We have investigated the localization and elementary properties of Ca2+ channels in immature mouse IHCs under near-physiological recording conditions. CaV1.3 Ca2+ channels at the cell pre-synaptic site co-localize with about half of the total number of ribbons present in immature IHCs. These channels activated at relatively hyperpolarized membrane potentials (about -70 mV), showed a relatively short first latency and weak inactivation, which would allow IHCs to generate and accurately encode spontaneous Ca2+ action potential activity characteristic of these immature cells. The CaV1.3 Ca2+ channels showed a very low open probability (about 0.15 at -20 mV: near the peak of an action potential). Comparison of elementary and macroscopic Ca2+ currents indicated that very few Ca2+ channels are associated with each docked vesicle at IHC ribbon synapses. Finally, we found that the open probability of Ca2+ channels, but not their opening time, was voltage dependent. This finding provides a possible correlation between presynaptic Ca2+ channel properties and the characteristic frequency/amplitude of EPSCs in auditory afferent fibres.},
  OPTgroup = 	 {}
}
@InProceedings{Darby:backing09,
  author =	 {John Darby and Baihua Li and Nicholas Costen and
                  David J. Fleet and Neil D. Lawrence},
  title =	 {Backing Off: Hierarchical Decomposition of Activity
                  for 3D Novel Pose Recovery},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {British Machine Vision Conference},
  OPTpages =	 {},
  year =	 2009,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {BMVC2009CR06.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = {http://www.docm.mmu.ac.uk/STAFF/J.Darby/code.htm},
  abstract =	 {For model-based 3D human pose estimation, even
                  simple models of the human body lead to
                  high-dimensional state spaces. Where the class of
                  activity is known a priori, lowdimensional activity
                  models learned from training data make possible a
                  thorough and efficient search for the best
                  pose. Conversely, searching for solutions in the
                  full state space places no restriction on the class
                  of motion to be recovered, but is both difficult and
                  expensive. This paper explores a potential middle
                  ground between these approaches, using the
                  hierarchical Gaussian process latent variable model
                  to learn activity at different hierarchical scales
                  within the human skeleton. We show that by training
                  on full-body activity data then descending through
                  the hierarchy in stages and exploring subtrees
                  independently of one another, novel poses may be
                  recovered. Experimental results on motion capture
                  data and monocular video sequences demonstrate the
                  utility of the approach, and comparisons are drawn
                  with existing low-dimensional activity models},
  OPTgroup =	 {}
}

@TechReport{Kalaitzis:rca11,
  author = 	 {Alfredo A. Kalaitzis and Neil D. Lawrence},
  title = 	 {Residual Component Analysis},
  institution =  {University of Sheffield},
  year = 	 2011,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  note =	 {arXiv report},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1106.4333v1},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {rca},
  OPTlinksoftware = {},
  abstract =	 {Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set in the presence of independent spherical Gaussian noise, $\Sigma = (\sigma^2)\mathbf{I}$. The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix. In this paper we consider the situation where the data variance is already partially explained by other factors, e.g. covariates of interest, or temporal correlations leaving some residual variance. We decompose the residual variance into its components through a generalized eigenvalue problem, which we call residual component analysis (RCA). We show that canonical covariates analysis (CCA) is a special case of our algorithm and explore a range of new algorithms that arise from the framework. We illustrate the ideas on a gene expression time series data set and the recovery of human pose from silhouette.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1106.4333},
  primaryClass = {stat.ML},
}
@TechReport{Alvarez:llfm11,
  author = 	 {Mauricio A. \'Alvarez and David Luengo and Neil D. Lawrence},
  title = 	 {Linear Latent Force Models Using {G}aussian Processes},
  institution =  {University of Sheffield},
  year = 	 2011,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1107.2699},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {multigp},
  OPTlinksoftware = {},
  abstract =	 {Purely data driven approaches for machine learning present
                 difficulties when data is scarce relative to the
                 complexity of the model or when the model is forced
                 to extrapolate. On the other hand, purely mechanistic
                 approaches need to identify and specify all the
                 interactions in the problem at hand (which may not be
                 feasible) and still leave the issue of how to
                 parameterize the system. In this paper, we present a
                 hybrid approach using Gaussian processes and
                 differential equations to combine data driven
                 modelling with a physical model of the system. We
                 show how different, physically-inspired, kernel
                 functions can be developed through sensible, simple,
                 mechanistic assumptions about the underlying
                 system. The versatility of our approach is
                 illustrated with three case studies from motion
                 capture, computational biology and geostatistics.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1107.2699},
  primaryClass = {stat.ML}
}
@TechReport{Alvarez:kernels11,
  author = 	 {Mauricio A. \'Alvarez and Lorenzo Rosasco and Neil D. Lawrence},
  title = 	 {Kernels for Vector-Valued Functions: a Review},
  institution =  {University of Sheffield},
  year = 	 2011,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1106.6251v1},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {multigp},
  OPTlinksoftware = {},
  abstract =	 {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1106.6251},
  primaryClass = {stat.ML},
}


@Article{Penfold:meiotic12,
  author = 	 {Christopher A. Penfold and Paul E. Brown and Neil D. Lawrence and Alastair S. H. Goldman},
  title = 	 {Modeling Meiotic Chromosomes Indicates a Size Dependent Contribution of Telomere Clustering and Chromosome Rigidity to Homologue Juxtaposition},
  journal = 	 {PLoS Computat Biol},
  year = 	 2012,
  OPTkey = 	 {},
  volume =	 {8},
  number =	 {5},
  pages =	 {e1002496},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1371/journal.pcbi.1002496},
  linkpdf =	 {http://www.ploscompbiol.org/article/fetchObjectAttachment.action;jsessionid=AC52AAF9AB8D2E1F9CF3399F6F8C2E3A?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002496&representation=PDF},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Meiosis is the cell division that halves the genetic component of diploid cells to form gametes or spores. To achieve this, meiotic cells undergo a radical spatial reorganisation of chromosomes. This reorganisation is a prerequisite for the pairing of parental homologous chromosomes and the reductional division, which halves the number of chromosomes in daughter cells. Of particular note is the change from a centromere clustered layout (Rabl configuration) to a telomere clustered conformation (bouquet stage). The contribution of the bouquet structure to homologous chromosome pairing is uncertain. We have developed a new in silico model to represent the chromosomes of Saccharomyces cerevisiae in space, based on a worm-like chain model constrained by attachment to the nuclear envelope and clustering forces. We have asked how these constraints could influence chromosome layout, with particular regard to the juxtaposition of homologous chromosomes and potential nonallelic, ectopic, interactions. The data support the view that the bouquet may be sufficient to bring short chromosomes together, but the contribution to long chromosomes is less. We also find that persistence length is critical to how much influence the bouquet structure could have, both on pairing of homologues and avoiding contacts with heterologues. This work represents an important development in computer modeling of chromosomes, and suggests new explanations for why elucidating the functional significance of the bouquet by genetics has been so difficult.},
  OPTgroup = 	 {}
}

@Article{Lawrence:unifying12,
  author = 	 {Neil D. Lawrence},
  title = 	 {A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models},
  journal = 	 jmlr,
  year = 	 2012,
  OPTkey = 	 {},
  volume =	 {13},
  OPTnumber = 	 {},
  OPTpages = 	 {1609--1638},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  url =	 {http://jmlr.csail.mit.edu/papers/v13/lawrence12a.html},
  OPTdoi = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linkpdf =	 shefftp # {spectral.pdf},
  linksoftware = sheffieldgit # {meu},
  abstract =	 {We introduce a new perspective on spectral
                 dimensionality reduction which views these methods as
                 Gaussian Markov random fields (GRFs). Our unifying
                 perspective is based on the maximum entropy principle
                 which is in turn inspired by maximum variance
                 unfolding. The resulting model, which we call maximum
                 entropy unfolding (MEU) is a nonlinear generalization
                 of principal component analysis. We relate the model
                 to Laplacian eigenmaps and isomap. We show that
                 parameter fitting in the locally linear embedding
                 (LLE) is approximate maximum likelihood MEU. We
                 introduce a variant of LLE that performs maximum
                 likelihood exactly: Acyclic LLE (ALLE).  We show that
                 MEU and ALLE are competitive with the leading
                 spectral approaches on a robot navigation
                 visualization and a human motion capture data
                 set. Finally the maximum likelihood perspective
                 allows us to introduce a new approach to
                 dimensionality reduction based on L1 regularization
                 of the Gaussian random field via the graphical
                 lasso.},
  OPTgroup = 	 {}
}

@TechReport{Lawrence:unifying10,
  author = 	 {Neil D. Lawrence},
  title = 	 {A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction},
  institution =  {University of Sheffield},
  year = 	 2010,
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  linkpdf =	 {http://arxiv.org/pdf/1010.4830v1},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = sheffieldgit # {meu},
  OPTlinksoftware = {},
  abstract =	 {We introduce a new perspective on spectral dimensionality
                 reduction which views these methods as Gaussian
                 random fields (GRFs). Our unifying perspective is
                 based on the maximum entropy principle which is in
                 turn inspired by maximum variance unfolding. The
                 resulting probabilistic models are based on GRFs. The
                 resulting model is a nonlinear generalization of
                 principal component analysis. We show that parameter
                 fitting in the locally linear embedding is
                 approximate maximum likelihood in these models. We
                 develop new algorithms that directly maximize the
                 likelihood and show that these new algorithms are
                 competitive with the leading spectral approaches on a
                 robot navigation visualization and a human motion
                 capture data set. Finally the maximum likelihood
                 perspective allows us to introduce a new approach to
                 dimensionality reduction based on L1 regularization
                 of the Gaussian random field via the graphical
                 lasso.},
  OPTgroup = 	 {},
  archivePrefix ={arXiv},
  eprint =	 {1010.4830},
  primaryClass = {cs-AI},
}


@TechReport{Alvarez:vikTech09,
  author =	 {Mauricio A. \'Alvarez and David Luengo and Michalis
                  K. Titsias and Neil D. Lawrence},
  title =	 {Variational Inducing Kernels for Sparse Convolved
                  Multiple Output {G}aussian Processes},
  institution =	 {University of Manchester},
  year =	 {2009},
  OPTkey =	 {},
  OPTtype =	 {},
  OPTnumber =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 {http://arxiv.org/pdf/0912.3268v1},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  archivePrefix ={arXiv},
  eprint =	 {0912.3268},
  primaryClass = {stat-ml},
  abstract =	 {Interest in multioutput kernel methods is
                  increasing, whether under the guise of multitask
                  learning, multisensor networks or structured output
                  data. From the Gaussian process perspective a
                  multioutput Mercer kernel is a covariance function
                  over correlated output functions. One way of
                  constructing such kernels is based on convolution
                  processes (CP). A key problem for this approach is
                  efficient inference. \'Alvarez and Lawrence (2009)
                  recently presented a sparse approximation for CPs
                  that enabled efficient inference. In this paper, we
                  extend this work in two directions: we introduce the
                  concept of variational inducing functions to handle
                  potential non-smooth functions involved in the
                  kernel CP construction and we consider an
                  alternative approach to approximate inference based
                  on variational methods, extending the work by
                  Titsias (2009) to the multiple output case. We
                  demonstrate our approaches on prediction of school
                  marks, compiler performance and financial time
                  series.},
  OPTgroup =	 {}
}

@TechReport{Alvarez:multiTech09,
  author =	 {Mauricio A. \'Alvarez and Neil D. Lawrence},
  title =	 {Sparse Convolved Multiple Output {G}aussian
                  Processes},
  journal =	 {},
  year =	 {2009},
  OPTkey =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTpages =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 {http://arxiv.org/pdf/0911.5107v1},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  archivePrefix ={arXiv},
  eprint =	 {0911.5107v1},
  primaryClass = {stat-ml},
  abstract =	 {Recently there has been an increasing interest in
                  methods that deal with multiple outputs. This has
                  been motivated partly by frameworks like multitask
                  learning, multisensor networks or structured output
                  data. From a Gaussian processes perspective, the
                  problem reduces to specifying an appropriate
                  covariance function that, whilst being positive
                  semi-definite, captures the dependencies between all
                  the data points and across all the outputs. One
                  approach to account for non-trivial correlations
                  between outputs employs convolution processes. Under
                  a latent function interpretation of the convolution
                  transform we establish dependencies between output
                  variables. The main drawbacks of this approach are
                  the associated computational and storage demands. In
                  this paper we address these issues. We present
                  different sparse approximations for dependent output
                  Gaussian processes constructed through the
                  convolution formalism. We exploit the conditional
                  independencies present naturally in the model. This
                  leads to a form of the covariance similar in spirit
                  to the so called PITC and FITC approximations for a
                  single output. We show experimental results with
                  synthetic and real data, in particular, we show
                  results in pollution prediction, school exams score
                  prediction and gene expression data.},
  OPTgroup =	 {}
}

@Article{Pearson:puma09,
  author =	 {Richard D. Pearson and Xuejun Liu and Guido
                  Sanguinetti and Marta Milo and Neil D. Lawrence and
                  Magnus Rattray},
  title =	 {puma: a {B}ioconductor package for Propagating
                  Uncertainty in Microarray Analysis},
  journal =	 {BMC Bioinformatics},
  year =	 2009,
  OPTkey =	 {},
  volume =	 10,
  number =	 211,
  OPTpages =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  pmid =	 {19589155},
  doi =		 {10.1186/1471-2105-10-211},
  OPTlinkpdf =	 {},
  abstract =	 {{\bf Background}\\\\ Most analyses of microarray
                  data are based on point estimates of expression
                  levels and ignore the uncertainty of such
                  estimates. By determining uncertainties from
                  Affymetrix GeneChip data and propagating these
                  uncertainties to downstream analyses it has been
                  shown that we can improve results of differential
                  expression detection, principal component analysis
                  and clustering. Previously, implementations of these
                  uncertainty propagation methods have only been
                  available as separate packages, written in different
                  languages. Previous implementations have also
                  suffered from being very costly to compute, and in
                  the case of differential expression detection, have
                  been limited in the experimental designs to which
                  they can be applied.\\\\ {\bf Results}\\\\ puma is a
                  Bioconductor package incorporating a suite of
                  analysis methods for use on Affymetrix GeneChip
                  data. puma extends the differential expression
                  detection methods of previous work from the 2-class
                  case to the multi-factorial case. puma can be used
                  to automatically create design and contrast matrices
                  for typical experimental designs, which can be used
                  both within the package itself but also in other
                  Bioconductor packages. The implementation of
                  differential expression detection methods has been
                  parallelised leading to significant decreases in
                  processing time on a range of computer
                  architectures. puma incorporates the first R
                  implementation of an uncertainty propagation version
                  of principal component analysis, and an
                  implementation of a clustering method based on
                  uncertainty propagation. All of these techniques are
                  brought together in a single, easy-to-use package
                  with clear, task-based documentation.\\\\ {\bf
                  Conclusions}\\\\ For the first time, the puma
                  package makes a suite of uncertainty propagation
                  methods available to a general audience. These
                  methods can be used to improve results from more
                  traditional analyses of microarray data. puma also
                  offers improvements in terms of scope and speed of
                  execution over previously available methods. puma is
                  recommended for anyone working with the Affymetrix
                  GeneChip platform for gene expression analysis and
                  can also be applied more generally.},
  group =	 {puma}
}

@InCollection{Lawrence:licsbintro10,
  author =	 {Neil D. Lawrence},
  title =	 {Introduction to Learning and Inference in
                  Computational Systems Biology},
  chapter =	 1,
  year =	 2010,
  crossref =	 {Lawrence:licsb10},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTtype =	 {},
  OPTaddress =	 {},
  OPTedition =	 {},
  OPTmonth =	 {},
  OPTpages =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  OPTabstract =	 {},
  OPTgroup =	 {}
}

@InCollection{Lawrence:licsbbayes10,
  author =	 {Neil D. Lawrence and Magnus Rattray},
  title =	 {A Brief Introduction to {B}ayesian Inference},
  chapter =	 5,
  year =	 2010,
  crossref =	 {Lawrence:licsb10},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTtype =	 {},
  OPTaddress =	 {},
  OPTedition =	 {},
  OPTmonth =	 {},
  OPTpages =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  OPTabstract =	 {},
  OPTgroup =	 {}
}

@InCollection{Lawrence:licsbgp10,
  author =	 {Neil D. Lawrence and Magnus Rattray and Pei Gao and
                  Michalis K. Titsias},
  title =	 {Gaussian Processes for Missing Species in
                  Biochemical Systems},
  chapter =	 9,
  year =	 2010,
  crossref =	 {Lawrence:licsb10},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTtype =	 {},
  OPTaddress =	 {},
  OPTedition =	 {},
  OPTmonth =	 {},
  OPTpages =	 {},
  OPTnote =	 {},
  OPTannote =	 {},
  pmid =	 {18689843},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  OPTabstract =	 {},
  OPTgroup =	 {}
}

@InProceedings{Alvarez:lfm09,
  author =	 {Mauricio A. \'Alvarez and David Luengo and Neil D. Lawrence},
  title =	 {Latent Force Models},
  crossref =	 {Welling:aistats09},
  pages =	 {9--16},
  year =	 2009,
  OPTeditor =	 {},
  OPTvolume =	 5,
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =      {http://jmlr.csail.mit.edu/proceedings/papers/v5/alvarez09a/alvarez09a.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {multigp},
  abstract =	 {Purely data driven approaches for machine learning
                  present difficulties when data is scarce relative to
                  the complexity of the model or when the model is
                  forced to extrapolate. On the other hand, purely
                  mechanistic approaches need to identify and specify
                  all the interactions in the problem at hand (which
                  may not be feasible) and still leave the issue of
                  how to parameterize the system. In this paper, we
                  present a hybrid approach using Gaussian processes
                  and differential equations to combine data driven
                  modeling with a physical model of the system. We
                  show how different, physically-inspired, kernel
                  functions can be developed through sensible, simple,
                  mechanistic assumptions about the underlying
                  system. The versatility of our approach is
                  illustrated with three case studies from
                  computational biology, motion capture and
                  geostatistics.},
  OPTgroup =	 {}
}

@InProceedings{Lawrence:nlmf09,
  author =	 {Neil D. Lawrence and Raquel Urtasun},
  title =	 {Non-Linear Matrix Factorization with {G}aussian
                  Processes},
  crossref =	 {Bottou:icml09},
  OPTkey =	 {},
  OPTbooktitle = {},
  OPTpages =	 {},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {collab.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {collab/},
  abstract =	 {A popular approach to collaborative filtering is
                  matrix factorization. In this paper we develop a
                  non-linear probabilistic matrix factorization using
                  Gaussian process latent variable models. We use
                  stochastic gradient descent (SGD) to optimize the
                  model. SGD allows us to apply Gaussian processes to
                  data sets with millions of observations without
                  approximate methods. We apply our approach to
                  benchmark movie recommender data sets. The results
                  show better than previous state-of-the-art
                  performance.},
  group =	 {gplvm, collaborative filtering}
}


@InProceedings{Ek:ambiguity08,
  author =	 {Carl Henrik Ek and Jon Rihan and Philip H. S. Torr and
                  Gregory Rogez and Neil D. Lawrence},
  title =	 {Ambiguity Modeling in Latent Spaces},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Machine Learning for Multimodal Interaction (MLMI
                  2008)},
  pages =	 {62--73},
  year =	 2008,
  editor =	 {Andrei {Popescu-Belis} and Rainer Stiefelhagen},
  OPTvolume =	 {},
  OPTnumber =	 {},
  series =	 {LNCS},
  OPTaddress =	 {},
  month =	 {28--30 June},
  OPTorganization ={},
  publisher =	 springer,
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {mlmi2008.pdf},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {SGPLVM/},
  abstract =	 {We are interested in the situation where we have two
                  or more representations of an underlying
                  phenomenon. In particular we are interested in the
                  scenario where the representation are
                  complementary. This implies that a single individual
                  representation is not sufficient to fully
                  discriminate a specific instance of the underlying
                  phenomenon, it also means that each representation
                  is an ambiguous representation of the other
                  complementary spaces. In this paper we present a
                  latent variable model capable of consolidating
                  multiple complementary representations. Our method
                  extends canonical correlation analysis by
                  introducing additional latent spaces that are
                  specific to the different representations, thereby
                  explaining the full variance of the
                  observations. These additional spaces, explaining
                  representation specific variance, separately model
                  the variance in a representation ambiguous to the
                  other. We develop a spectral algorithm for fast
                  computation of the embeddings and a probabilistic
                  model (based on Gaussian processes) for validation
                  and inference. The proposed model has several
                  potential application areas, we demonstrate its use
                  for multi-modal regression on a benchmark human pose
                  estimation data set.},
  OPTgroup =	 {}
}

@InProceedings{Urtasun:topology08,
  author =	 {Raquel Urtasun and David J. Fleet and Andreas Geiger
                  and Jovan Popovi\'c and Trevor J. Darrell and Neil D. Lawrence},
  title =	 {Topologically-Constrained Latent Variable Models},
  crossref =	 {Roweis:icml08},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {1080-1087},
  year =	 2008,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  doi =		 {10.1145/1390156.1390292},
  linkpdf =	 shefftp # {topology.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  OPTlinksoftware ={},
  abstract =	 {In dimensionality reduction approaches, the data are
                  typically embedded in a Euclidean latent
                  space. However for some data sets this is
                  inappropriate. For example, in human motion data we
                  expect latent spaces that are cylindrical or a
                  toroidal, that are poorly captured with a Euclidean
                  space. In this paper, we present a range of
                  approaches for embedding data in a non-Euclidean
                  latent space. Our focus is the Gaussian Process
                  latent variable model. In the context of human
                  motion modeling this allows us to (a) learn models
                  with interpretable latent directions enabling, for
                  example, style/content separation, and (b)
                  generalise beyond the data set enabling us to learn
                  transitions between motion styles even though such
                  transitions are not present in the data.},
  group =	 {}
}

@InProceedings{Ek:pose07,
  author =	 {Carl Henrik Ek and Philip H. S. Torr and Neil
                  D. Lawrence},
  title =	 {{G}aussian Process Latent Variable Models For Human
                  Pose Estimation},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Machine Learning for Multimodal Interaction (MLMI
                  2007)},
  pages =	 {132--143},
  year =	 2008,
  editor =	 {Andrei {Popescu-Belis} and Steve Renals and Herv\'e
                  Bourlard},
  volume =	 4892,
  OPTnumber =	 {},
  series =	 {LNCS},
  address =	 {Brno, Czech Republic},
  OPTmonth =	 {},
  OPTorganization ={},
  publisher =	 springer,
  OPTannote =	 {},
  OPTpmid =	 {},
  doi =		 {10.1007/978-3-540-78155-4_12},
  linkpdf =	 shefftp # {mlmi.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = sheffieldgit # {SGPLVM/},
  abstract =	 {We describe a method for recovering 3D human body
                  pose from silhouettes. Our model is based on
                  learning a latent space using the Gaussian Process
                  Latent Variable Model (GP-LVM) [1] encapsulating
                  both pose and silhouette features Our method is
                  generative, this allows us to model the ambiguities
                  of a silhouette representation in a principled
                  way. We learn a dynamical model over the latent
                  space which allows us to disambiguate between
                  ambiguous silhouettes by temporal consistency. The
                  model has only two free parameters and has several
                  advantages over both regression approaches and other
                  generative methods. In addition to the application
                  shown in this paper the suggested model is easily
                  extended to multiple observation spaces without
                  constraints on type.},
  group =	 {gplvm,pose estimation}
}

@InProceedings{Eciolaza:fault07,
  author =	 {Luka Eciolaza and M. Alkarouri and Neil D. Lawrence
                  and Visakan Kadirkamanathan and Peter J. Fleming},
  title =	 {{G}aussian Process Latent Variable Models for Fault
                  Detection},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Computational Intelligence and Data Mining},
  pages =	 {287--292},
  year =	 2007,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  doi =		 {10.1109/CIDM.2007.368886},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTlinkpdf =	 {},
  OPTlinksoftware ={},
  abstract =	 {The Gaussian process latent variable model (GPLVM)
                  is a novel unsupervised approach to nonlinear low
                  dimensional embedding proposed by Lawrence
                  (2005). This paper presents the development of a
                  framework for the implementation of the GPLVM for
                  fault detection. A series of experiments have been
                  carried out comparing and combining the GPLVM to the
                  conventional and widely used linear dimension
                  reduction technique of principal component analysis
                  (PCA). The inclusion of the GPLVM for the
                  visualisation and data analysis, led to a
                  considerable improvement in the classification
                  results},
  group =	 {gplvm}
}

@InProceedings{Lawrence:hgplvm07,
  author =	 {Neil D. Lawrence and Andrew J. Moore},
  title =	 {Hierarchical {G}aussian Process Latent Variable
                  Models},
  crossref =	 {Ghahramani:icml07},
  pages =	 {481--488},
  abstract =	 {The Gaussian process latent variable model (GP-LVM)
                  is a powerful approach for probabilistic modelling
                  of high dimensional data through dimensional
                  reduction. In this paper we extend the GP-LVM
                  through hierarchies. A hierarchical model (such as a
                  tree) allows us to express conditional
                  independencies in the data as well as the manifold
                  structure. We first introduce Gaussian process
                  hierarchies through a simple dynamical model, we
                  then extend the approach to a more complex hierarchy
                  which is applied to the visualisation of human
                  motion data sets.},
  year =	 2007,
  linkpdf =	 shefftp # {hgplvm.pdf},
  linksoftware = sheffieldgit # {hgplvm/},
  group =	 {manml,gp,gplvm,dimensional reduction}
}

@InProceedings{Laidler:model07,
  author =	 {Jonathan Laidler and Martin Cooke and Neil
                  D. Lawrence},
  title =	 {Model-driven detection of Clean Speech Patches in
                  Noise},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Proceedings of Interspeech 2007},
  OPTpages =	 {},
  year =	 2007,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {Antwerp, Belgium},
  month =	 {August 27th-31st},
  OPTorganization ={},
  OPTpublisher = {},
  note =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =	 shefftp # {LaidlerInterspeech2007.pdf},
  OPTlinksoftware ={},
  abstract =	 {Listeners may be able to recognise speech in adverse
                  conditions by glimpsing time-frequency regions where
                  the target speech is dominant. Previous
                  computational attempts to identify such regions have
                  been source-driven, using primitive cues. This paper
                  describes a model-driven approach in which the
                  likelihood of spectro-temporal patches of a noisy
                  mixture representing speech is given by a generative
                  model. The focus is on patch size and patch
                  modelling. Small patches lead to a lack of
                  discrimination, while large patches are more likely
                  to contain contributions from other sources. A
                  cleanness measure reveals that a good patch size is
                  one which extends over a quarter of the speech
                  frequency range and lasts for 40 ms. Gaussian
                  mixture models are used to represent patches. A
                  compact representation based on a 2D discrete cosine
                  transform leads to reasonable speech/background
                  discrimination.},
  group =	 {speech separation, glimpsing, model-driven,
                  spectro-temporal patches}
}

@InProceedings{Titsias:efficient08,
  author =	 {Michalis K. Titsias and Neil D. Lawrence and Magnus
                  Rattray},
  title =	 {Efficient Sampling for {G}aussian Process Inference
                  using Control Variables},
  crossref =	 {Koller:nips08},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {1681--1688},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {nipsSamGP08.pdf},
  OPTlinksoftware ={},
  abstract =	 {Sampling functions in Gaussian process (GP) models
                  is challenging because of the highly correlated
                  posterior distribution. We describe an efficient
                  Markov chain Monte Carlo algorithm for sampling from
                  the posterior process of the GP model. This
                  algorithm uses control variables which are auxiliary
                  function values that provide a low dimensional
                  representation of the function. At each iteration,
                  the algorithm proposes new values for the control
                  variables and generates the function from the
                  conditional GP prior. The control variable input
                  locations are found by continuously minimizing an
                  objective function. We demonstrate the algorithm on
                  regression and classification problems and we use it
                  to estimate the parameters of a differential
                  equation model of gene regulation.},
  OPTgroup =	 {}
}

@InProceedings{Alvarez:convolved08,
  author =	 {Mauricio A. \'Alvarez and Neil D. Lawrence},
  title =	 {Sparse Convolved {G}aussian Processes for
                  Multi-output Regression},
  crossref =	 {Koller:nips08},
  linkpdf =	 shefftp # {spmulti.pdf},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {57--64},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  linksoftware = sheffieldgit # {multigp/},
  abstract =	 {We present a sparse approximation approach for
                  dependent output Gaussian processes (GP). Employing
                  a latent function framework, we apply the
                  convolution process formalism to establish
                  dependencies between output variables, where each
                  latent function is represented as a GP. Based on
                  these latent functions, we establish an
                  approximation scheme using a conditional
                  independence assumption between the output
                  processes, leading to an approximation of the full
                  covariance which is determined by the locations at
                  which the latent functions are evaluated. We show
                  results of the proposed methodology for synthetic
                  data and real world applications on pollution
                  prediction and a sensor network.},
  OPTgroup =	 {}
}

@InProceedings{Calderhead:accelerating08,
  author =	 {Ben Calderhead and Mark Girolami and Neil D. Lawrence},
  title =	 {Accelerating {B}ayesian Inference over Nonlinear
                  Differential Equations with {G}aussian Processes},
  crossref =	 {Koller:nips08},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {217--224},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  OPTlinkpdf =	 {},
  OPTlinksoftware ={},
  abstract =	 {Identification and comparison of nonlinear dynamical
                  system models using noisy and sparse experimental
                  data is a vital task in many fields, however current
                  methods are computationally expensive and prone to
                  error due in part to the nonlinear nature of the
                  likelihood surfaces induced. We present an
                  accelerated sampling procedure which enables
                  Bayesian inference of parameters in nonlinear
                  ordinary and delay differential equations via the
                  novel use of Gaussian processes (GP). Our methdo
                  involves GP regression over time-series data, and
                  the resulting derivative and time delay estimates
                  make parameter inference possible \emph{without}
                  solving the dynamical system explicitly, resulting
                  in dramatic savings of computational time. We
                  demonstrate the speed and statistical accuracy of
                  our approach using examples of both ordinary and
                  elay differential equations, and provide a
                  comprehensive comparison with current state of the
                  art methods.},
  group =	 {GP, differential equations, systems biology}
}

@Article{Gao:latent08,
  author =	 {Pei Gao and Antti Honkela and Magnus Rattray and
                  Neil D. Lawrence},
  title =	 {{G}aussian Process Modelling of Latent Chemical
                  Species: Applications to Inferring Transcription
                  Factor Activities},
  journal =	 bioinf,
  year =	 2008,
  OPTkey =	 {},
  volume =	 24,
  OPTnumber =	 {},
  pages =	 {i70--i75},
  OPTmonth =	 {},
  OPTnote =	 {To appear at ECCB '08},
  OPTannote =	 {},
  OPTpmid =	 {},
  doi =		 {10.1093/bioinformatics/btn278},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/24/16/i70.pdf?ijkey=FauSn114lAUC1Ey&keytype=ref},
  linksoftware = softwarehttp # {gpsim/},
  abstract =	 {{\bf Motivation:} Inference of \emph{latent chemical
                  species} in biochemical interaction networks is a
                  key problem in estimation of the structure and
                  parameters of the genetic, metabolic and protein
                  interaction networks that underpin all biological
                  processes. We present a framework for Bayesian
                  marginalisation of these latent chemical species
                  through Gaussian process priors.\\\\ {\bf Results:}
                  We demonstrate our general approach on three
                  different biological examples of single input
                  motifs, including both activation and repression of
                  transcription. We focus in particular on the problem
                  of inferring transcription factor activity when the
                  concentration of active protein cannot easily be
                  measured. We show how the uncertainty in the
                  inferred transcription factor activity can be
                  integrated out in order to derive a likelihood
                  function that can be used for the estimation of
                  regulatory model parameters. An advantage of our
                  approach is that we avoid the use of a
                  coarse-grained discretization of continuous-time
                  functions, which would lead to a large number of
                  additional parameters to be estimated. We develop
                  efficient exact and approximate inference schemes,
                  which are much more efficient than competing
                  sampling-based schemes and therefore provide us with
                  a practical toolkit for model-based inference.\\\\
                  {\bf Availability:} The software and data for
                  recreating all the experiments in this paper is
                  available in MATLAB from
                  \url{http://inverseprobability.com/gpsim}\\\\
                  {\bf Contact:} Neil Lawrence},
  group =	 {gene networks, TFA, gp}
}

@Misc{Lawrence:nipsw07,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Optimisation by Marginal Matching},
  howpublished = {Poster presentation at NIPS Workshop on Approximate Bayesian Inference, see spotlight at \url{http://videolectures.net/abi07_lawrence_vap/}},
  videolectures = {abi07_lawrence_vap},
  OPTmonth = 	 {},
  year =	 {2007},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@InProceedings{Ferris:wifi07,
  author =	 {Brian D. Ferris and Dieter Fox and Neil D. Lawrence},
  title =	 {{WiFi-SLAM} Using {G}aussian Process Latent Variable
                  Models},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {Proceedings of the 20th International Joint
                  Conference on Artificial Intelligence (IJCAI 2007)},
  pages =	 {2480--2485},
  year =	 2007,
  editor =	 {Manuela M. Veloso},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =
                  {http://www.ijcai.org/papers07/Papers/IJCAI07-399.pdf},
  OPTlinksoftware ={},
  abstract =	 {WiFi localization, the task of determining the
                  physical location of a mobile device from wireless
                  signal strengths, has been shown to be an accurate
                  method of indoor and outdoor localization and a
                  powerful building block for location-aware
                  applications. However, most localization techniques
                  require a training set of signal strength readings
                  labeled against a ground truth location map, which
                  is prohibitive to collect and maintain as maps grow
                  large. In this paper we propose a novel technique
                  for solving the WiFi SLAM problem using the Gaussian
                  Process Latent Variable Model (GP-LVM) to determine
                  the latent-space locations of unlabeled signal
                  strength data. We show how GP-LVM, in combination
                  with an appropriate motion dynamics model, can be
                  used to reconstruct a topological connectivity graph
                  from a signal strength sequence which, in
                  combination with the learned Gaussian Process signal
                  strength model, can be used to perform efficient
                  localization.},
  group =	 {gplvm,dimensional reduction}
}

@Article{Sanguinetti:chipvar06,
  author =	 {Guido Sanguinetti and Neil D. Lawrence and Magnus
                  Rattray},
  title =	 {Probabilistic inference of transcription factor
                  concentrations and gene-specific regulatory
                  activities},
  year =	 2006,
  journal =	 bioinf,
  linksoftware = softwarehttp # {chipvar/},
  volume =	 22,
  number =	 22,
  doi =		 {10.1093/bioinformatics/btl473},
  pmid =	 16966362,
  pages =	 {2275--2281},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/btl473v1},
  abstract =	 {{\bf Motivation}: Quantitative estimation of the
                  regulatory relationship between transcription
                  factors and genes is a fundamental stepping stone
                  when trying to develop models of cellular
                  processes. Recent experimental high-throughput
                  techniques such as Chromatine Immunoprecipitation
                  provide important information about the architecture
                  of the regulatory networks in the cell. However, it
                  is very difficult to measure the concentration
                  levels of transcription factor proteins and
                  determine their regulatory effect on gene
                  transcription. It is therefore an important
                  computational challenge to infer these quantities
                  using gene expression data and network architecture
                  data.\\\\ {\bf Results}: We develop a probabilistic
                  state space model that allows genome-wide inference
                  of both transcription factor protein concentrations
                  and their effect on the transcription rates of each
                  target gene from microarray data. We use variational
                  inference techniques to learn the model parameters
                  and perform posterior inference of protein
                  concentrations and regulatory strengths. The
                  probabilistic nature of the model also means that we
                  can associate credibility intervals to our
                  estimates, as well as providing a tool to detect
                  which binding events lead to significant
                  regulation. We demonstrate our model on artificial
                  data and on two yeast data sets in which the network
                  structure has previously been obtained using
                  Chromatine Immunoprecipitation data. Predictions
                  from our model are consistent with the underlying
                  biology and offer novel quantitative insights into
                  the regulatory structure of the yeast cell.\\\\ {\bf
                  Availability}: MATLAB code is available from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.},
  errata1 =	 {Equation (5) the normal density for $y_n(t)$ should
                  have existing term $+ \mu_n(t)$ for the mean.},
  errataCredit1 = {Kevin Sharp},
  errata2 =	  {Equation (7), the two terms that involve
                  $\mathbf{K}$ should be outside the sum over
                  $n$. There should be no $T$ in the term
                  $Tq\log(\alpha^2)$.},
  errataCredit2 = {Junfeng Chen},
  errata3 =	 {Equation (8), $\alpha^2$ should be $\alpha^{-2}$.},
  errataCredit3 = {Junfeng Chen},
  errata4 =	 {Equation (10), second line, there should be $T$
                  before the $\sigma^{-2}$.},
  errataCredit4 = {Junfeng Chen},
  errata5 =	 {Equation (11), sums should be normalised, first one
                  by $Nq$, second one by $NT$.},
  errataCredit5 = {Junfeng Chen},
  group =	 {shefml,puma,gene networks}
}

@TechReport{Sanguinetti:integrate06,
  author =	 {Guido Sanguinetti and Magnus Rattray and Neil D. Lawrence},
  title =	 {A Probabilistic Model to Integrate Chip and
                  Microarray Data},
  institution =	 {Department of Computer Science, University of
                  Sheffield},
  year =	 2006,
  OPTkey =	 {},
  OPTtype =	 {},
  number =	 {CS-06-02},
  linkpdf =	 shefftp # {chipTech.pdf},
  linksoftware = sheffieldgit # {chipdyno/},
  address =	 {},
  OPTmonth =	 {},
  OPTnote =	 {},
  group =	 {shefml,puma,gene networks}
}

@Article{Rattray:propagating06,
  author =	 {Magnus Rattray and Xuejun Liu and Guido Sanguinetti
                  and Marta Milo and Neil D. Lawrence},
  title =	 {Propagating Uncertainty in Microarray Data Analysis},
  journal =	 {Briefings in Bioinformatics},
  year =	 2006,
  volume =	 7,
  number =	 1,
  pages =	 {37--47},
  pmid =	 16761363,
  errata1 =	 {The error bars in Figure 3 are incorrect, as
                  explained in the errata to Liu et al. Bioinformatics
                  22, 2107-2113 \cite{Liu:variances06}.},
  errataCredit1 ={Richard Pearson},
  linkpdf =	 {http://bib.oxfordjournals.org/cgi/reprint/7/1/37},
  abstract =	 {Microarray technology is associated with many
                  sources of experimental uncertainty. In this review
                  we discuss a number of approaches for dealing with
                  this uncertainty in the processing of data from
                  microarray experiments. We focus here on the
                  analysis of high-density oligonucleotide arrays,
                  such as the popular Affymetrix GeneChipÂ® array,
                  which contain multiple probes for each target. This
                  set of probes can be used to determine an estimate
                  for the target concentration and can also be used to
                  determine the experimental uncertainty associated
                  with this measurement. This measurement uncertainty
                  can then be propagated through the downstream
                  analysis using probabilistic methods. We give
                  examples showing how these credibility intervals can
                  be used to help identify differential expression, to
                  combine information from replicated experiments and
                  to improve the performance of principal component
                  analysis.},
  group =	 {puma,shefml}
}

@Article{Liu:variances06,
  author =	 {Xuejun Liu and Marta Milo and Neil D. Lawrence and
                  Magnus Rattray},
  title =	 {Probe-level Measurement Error Improves Accuracy in
                  Detecting Differential Gene Expression},
  journal =	 bioinf,
  year =	 2006,
  doi =		 {10.1093/bioinformatics/btl361},
  volume =	 22,
  number =	 17,
  pages =	 {2107--2113},
  pmid =	 16820429,
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/btl361v1.pdf},
  OPTlabel1 =	 {Advance Access},
  OPTlink1 =
                  {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/btl361v1},
  group =	 {shefml,puma,gene networks},
  errata1 =	 {The error bars in Figure 2 were incorrectly
                  calculated. The posterior variance was used instead
                  of the posterior standard deviation to compute the
                  credibility intervals. The ranking was correct in
                  both plots. Here is the revised figure:
                  \url{http://www.bioinf.manchester.ac.uk/resources/puma/intervals.pdf}. There
                  are now 7 false positives in to top 50 genes ranked
                  by differential expression in the left-hand
                  plot. None of the main quantitative results in the
                  paper (i.e. PPLR values, ROC plots, AUC scores) are
                  affected. The main point is that ranking by
                  differential expression alone leads to many false
                  positives while using the PPLR criterion will
                  greatly reduce the number of false positives. This
                  conclusion remains valid.},
  errataCredit1 ={Richard Pearson},
  abstract =	 {{\bf Motivation:} Finding differentially expressed
                  genes is a fundamental objective of a microarray
                  experiment. Numerous methods have been proposed to
                  perform this task. Existing methods are based on
                  point estimates of gene expression level obtained
                  from each microarray experiment. This approach
                  discards potentially useful information about
                  measurement error that can be obtained from an
                  appropriate probe-level analysis. Probabilistic
                  probe-level models can be used to measure gene
                  expression and also provide a level of uncertainty
                  in this measurement. This probe-level variance
                  provides useful information which can help in the
                  identification of differentially expressed
                  genes.\\\\ {\bf Results:} We propose a Bayesian
                  method to include probe-level variances into the
                  detection of differentially expressed genes from
                  replicated experiments. A variational approximation
                  is used for effcient parameter estimation. We
                  compare this approximation with MAP and MCMC
                  parameter estimation in terms of computational
                  effciency and accuracy. The method is used to
                  calculate the probability of positive log-ratio
                  (PPLR) of expression levels between
                  conditions. Using the measurements from a recently
                  developed Affymetrix probe-level model, multi-mgMOS,
                  we test PPLR on a spike-in data set and a mouse
                  time-course data set. Results show that the
                  inclusion of probelevel measurement error improves
                  accuracy in detecting differential gene
                  expression.\\\\ {\bf Availability:} The methods
                  described in this paper have been implemented in an
                  R package \emph{pplr} that is currently available
                  from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.\\\\
                  {\bf Contact:} Magnus Rattray}
}

@InProceedings{Sanguinetti:trento06,
  author =	 {Guido Sanguinetti and Magnus Rattray and Neil D. Lawrence},
  title =	 {Identifying submodules of cellular regulatory
                  networks},
  OPTcrossref =	 {},
  OPTkey =	 {},
  booktitle =	 {International Conference on Computational Methods in
                  Systems Biology},
  OPTpages =	 {155--168},
  year =	 {2006},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  series =	 {LNCS},
  doi =		 {10.1007/11885191_11},
  OPTmonth =	 {},
  OPTorganization ={},
  publisher =	 springer,
  abstract =	 {Recent high throughput techniques in molecular
                  biology have brought about the possibility of
                  directly identifying the architecture of regulatory
                  networks on a genome-wide scale. However, the
                  computational task of estimating fine-grained models
                  on a genome-wide scale is daunting. Therefore, it is
                  of great importance to be able to reliably identify
                  submodules of the network that can be effectively
                  modelled as independent subunits. In this paper we
                  present a procedure to obtain submodules of a
                  cellular network by using information from
                  gene-expression measurements. We integrate network
                  architecture data with genome-wide gene expression
                  measurements in order to determine which regulatory
                  relations are actually confirmed by the expression
                  data. We then use this information to obtain
                  non-trivial submodules of the regulatory network
                  using two distinct algorithms, a naive exhaustive
                  algorithm and a spectral algorithm based on the
                  eigendecomposition of an affinity matrix. We test
                  our method on two yeast biological data sets, using
                  regulatory information obtained from chromatin
                  immunoprecipitation.},
  group =	 {gene networks,shefml,puma}
}

@Article{Lawrence:pnpca05,
  author =	 {Neil D. Lawrence},
  title =	 {Probabilistic Non-linear Principal Component
                  Analysis with {G}aussian Process Latent Variable
                  Models},
  journal =	 jmlr,
  year =	 2005,
  volume =	 6,
  pages =	 {1783--1816},
  month =	 11,
  label1 =	 {C++ Software},
  link1 =	 sheffieldgit # {GPc/},
  label2 =	 {MATLAB Software},
  link2 =	 sheffieldgit # {GPmat/},
  label3 =	 {JMLR PDF},
  link3 =
                  {http://www.jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf},
  label4 =	 {JMLR Abstract},
  link4 =	 {http://www.jmlr.org/papers/v6/lawrence05a.html},
  errata1 =	 {Page 1791: eqn (10) there is a factor of a half
                  missing on both terms of the right hand side.},
  errataCredit1 ={Andreas Geiger},
  errata2 =	 {Page 1789: second line, the exponent of $(\lambda_j
                  - \beta^{-1})$ should be 1/2, not -1/2.},
  errataCredit2 ={Mathieu Saltzman},
  errata3 =	 {Page 1812: end of Appendix A, instead of `... for
                  any symmetric matrix $\mathbf{S}$ ...' line should
                  read `... for any positive definite symmetric matrix
                  $\mathbf{S}$.'},
  OPTerrataCredit3 ={},
  errata4 =	 {Page 1812: after eqn (25) last line of paragraph,
                  instead of `... (for kernel PCA) in where ...' line
                  should read `... (for kernel PCA) in which ...'.},
  OPTerrataCredit4 ={},
  group =	 {shefml,gplvm,ppca,pca,dimensional reduction},
  abstract =	 {Summarising a high dimensional data set with a low
                  dimensional embedding is a standard approach for
                  exploring its structure. In this paper we provide an
                  overview of some existing techniques for discovering
                  such embeddings. We then introduce a novel
                  probabilistic interpretation of principal component
                  analysis (PCA) that we term dual probabilistic PCA
                  (DPPCA). The DPPCA model has the additional
                  advantage that the linear mappings from the embedded
                  space can easily be non-linearised through Gaussian
                  processes. We refer to this model as a Gaussian
                  process latent variable model (GP-LVM). Through
                  analysis of the GP-LVM objective function, we relate
                  the model to popular spectral techniques such as
                  kernel PCA and multidimensional scaling. We then
                  review a practical algorithm for GP-LVMs in the
                  context of large data sets and develop it to also
                  handle discrete valued data and missing
                  attributes. We demonstrate the model on a range of
                  real-world and artificially generated data sets.}
}


@TechReport{Lawrence:embodiment17,
  author = 	 {Neil D. Lawrence},
  title = 	 {Living Together: Mind and Machine Intelligence},
  institution =  {arXiv},
  year = 	 {2017},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {In this paper we consider the nature of the machine intelligences we have created in the context of our human intelligence. We suggest that the fundamental difference between human and machine intelligence comes down to \emph{embodiment factors}. We define embodiment factors as the ratio between an entity's ability to communicate information vs compute information. We speculate on the role of embodiment factors in driving our own intelligence and consciousness. We briefly review dual process models of cognition and cast machine intelligence within that framework, characterising it as a dominant System Zero, which can drive behaviour through interfacing with us subconsciously. Driven by concerns about the consequence of such a system we suggest prophylactic courses of action that could be considered. Our main conclusion is that it is \emph{not} sentient intelligence we should fear but \emph{non-sentient} intelligence.},
  url =	 {https://arxiv.org/abs/1705.07996}
}

@InProceedings{Stegle:sparse11,
  author = 	 {Oliver Stegle and Christoph Lippert and Joris Mooij and Neil D. Lawrence and Karsten Borgwardt},
  title = 	 {Efficient Inference in Matrix-Variate {G}aussian Models with i.i.d. Observation Noise},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle =	 {Neural Information Processing Systems},
  OPTpages = 	 {},
  year =	 {2011},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Article{Fusi:genomics12,
  author = 	 {Nicol\'o Fusi and Oliver Stegle and Neil D. Lawrence},
  title = 	 {Joint Modelling of Confounding Factors and Prominent Genetic Regulators Provides Increased Accuracy in Genetical Genomics Studies},
  journal = 	 {PLoS Computat Biol},
  year = 	 2012,
  OPTkey = 	 {},
  volume =	 8,
  OPTnumber = 	 1,
  pages = {e1002330},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10.1371/journal.pcbi.1002330},
  linkpdf =	 {http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002330},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  linksoftware = {http://ml.sheffield.ac.uk/qtl/},
  abstract =	 {Expression quantitative trait loci (eQTL) studies are an integral tool to investigate the genetic component of gene expression variation. A major challenge in the analysis of such studies are hidden confounding factors, such as unobserved covariates or unknown subtle environmental perturbations. These factors can induce a pronounced artifactual correlation structure in the expression profiles, which may create spurious false associations or mask real genetic association signals. Here, we report PANAMA (Probabilistic ANAlysis of genoMic dAta), a novel probabilistic model to account for confounding factors within an eQTL analysis. In contrast to previous methods, PANAMA learns hidden factors jointly with the effect of prominent genetic regulators. As a result, this new model can more accurately distinguish true genetic association signals from confounding variation. We applied our model and compared it to existing methods on different datasets and biological systems. PANAMA consistently performs better than alternative methods, and finds in particular substantially more trans regulators. Importantly, our approach not only identifies a greater number of associations, but also yields hits that are biologically more plausible and can be better reproduced between independent studies. A software implementation of PANAMA is freely available online at \url{http://ml.sheffield.ac.uk/qtl/}.},
  publisher = {Public Library of Science},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1002330}
}

@TechReport{Fusi:accurate11,
  author = 	 {Nicol\'o Fusi and Oliver Stegle and Neil D. Lawrence},
  title = 	 {Accurate modeling of confounding variation in {eQTL} studies leads to a great increase in power to detect trans-regulatory effects},
  institution =  {Nature Precedings},
  year = 	 2011,
  OPTkey = 	 {http://precedings.nature.com/documents/5995/version/1},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  doi =		 {10101/npre.2011.5995.1},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  abstract =	 {Expression quantitative trait loci (eQTL) studies are an integral tool to investigate the genetic component of gene expression variation. A major challenge in the analysis of such studies are hidden confounding factors, such as unobserved covariates or unknown environmental influences. These factors can induce a pronounced artifactual correlation structure in the expression profiles, which may create spurious false associations or mask real genetic association signals.\\\\

Here, we report PANAMA (Probabilistic ANAlysis of genoMic dAta), a novel probabilistic model to account for confounding factors within an
eQTL analysis. In contrast to previous methods, PANAMA learns hidden factors jointly with the effect of prominent genetic regulators. As a result, PANAMA can more accurately distinguish between true genetic association signals and confounding variation.\\\\

We applied our model and compared it to existing methods on a variety of datasets and biological systems. PANAMA consistently performs better than alternative methods, and finds in particular substantially more trans regulators. Importantly, PANAMA not only identified a greater number of associations, but also yields hits that are biologically more plausible and can be better reproduced between independent studies.},
  OPTgroup = 	 {}
}

@Misc{Lawrence:mocap05,
  OPTkey =	 {},
  author =	 {Neil D. Lawrence},
  title =	 {MOCAP Toolbox for {MATLAB}},
  howpublished = {Available on-line.},
  OPTmonth =	 7,
  OPTyear =	 {2005},
  linksoftware = softwarehttp # {mocap/}
}

@Article{Sanguinetti:accounting05,
  author =	 {Guido Sanguinetti and Marta Milo and Magnus Rattray
                  and Neil D. Lawrence},
  title =	 {Accounting for Probe-level Noise in Principal
                  Component Analysis of Microarray Data},
  journal =	 {Bionformatics},
  year =	 2005,
  volume =	 21,
  number =	 19,
  doi =		 {10.1093/bioinformatics/bti617},
  pmid =	 16091409,
  pages =	 {3748--3754},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/21/19/3748},
  label1 =	 {Advance Access},
  link1 =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/bti617?ijkey=2Elnob2I7AyTWIM&keytype=ref},
  label2 =	 {Pre-print PDF},
  link2 =	 shefftp # {nppca.pdf},
  linksoftware = softwarehttp # {nppca/},
  label3 =	 {Bioinformatics Abstract},
  link3 =
                  {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/21/19/3748},
  abstract =	 {{\bf Motivation:} Principal Component Analysis (PCA)
                  is one of the most popular dimensionality reduction
                  techniques for the analysis of high-dimensional
                  datasets. However, in its standard form, it does not
                  take into account any error measures associated with
                  the data points beyond a standard spherical
                  noise. This indiscriminate nature provides one of
                  its main weaknesses when applied to biological data
                  with inherently large variability, such as
                  expression levels measured with microarrays. Methods
                  now exist for extracting credibility intervals from
                  the probe-level analysis of cDNA and oligonucleotide
                  microarray experiments. These credibility intervals
                  are gene and experiment specific, and can be
                  propagated through an appropriate probabilistic
                  downstream analysis.\\\\ {\bf Results:} We propose a
                  new model-based approach to PCA that takes into
                  account the variances associated with each gene in
                  each experiment. We develop an efficient
                  EM-algorithm to estimate the parameters of our new
                  model. The model provides significantly better
                  results than standard PCA, while remaining
                  computationally reasonable. We show how the model
                  can be used to 'denoise' a microarray dataset
                  leading to improved expression profiles and tighter
                  clustering across profiles. The probabilistic nature
                  of the model means that the correct number of
                  principal components is automatically obtained.\\\\
                  {\bf Availability:} The software used in the paper
                  is available from
                  \url{http://www.bioinf.manchester.ac.uk/resources/puma}. The
                  microarray data are deposited in the NCBI database.},
  group =	 {shefml,puma,pca,ppca}
}

@InProceedings{Sanguinetti:automatic05,
  author =	 {Guido Sanguinetti and Jonathan Laidler and Neil D. Lawrence},
  title =	 {Automatic Determination of the Number of Clusters
                  Using Spectral Algorithms},
  booktitle =	 {Procedings of MLSP'05},
  year =	 2005,
  linksoftware = {http://www.dcs.shef.ac.uk/\%7Eguido/software.html},
  linkpdf =	 shefftp # {clusterNumber.pdf},
  group =	 {shefml},
  OPTnumber =	 {},
  address =	 {Mystic, Connecticut, U.S.A.},
  OPTmonth =	 {},
  OPTnote =	 {},
  OPTannote =	 {}
}

@InProceedings{Lawrence:semisuper04,
  author =	 {Neil D. Lawrence and Michael I. Jordan},
  title =	 {Semi-supervised Learning via {G}aussian Processes},
  abstract =	 {We present a probabilistic approach to learning a
                  Gaussian Process classifier in the presence of
                  unlabeled data. Our approach involves a "null
                  category noise model" (NCNM) inspired by ordered
                  categorical noise models. The noise model reflects
                  an assumption that the data density is lower between
                  the class-conditional densities. We illustrate our
                  approach on a toy problem and present comparative
                  results for the semi-supervised classification of
                  handwritten digits.},
  pages =	 {753--760},
  crossref =	 {Saul:nips04},
  linkpsgz =	 shefftp # {ncnm.ps.gz},
  linksoftware = softwarehttp # {ncnm/},
  group =	 {shefml}
}

@InProceedings{Bishop:mixtures97,
  author =	 {Christopher M. Bishop and Neil D. Lawrence and Tommi
                  S. Jaakkola and Michael I. Jordan},
  title =	 {Approximating Posterior Distributions in Belief
                  Networks using Mixtures},
  pages =	 {416--422},
  crossref =	 {Jordan:nips97},
  abstract =	 {Exact inference in densely connected Bayesian
                  networks is computationally intractable, and so
                  there is considerable interest in developing
                  effective approximation schemes. One approach which
                  has been adopted is to bound the log likelihood
                  using a mean-field approximating distribution. While
                  this leads to a tractable algorithm, the mean field
                  distribution is assumed to be factorial and hence
                  unimodal. In this paper we demonstrate the
                  feasibility of using a richer class of approximating
                  distributions based on \emph{mixtures} of mean field
                  distributions. We derive an efficient algorithm for
                  updating the mixture parameters and apply it to the
                  problem of learning in sigmoid belief networks. Our
                  results demonstrate a systematic improvement over
                  simple mean field theory as the number of mixture
                  components is increased.},
  linkpsgz =	 myftp # {mixtures.ps.gz}
}

@InProceedings{Lawrence:learning04,
  author =	 {Neil D. Lawrence and John C. Platt},
  title =	 {Learning to Learn with the Informative Vector
                  Machine},
  pages =	 {512--519},
  doi =		 {10.1145/1015330.1015382},
  crossref =	 {Greiner:icml04},
  abstract =	 {This paper describes an efficient method for
                  learning the parameters of a Gaussian process
                  (GP). The parameters are learned from multiple tasks
                  which are assumed to have been drawn independently
                  from the same GP prior. An efficient algorithm is
                  obtained by extending the informative vector machine
                  (IVM) algorithm to handle the multi-task learning
                  case. The multi-task IVM (MT-IVM) saves computation
                  by greedily selecting the most informative examples
                  from the separate tasks. The MT-IVM is also shown to
                  be more efficient than sub-sampling on an artificial
                  data-set and more effective than the traditional IVM
                  in a speaker dependent phoneme recognition task.},
  year =	 2004,
  linkpsgz =	 shefftp # {mtivm.ps.gz},
  linkpdf =	 shefftp # {mtivm.pdf},
  linksoftware = softwarehttp # {mtivm/},
  group =	 {shefml,gp,spgp}
}

@InProceedings{Sanguinetti:missingkpca06,
  author =	 {Guido Sanguinetti and Neil D. Lawrence},
  title =	 {Missing Data in Kernel {PCA}},
  crossref =	 {Scheffer:ecml06},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {751--758},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {}
}

@InProceedings{King:klcorrection06,
  author =	 {Nathaniel J. King and Neil D. Lawrence},
  title =	 {Fast Variational Inference for {G}aussian {P}rocess
                  Models through {KL}-Correction},
  crossref =	 {Scheffer:ecml06},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {270--281},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTpmid =	 {},
  OPTdoi =	 {},
  linkpdf =	 shefftp # {ECMLppa.pdf},
  OPTlinkps =	 {},
  OPTlinkpsgz =	 {},
  linksoftware = softwarehttp # {ppa/},
  errata1 =	 {Page 276: first equation on page, after 'The
                  KL-corrected bound (9) can be written using (3) as
                  ...'. The left hand side of this bound should be
                  $\mathcal{L}^{\prime}(\theta)$ instead of
                  $L(\theta)$.},
  errataCredit1 ={Raquel Urtasun},
  abstract =	 {Variational inference is a exible approach to
                  solving problems of intractability in Bayesian
                  models. Unfortunately the convergence of variational
                  methods is often slow. We review a recently
                  suggested variational approach for approximate
                  inference in Gaussian process (GP) models and show
                  how convergence may be dramatically improved through
                  the use of a positive correction term to the
                  standard variational bound. We refer to the modied
                  bound as a KL-corrected bound. The KL-corrected
                  bound is a lower bound on the true likelihood, but
                  an upper bound on the original variational
                  bound. Timing comparisons between optimisation of
                  the two bounds show that optimisation of the new
                  bound consistently improves the speed of
                  convergence.},
  group =	 {gp,variational,shefml}
}

@InProceedings{Lawrence:backconstraints06,
  author =	 {Neil D. Lawrence and Joaquin {Qui\~nonero Candela}},
  title =	 {Local Distance Preservation in the {GP-LVM} through
                  Back Constraints},
  pages =	 {513--520},
  crossref =	 {Cohen:icml06},
  doi =		 {10.1145/1143844.1143909},
  year =	 2006,
  linkpdf =	 shefftp # {backConstraints.pdf},
  linksoftware = sheffieldgit # {GPmat/},
  group =	 {gplvm,dimensional reduction},
  errata1 =	 {Equation (3) is missing integration over
                  $\mathbf{f}$.},
  errataCredit1 ={Laurens van der Maaten},
  errata2 =	 {Equation after Equation (5), the $v_{ij}$ should be
                  inside the sum sign.},
  abstract =	 {The Gaussian process latent variable model (GP-LVM)
                  is a generative approach to non-linear low
                  dimensional embedding, that provides a smooth
                  probabilistic mapping from latent to data space. It
                  is also a non-linear generalization of probabilistic
                  PCA (PPCA) \cite{Tipping:probpca99}. While most
                  approaches to non-linear dimensionality methods
                  focus on preserving local distances in data space,
                  the GP-LVM focusses on exactly the opposite. Being a
                  smooth mapping from latent to data space, it
                  focusses on keeping things apart in latent space
                  that are far apart in data space. In this paper we
                  first provide an overview of dimensionality
                  reduction techniques, placing the emphasis on the
                  kind of distance relation preserved. We then show
                  how the GP-LVM can be generalized, through back
                  constraints, to additionally preserve local
                  distances. We give illustrative experiments on
                  common data sets.}
}

@Article{Pena:fbd04,
  author =	 {Tonatiuh {Pe\~na-Centeno} and Neil D. Lawrence},
  title =	 {Optimising Kernel Parameters and Regularisation
                  Coefficients for Non-linear Discriminant Analysis},
  journal =	 jmlr,
  volume =	 7,
  OPTnumber =	 {},
  pages =	 {455--491},
  month =	 2,
  abstract =	 {In this paper we consider a novel Bayesian
                  interpretation of Fisher's discriminiant
                  analysis. We relate Rayleigh's coefficient to a
                  noise model that minimizes a cost based on the most
                  probable class centres and that abandons the
                  `regression to the labels' assumption used by other
                  algorithms. This yields a direction of
                  discrimination equivalent to Fisher's
                  discriminant. We use Bayes' rule to infer the
                  posterior distribution for the direction of
                  discrimination and in this process, priors and
                  constraining distributions are incorporated to reach
                  the desired result. Going further, with the use of a
                  Gaussian process prior we show the equivalence of
                  our model to a regularised kernel Fisher's
                  discriminant. A key advantage of our approach is the
                  facility to determine kernel parameters and the
                  regularisation coefficient through the optimisation
                  of the marginal log-likelihood of the data. An added
                  bonus of the new formulation is that it enables us
                  to link the regularisation coefficient with the
                  generalisation error.},
  linksoftware = softwarehttp # {bfd/},
  linkpdf =
                  {http://www.jmlr.org/papers/volume7/centeno06a/centeno06a.pdf},
  label1 =	 {JMLR Abstract},
  link1 =	 {http://www.jmlr.org/papers/v7/centeno06a.html},
  annote =	 {An earlier version is available as technical report
                  number CS-04-13, see \cite{Pena:fbd-tech04}.},
  year =	 2006,
  group =	 {shefml}
}

@TechReport{Pena:fbd-tech04,
  author =	 {Tonatiuh {Pe\~na-Centeno} and Neil D. Lawrence},
  title =	 {Optimising Kernel Parameters and Regularisation
                  Coefficients for Non-linear Discriminant Analysis},
  abstract =	 {In this paper we consider a Bayesian interpretation
                  of Fisher's discriminant. By relating Rayleigh's
                  coefficient to a likelihood function and through the
                  choice of a suitable prior we use Bayes' rule to
                  infer a posterior distribution over
                  projections. Through the use of a Gaussian process
                  prior we show the equivalence of our model to a
                  regularised kernel Fisher's discriminant. A key
                  advantage of our approach is the facility to
                  determine kernel parameters and the regularisation
                  coefficient through optimisation of the marginalised
                  likelihood of the data.},
  linkpsgz =	 shefftp # {bfdPaper.ps.gz},
  linksoftware = softwarehttp # {bfd/},
  institution =	 sheftech,
  number =	 {CS-04-13},
  year =	 2004,
  group =	 {shefml}
}

@InProceedings{AbdelHaleem:acoustic04,
  author =	 {Yasser H. {Abdel-Haleem} and Steve Renals and Neil D. Lawrence},
  title =	 {Acoustic Space Dimensionality Selection and
                  Combination using the Maximum Entropy Principle},
  crossref =	 {icassp04},
  linkpdf =	 shefftp # {icassp04.pdf},
  abstract =	 {In this paper we propose a discriminative approach
                  to acoustic space dimensionality selection based on
                  maximum entropy modelling. We form a set of
                  constraints by composing the acoustic space with the
                  space of phone classes, and use a continuous feature
                  formulation of maximum entropy modelling to select
                  an optimal feature set. The suggested approach has
                  two steps: (1) the selection of the best acoustic
                  space that efficiently and economically represents
                  the acoustic data and its variability; (2) the
                  combination of selected acoustic features in the
                  maximum entropy framework to estimate the posterior
                  probabilities over the phonetic labels given the
                  acoustic input. Specific contributions of this paper
                  include a parameter estimation algorithm
                  (generalized improved iterative scaling) that
                  enables the use of negative features, the
                  parameterization of constraint functions using
                  Gaussian mixture models, and experimental results
                  using the TIMIT database.}
}

@TechReport{Frey:Markovian98,
  author =	 {Brendan J. Frey and Neil D. Lawrence and Christopher
                  M. Bishop},
  title =	 {Markovian inference in belief networks},
  institution =	 {The Beckman Institute},
  year =	 1998,
  address =	 {University of Illinois at Urbana-Champaign, 405
                  North Mathews Avenue, Urbana, IL 61801, USA},
  note =	 {Originally submitted to \emph{NIPS 1998}},
  abstract =	 {Bayesian belief networks can represent the
                  complicated probabilistic processes that form
                  natural sensory inputs. Once the parameters of the
                  network have been learned,nonlinear inferences about
                  the input can be made by computing the posterior
                  distribution over the hidden units (e.g., depth in
                  stereo vision) given the input. Computing the
                  posterior distribution exactly is not practical in
                  richly-connected networks, but it turns out that by
                  using a variational (a.k.a., mean field) method, it
                  is easy to find a product-form distribution that
                  approximates the true posterior distribution. This
                  approximation assumes that the hidden variables are
                  independent given the current input. In this paper,
                  we explore a more powerful variational technique
                  that models the posterior distribution using a
                  Markov chain. We compare this method with inference
                  using mean fields and mixtures of mean fields in
                  randomly generated networks.},
  linkpsgz =	 myftp # {mi.ps.gz}
}

@TechReport{Lawrence:GCA01,
  author =	 {Neil D. Lawrence and Michael E. Tipping},
  title =	 {Generalised Component Analysis},
  note =	 {},
  year =	 {2003},
  abstract =	 {Principal component analysis is a well known
                  approach for determining the principal sub-space of
                  a data-set. Independent component analysis is a
                  widely utilised technique for recovering the
                  linearly embedded independent components of a
                  data-set. In this paper we develop an algorithm
                  that, for super-Gaussian sources, extracts the
                  direction and number of independent components of a
                  data-set and determines the principal sub-space of
                  the remaining components. This is achieved through
                  the use of a latent variable model. We refer to the
                  approach as Generalised Component Analysis and
                  demonstrate its ability to both extract indpendent
                  and principal components, as well as to determine
                  the number of independent components, on toy and
                  real word data-sets.},
  institution =	 sheftech,
  number =	 {CS-03-10},
  linkpsgz =	 shefftp # {gca.ps.gz},
  linksoftware = softwarehttp # {gca/},
  group =	 {shefml}
}

@TechReport{Lawrence:ICA99,
  author =	 {Neil D. Lawrence and Christopher M. Bishop},
  title =	 {Variational {B}ayesian Independent Component
                  Analysis},
  year =	 2000,
  abstract =	 {Blind separation of signals through the info-max
                  algorithm may be viewed as maximum likelihood
                  learning in a latent variable model. In this paper
                  we present an alternative approach to maximum
                  likelihood learning in these models, namely Bayesian
                  inference. It has already been shown how Bayesian
                  inference can be applied to determine latent
                  dimensionality in principal component analysis
                  models \cite{Bishop:bayesPCA98}. In this paper we
                  derive a similar approach for removing unecessary
                  source dimensions in an independent component
                  analysis model. We present results on a toy data-set
                  and on some artificially mixed images.},
  linkpsgz =	 myftp # {bica_report.ps.gz}
}

@TechReport{Lawrence:largescale06,
  author =	 {Neil D. Lawrence},
  title =	 {Large Scale Learning with the {G}aussian Process
                  Latent Variable Model},
  institution =	 {University of Sheffield},
  year =	 {2006},
  OPTkey =	 {},
  OPTtype =	 {},
  number =	 {CS-06-05},
  linkpdf =	 shefftp # {gplvmSparse.pdf},
  linksoftware = sheffieldgit # {GPmat/},
  month =	 {},
  errata1 =	 {Page 14: Sign and inverse wrong on definition of
                  $\mathbf{C}$ just before (4). Corrected in version
                  from December 16, 2008.},
  errataCredit1 ={John Guiver},
  abstract =	 {In this paper we apply the latest techniques in
                  sparse Gaussian process regression (GPR) to the
                  Gaussian process latent variable model (GP-LVM). We
                  review three techniques and discuss how they may be
                  implemented in the context of the GP-LVM. We briefly
                  consider a GPR toy problem to highlight the
                  strenghts and weaknesses of the different approaches
                  before studying the perfomance of these techniques
                  on a benchmark visualisation data set.},
  note =	 {Document updated on December 16, 2008. Original from
                  February 17th, 2006.},
  group =	 {gplvm,motion,shefml,dimensional reduction}
}

@InProceedings{Lawrence:gplvm03,
  author =	 {Neil D. Lawrence},
  title =	 {{G}aussian Process Models for Visualisation of High
                  Dimensional Data},
  pages =	 {329--336},
  crossref =	 {Thrun:nips03},
  abstract =	 {In this paper we introduce a new underlying
                  probabilistic model for principal component analysis
                  (PCA). Our formulation interprets PCA as a
                  particular Gaussian process prior on a mapping from
                  a latent space to the observed data-space. We show
                  that if the prior's covariance function constrains
                  the mappings to be linear the model is equivalent to
                  PCA, we then extend the model by considering less
                  restrictive covariance functions which allow
                  non-linear mappings. This more general Gaussian
                  process latent variable model (GPLVM) is then
                  evaluated as an approach to the visualisation of
                  high dimensional data for three different
                  data-sets. Additionally our non-linear algorithm can
                  be \emph{further} kernelised leading to `twin kernel
                  PCA' in which a \emph{mapping} \emph{between feature
                  spaces} occurs.},
  linkpsgz =	 shefftp # {gplvm.ps.gz},
  linksoftware = softwarehttp # {gplvm/},
  group =	 {shefml,gplvm,dimensional reduction}
}

@InCollection{Lawrence:extensions05,
  author =	 {Neil D. Lawrence and John C. Platt and Michael
                  I. Jordan},
  title =	 {Extensions of the Informative Vector Machine},
  crossref =	 {Winkler:smlw04},
  pages =	 {56--87},
  abstract =	 {The informative vector machine (IVM) is a practical
                  method for Gaussian process regression and
                  classification. The IVM produces a sparse
                  approximation to a Gaussian process by combining
                  assumed density filtering with a heuristic for
                  choosing points based on minimizing posterior
                  entropy. This paper extends IVM in several
                  ways. First, we propose a novel noise model that
                  allows the IVM to be applied to a mixture of labeled
                  and unlabeled data. Second, we use IVM on a
                  block-diagonal covariance matrix, for ``learning to
                  learn'' from related tasks. Third, we modify the IVM
                  to incorporate prior knowledge from known
                  invariances. All of these extensions are tested on
                  artificial and real data.},
  linkpsgz =	 shefftp # {ivmdev.ps.gz},
  linksoftware = softwarehttp # {ivm/},
  group =	 {shefml,gp,spgp}
}

@Unpublished{Milo:proboligo04,
  author =	 {Marta Milo and Mahesan Niranjan and Matthew
                  C. Holley and Magnus Rattray and Neil D. Lawrence},
  title =	 {A Probabilistic Approach for Summarising
                  Oligonucleotide Gene Expression Data},
  year =	 2005
}

@Article{Liu:tractable04,
  author =	 {Xuejun Liu and Marta Milo and Neil D. Lawrence and
                  Magnus Rattray},
  title =	 {A Tractable Probabilistic Model for {A}ffymetrix
                  Probe-level Analysis across Multiple Chips},
  journal =	 bioinf,
  year =	 2005,
  volume =	 21,
  number =	 18,
  pages =	 {3637--3644},
  month = 7,
  day = 14,
  doi =		 {10.1093/bioinformatics/bti583},
  pmid =	 16020470,
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/21/18/3637},
  label1 =	 {Advance Access},
  link1 =
                  {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/bti583?ijkey=NwpQ2i5ZVAlgBwj&keytype=ref},
  linksoftware =
                  {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  label2 =	 {Pre-print PDF},
  link2 =	 shefftp # {multigmos.pdf},
  group =	 {shefml,puma},
  abstract =	 {{\bf Motivation:} Affymetrix GeneChip arrays are
                  currently the most widely used microarray
                  technology. Many summarisation methods have been
                  developed to provide gene expression levels from
                  Affymetrix probe-level data. Most of the currently
                  popular methods do not provide a measure of
                  uncertainty for the expression level of each
                  gene. The use of probabilistic models can overcome
                  this limitation. A full hierarchical Bayesian
                  approach requires the use of computationally
                  intensive MCMC methods that are impractical for
                  large data sets. An alternative computationally
                  efficient probabilistic model, mgMOS, uses Gamma
                  distributions to model specific and non-specific
                  binding with a latent variable to capture variations
                  in probe affinity. Although promising, the main
                  limitations of this model are that it does not use
                  information from multiple chips and that it does not
                  account for specific binding to the mismatch (MM)
                  probes.\\\\ {\bf Results:} We extend mgMOS to model
                  the binding affinity of probe-pairs across multiple
                  chips and to capture the effect of specific binding
                  to MM probes. The new model, multi-mgMOS, provides
                  improved accuracy, as demonstrated on some
                  bench-mark data sets and a real time-course data
                  set, and is much more computationally efficient than
                  a competing hierarchical Bayesian approach that
                  requires MCMC sampling. We demonstrate how the
                  probabilistic model can be used to estimate
                  credibility intervals for expression levels and
                  their log-ratios between conditions.\\\\ {\bf
                  Availability:} Both mgMOS and the new model
                  multi-mgMOS have been implemented in an R package
                  that is currently available from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.}
}

@Misc{Lawrence:informative01,
  author =	 {Neil D. Lawrence and Ralf Herbrich},
  title =	 {A Sparse {B}ayesian Compression Scheme --- The
                  Informative Vector Machine},
  howpublished = {Presentation during the Kernel Workshop at NIPS
                  2001},
  year =	 2001,
  abstract =	 {Kernel based learning algorithms allow the mapping
                  of data-set into an infinite dimensional feature
                  space in which a classification may be performed. As
                  such kernel methods represent a powerful approach to
                  the solution of many non-linear problems. However
                  kernel methods do suffer from one unfortunate
                  drawback, the Gram matrix contains m rows and
                  columns where m is the number of data-points. Many
                  operations are therefore precluded (e.g.~matrix
                  inverse $O(m^3)$) when data-sets containing more
                  than about $10^4$ points are encountered. One approach
                  to resolving these issues is to look for sparse
                  representations of the data-set A sparse
                  representation contains a reduced number of
                  examples. Loosely speaking we are interested in
                  extracting the maximum amount of information from
                  the minimum number of data-points. To achieve this
                  in a principled manner we are interested in
                  estimating the amount of information each data-point
                  contains. In the framework presented here we make
                  use of the Bayesian methodology to determine how
                  much information is gained from each data-point.},
  linkpsgz =	 myftp # {kips01.ps.gz},
  group =	 {shefml}
}

@TechReport{Lawrence:ivmTech04,
  author =	 {Neil D. Lawrence and Matthias Seeger and Ralf
                  Herbrich},
  title =	 {The Informative Vector Machine: A Practical
                  Probabilistic Alternative to the Support Vector
                  Machine},
  institution =	 {Department of Computer Science, University of
                  Sheffield},
  year =	 2004,
  linkpsgz =	 shefftp # {ivmTechreport.ps.gz},
  label1 =	 {Matlab Software},
  link1 =	 softwarehttp # {ivm/},
  label2 =	 {C++ Software},
  link2 =	 sheffieldgit # {GPc/},
  note =	 {Last updated December 2005},
  OPTtype =	 {},
  number =	 {CS-04-07},
  OPTaddress =	 {},
  abstract =	 {We present a practical probabilistic alternative to
                  the popular support vector machine (SVM). The
                  algorithm is an approximation to a Gaussian process,
                  and is probabilistic in the sense that it maintains
                  the process variance that is implied by the use of a
                  kernel function, which the SVM discards. We show
                  that these variances may be tracked and made use of
                  selection of an active set which gives a sparse
                  representation for the model. For an active set size
                  of $d$ our algorithm exhibits $O(d^{2}N)$
                  computational complexity and $O(dN)$ storage
                  requirements. It has already been shown that the
                  approach is comptetive with the SVM in terms of
                  performance and running time, here we give more
                  details of the approach and demonstrate that kernel
                  parameters may also be learned in a practical and
                  effective manner.},
  group =	 {shefml}
}

@TechReport{Lawrence:gplvmTech04,
  author =	 {Neil D. Lawrence},
  title =	 {Probabilistic Non-linear Principal Component
                  Analysis with {G}aussian Process Latent Variable
                  Models},
  institution =	 {Department of Computer Science, University of
                  Sheffield},
  year =	 2004,
  number =	 {CS-04-08},
  abstract =	 {Summarising a high dimensional data-set with a low
                  dimensional embedding is a standard approach for
                  exploring its structure. In this paper we provide an
                  overview of some existing techniques for discovering
                  such embeddings. We then introduce a novel
                  probabilistic interpretation of principal component
                  analysis (PCA) that we term dual probabilistic PCA
                  (DPPCA). The DPPCA model has the additional
                  advantage that the linear mappings from the embedded
                  space can easily be non-linearised through Gaussian
                  processes. We refer to this model as a Gaussian
                  process latent variable model (GPLVM). We develop a
                  practical algorithm for GPLVMs which allow for
                  non-linear mappings from the embedded space giving a
                  non-linear probabilistic version of PCA. We develop
                  the new algorithm to provide a principled approach
                  to handling discrete valued data and missing
                  attributes. We demonstrate the algorithm on a range
                  of real-world and artificially generated data-sets
                  and finally, through analysis of the GPLVM objective
                  function, we relate the algorithm to popular
                  spectral techniques such as kernel PCA and
                  multidimensional scaling.},
  errata1 =	 {Page 9: just before Section 5. The inequality should
                  be $N>>d$ not $N<<d$.},
  errataCredit1 ={Shaobo Hou},
  linkpsgz =	 shefftp # {nlpca.ps.gz},
  linksoftware = softwarehttp # {gplvm/},
  group =	 {shefml,dimensional reduction}
}

@InProceedings{Lawrence:ivm02,
  author =	 {Neil D. Lawrence and Matthias Seeger and Ralf
                  Herbrich},
  title =	 {Fast Sparse {G}aussian Process Methods: The
                  Informative Vector Machine},
  crossref =	 {Becker:nips02},
  pages =	 {625--632},
  abstract =	 {We present a framework for sparse Gaussian process
                  (GP) methods which uses forward selection with
                  criteria based on information-theoretical
                  principles, previously suggested for active
                  learning. In contrast to most previous work on
                  sparse GPs, our goal is not only to learn sparse
                  predictors (which can be evaluated in $O(d)$ rather
                  than $O(n)$, $d<<n$, $n$ the number of training
                  points), but also to perform training under strong
                  restrictions on time and memory requirements. The
                  scaling of our method is at most $O(nd^2)$, and in
                  large real-world classification experiments we show
                  that it can match prediction performance of the
                  popular support vector machine (SVM), yet it
                  requires only a fraction of the training time. In
                  contrast to the SVM, our approximation produces
                  estimates of predictive probabilities (`error
                  bars'), allows for Bayesian model selection and is
                  less complex in implementation.},
  linkpsgz =	 shefftp # {ivm.ps.gz},
  linksoftware = softwarehttp # {ivm},
  group =	 {shefml,gp,spgp}
}

@InProceedings{Lawrence:larger07,
  author =	 {Neil D. Lawrence},
  title =	 {Learning for Larger Datasets with the {G}aussian
                  Process Latent Variable Model},
  crossref =	 {Meila:aistats07},
  OPTkey =	 {},
  OPTbooktitle = {},
  pages =	 {243--250},
  OPTyear =	 {},
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  OPTmonth =	 {},
  OPTorganization ={},
  OPTpublisher = {},
  OPTnote =	 {},
  OPTannote =	 {},
  OPTpmid =	 {},
  linkpdf =	 shefftp # {gplvmLarger.pdf},
  linksoftware = sheffieldgit # {GPmat/},
  abstract =	 {In this paper we apply the latest techniques in
                  sparse Gaussian process regression (GPR) to the
                  Gaussian process latent variable model (GP-LVM). We
                  review three techniques and discuss how they may be
                  implemented in the context of the GP-LVM. Each
                  approach is then implemented on a well known
                  benchmark data set and compared with earlier
                  attempts to sparsify the model.},
  group =	 {shefml,gp,spgp,gplvm,dimensional reduction}
}

@InProceedings{Lawrence:ltu01,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Learning for Multi-layer networks of
                  Linear Threshold Units},
  crossref =	 {Jaakkola:aistats01},
  pages =	 {245--252},
  abstract =	 {Linear threshold units were originally proposed as
                  models of biological neurons. They were widely
                  studied in the context of the perceptron
                  \cite{Rosenblatt:book62}. Due to the difficulties of
                  finding a general algorithm for networks with hidden
                  nodes, they never passed into general use. We derive
                  an algorithm in the context of graphical models and
                  show how it may be applied in multi-layer networks
                  of linear threshold units. We demonstrate the
                  algorithm through three well known datasets.},
  linkpsgz =	 myftp # {ltus.ps.gz},
  linkpdf =	 myftp # {ltus.pdf}
}

@TechReport{Lawrence:ltu_report00,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Learning for Multi-layer networks of
                  Linear Threshold Units},
  year =	 2000,
  note =	 {Draft report, slightly extended version of
                  \cite{Lawrence:ltu01}.},
  abstract =	 {Linear threshold units were originally proposed as
                  models of biological neurons. They were widely
                  studied in the context of the perceptron
                  \cite{Rosenblatt:book62}. Due to the difficulties of
                  finding a general algorithm for networks with hidden
                  nodes, they never passed into general use. We derive
                  an algorithm in the context of graphical models and
                  show how it may be applied in multi-layer networks
                  of linear threshold units. We demonstrate the
                  algorithm through three well known datasets.},
  linkpsgz =	 myftp # {ltupaper.ps.gz}
}

@InProceedings{Lawrence:microarray03,
  author =	 {Neil D. Lawrence and Marta Milo and Mahesan Niranjan
                  and Penny Rashbass and Stephan Soullier},
  title =	 {{B}ayesian Processing of Microarray Images},
  pages =	 {71--80},
  crossref =	 {Molina:nnsp03},
  abstract =	 {Gene expression measurements quantify the level of
                  mRNA produced from each gene. Two principal methods
                  exist for producing slides for extracting these
                  levels: photolithography and spotted arrays. One
                  difficulty with the spotted array format is
                  determining the size and location of the spots on
                  the array. In this paper we present a Bayesian
                  approach to processing images produced by these
                  arrays that seeks posterior distributions over the
                  size and positions of the spots. This enables us to
                  estimate expression ratios and their
                  variances. Exact inference for the model we specify
                  is intractable; we develop an approximate inference
                  technique which combines importance sampling with
                  variational inference. Our technique has already
                  been shown to be more consistent than both manual
                  processing and another automated technique
                  \cite{Lawrence:variability03}. Here we present
                  large-scale results for twenty-four microarray
                  slides each representing 5760 genes and show the
                  dramatic effects of incorporating variance in our
                  downsteam analysis. Software based on this algorithm
                  is available for academic use.},
  linkpsgz =	 shefftp # {visMicroarray.ps.gz},
  linkpdf =	 shefftp # {visMicroarray.pdf},
  linksoftware = softwarehttp # {vis/},
  errata1 =	 {Page 7: on the bottom half of the page in Algorithm
                  1. Line number 6 (the first line in the repeat
                  loop). This statement should be before the repeat
                  loop starts.},
  errataCredit1 ={Nilanjan Dasgupta},
  group =	 {shefml,mig}
}

@InProceedings{Lawrence:mixtures98,
  author =	 {Neil D. Lawrence and Christopher M. Bishop and
                  Michael I. Jordan},
  title =	 {Mixture Representations for Inference and Learning
                  in {B}oltzmann Machines},
  pages =	 {320--327},
  crossref =	 {Cooper:uai98},
  year =	 1998,
  abstract =	 {Boltzmann machines are undirected graphical models
                  with two-state stochastic variables, in which the
                  logarithms of the clique potentials are quadratic
                  functions of the node states. They have been widely
                  studied in the neural computing literature, although
                  their practical applicability has been limited by
                  the difficulty of finding an effective learning
                  algorithm. One well-established approach, known as
                  mean field theory, represents the stochastic
                  distribution using a factorized
                  approximation. However, the corresponding learning
                  algorithm often fails to find a good solution. We
                  conjecture that this is due to the implicit
                  uni-modality of the mean field approximation which
                  is therefore unable to capture multi-modality in the
                  true distribution. In this paper we use variational
                  methods to approximate the stochastic distribution
                  using multi-modal \emph{mixtures} of factorized
                  distributions. We present results for both inference
                  and learning to demonstrate the effectiveness of
                  this approach.},
  linkpsgz =	 myftp # {boltzmann.ps.gz}
}

@TechReport{Lawrence:nnmixtures99,
  title =	 {A Variational {B}ayesian Committee of Neural
                  Networks},
  author =	 {Neil D. Lawrence and Mehdi Azzouzi},
  year =	 1999,
  abstract =	 {Exact inference in Bayesian neural networks is non
                  analytic to compute and as a result approximate
                  approaches such as the evidence procedure,
                  Monte-Carlo sampling and variational inference have
                  been proposed. In this paper we present a general
                  overview of the Bayesian approach with a particular
                  emphasis on the variational procedure. We then
                  present a new approximating distribution based on
                  \emph{mixtures} of Gaussian distributions and show
                  how it may be implemented. We present results on a
                  simple toy problem and on two real world data-sets.},
  linkpsgz =	 myftp # {nnmixture.ps.gz}
}

@InProceedings{Lawrence:noisy01,
  author =	 {Neil D. Lawrence and Bernhard Sch\"olkopf},
  title =	 {Estimating a Kernel {F}isher Discriminant in the
                  Presence of Label Noise},
  crossref =	 {Brodley:icml01},
  OPTpages =	 {},
  abstract =	 {Data noise is present in many machine learning
                  problems domains, some of these are well studied but
                  others have received less attention. In this paper
                  we propose an algorithm for constructing a kernel
                  Fisher discriminant (KFD) from training examples
                  with \emph{noisy labels}. The approach allows to
                  associate with each example a probability of the
                  label being flipped. We utilise an expectation
                  maximization (EM) algorithm for updating the
                  probabilities. The E-step uses class conditional
                  probabilities estimated as a by-product of the KFD
                  algorithm. The M-step updates the flip probabilities
                  and determines the parameters of the
                  discriminant. We have applied the approach to two
                  real-world data-sets. The results show the
                  feasibility of the approach.},
  linksoftware = softwarehttp # {nkfd/},
  linkpsgz =	 shefftp # {noisyfisher.ps.gz},
  group =	 {shefml}
}

@InProceedings{Lawrence:nrd01,
  author =	 {Neil D. Lawrence},
  title =	 {Node Relevance Determination},
  crossref =	 {Marinaro:wirn01},
  OPTpages =	 {},
  abstract =	 {Hierarchical Bayesian inference in parameterised
                  models offers an approach for controlling
                  complexity. In this paper we utilise a novel prior
                  for the leaning of a model's structure. We call the
                  prior \emph{node relevance determination}. It is
                  applicable in a range of models including sigmoid
                  belief networks and Boltzmann machines. We
                  demonstrate how the approach may be applied to
                  determine structure in a multi-layer perceptron.},
  linkpdf =	 myftp # {structure.pdf},
  linkpsgz =	 myftp # {structure.ps.gz}
}

@Patent{Lawrence:patent01,
  author =	 {Neil D. Lawrence and Anthony I. T. Rowstron and
                 Christopher M. Bishop and Michael J. Taylor},
  title =	 {System and Method for Replicating Data in a Distributed System},
  patentnumber = {6889333},
  country =      {U.S.A.},
  link1 =        {http://www.google.com/patents?vid=USPAT6889333},
  label1 =       {Google Patents},
  abstract =     {It is common in distributed systems to replicate data.
                 In many cases, this data evolves in a consistent
                 fashion, and this evolution can be modeled. A
                 probabilistic model of the evolution allows us to
                 estimate the divergence of the replicas and can be
                 used by the application to alter its behavior, for
                 example, to control synchronization times, to
                 determine the propagation of writes, and to convey to
                 the user information about how much the data may have
                 evolved. In this paper, we describe how the evolution
                 of the data may be modeled and outline how the
                 probabilistic model may be utilized in various
                 applications, concentrating on a news database
                 example.},
  note =         {Filing date: Nov 1st 2001, issue date May 3rd, 2005},
  year =	 2005
}
@TechReport{Lawrence:structure01,
  author =	 {Neil D. Lawrence and Mehdi Azzouzi},
  title =	 {The Structure of Neural Network Posteriors},
  note =	 {},
  OPTkey =	 {},
  OPTmonth =	 {},
  year =	 2001,
  OPTannote =	 {},
  abstract =	 {Exact inference in Bayesian neural networks is non
                  analytic to compute and as a result approximate
                  approaches such as the evidence procedure,
                  Monte-Carlo sampling and variational inference have
                  been proposed. In this paper we explore the
                  structure of the posterior distributions in such a
                  model through a new approximating distribution based
                  on \emph{mixtures} of Gaussian distributions and
                  show how it may be implemented.},
  linkpdf =	 myftp # {mixture.pdf},
  linkpsgz =	 myftp # {mixture.ps.gz}
}

@InProceedings{Lawrence:sync01,
  author =	 {Neil D. Lawrence and Anthony I. T. Rowstron and
                  Christopher M. Bishop and Michael J. Taylor},
  title =	 {Optimising Synchronisation Times for Mobile Devices},
  pages =	 {1401--1408},
  crossref =	 {Dietterich:nips01},
  abstract =	 {With the increasing number of users of mobile
                  computing devices (e.g. personal digital assistants)
                  and the advent of third generation mobile phones,
                  wireless communications are becoming increasingly
                  important. Many applications rely on the device
                  maintaining a \emph{replica} of a data-structure
                  which is stored on a server, for example news
                  databases, calendars and e-mail. In this paper we
                  explore the question of the optimal strategy for
                  synchronising such replicas. We utilise
                  probabilistic models to represent how the
                  data-structures evolve and to model user
                  behaviour. We then formulate objective functions
                  which can be minimised with respect to the
                  synchronisation timings. We demonstrate, using two
                  real world data-sets, that a user can obtain more
                  up-to-date information using our approach.},
  linkpsgz =	 myftp # {jitcache.ps.gz},
  group =	 {shefml}
}

@PhdThesis{Lawrence:thesis00,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Inference in Probabilistic Models},
  school =	 {Computer Laboratory, University of Cambridge},
  address =	 {New Museums Site, Pembroke Street, Cambridge, CB2
                  3QG, U.K.},
  year =	 2000,
  note =	 {Available from
                  \url{http://www.thelawrences.net/neil}},
  abstract =	 {One of the key objectives of modern artificial
                  intelligence is the handling of
                  uncertainty. Probability theory provides a framework
                  for handling of uncertainty in a principled
                  manner. Probabilistic inference is the process of
                  reasoning under uncertainty and as such is vital for
                  both learning and decision making in probabilistic
                  models. For many models of interest this inference
                  proves to be intractable and to make progress
                  approximate methods need to be considered. This
                  thesis concerns itself with a particular
                  approximating formalism known as variational
                  inference \cite{Jordan:variational98}. Variational
                  inference has the advantage that it often provides
                  bounds on quantities of interest, such as
                  marginalised likelihoods. In this thesis we describe
                  a general framework for the implementation of the
                  variational approach. We then explore the approach
                  in several different probabilistic models including
                  undirected and directed graphs, Bayesian neural
                  networks and independent component analysis. In
                  those models which have been handled using
                  variational inference previously we show how we may
                  improve the quality of our inference engine by
                  considering more complex variational
                  distributions.},
  linkpsgz =	 myftp # {thesis.ps.gz},
  linkpdf =	 myftp # {thesis.pdf}
}

@Article{Lawrence:variability03,
  author =	 {Neil D. Lawrence and Marta Milo and Mahesan Niranjan
                  and Penny Rashbass and Stephan Soullier},
  title =	 {Reducing the Variability in {cDNA} Microarray Image
                  Processing by {B}ayesian Inference},
  journal =	 bioinf,
  year =	 {2004},
  volume =	 20,
  number =	 4,
  month = 1,
  day = 22,
  doi =		 {10.1093/bioinformatics/btg438},
  pages =	 {518--526},
  pmid =	 14990447,
  abstract =	 {{\bf Motivation:} Gene expression levels are
                  obtained from microarray experiments through the
                  extraction of pixel intensities from a scanned image
                  of the slide. It is widely acknowledged that
                  variabilities can occur in expression levels
                  extracted from the same images by different users
                  with the same software packages. These
                  inconsistencies arise due to differences in the
                  refinement of the placement of the microarray
                  `grids'. We introduce a novel automated approach to
                  the refinement of grid placements that is based upon
                  the use of Bayesian inference for determining the
                  size, shape and positioning of the microarray
                  `spots', capturing uncertainty that can be passed to
                  downstream analysis.\\\\ {\bf Results:} Our
                  experiments demonstrate that variability between
                  users can be significantly reduced using the
                  approach. The automated nature of the approach also
                  saves hours of researchers' time normally spent in
                  refining the grid placement.\\\\ {\bf Availability:}
                  A MATLAB implementation of the algorithm and an
                  image of the slide used in our experiments, as well
                  as the code necessary to recreate them are available
                  for non-commercial use from
                  \url{http://inverseprobability.com/vis}.},
  linkpsgz =	 shefftp # {microarrayImage.ps.gz},
  link1 =	 shefftp # {microarrayImage.pdf},
  label1 =	 {Pre-print PDF},
  linksoftware = softwarehttp # {vis/},
  group =	 {shefml,mig,puma}
}

@Article{Lerner:comparison01,
  author =	 {Boaz Lerner and Neil D. Lawrence},
  title =	 {A Comparison of State-of-the-Art Classification
                  Techniques with Application to Cytogenetics},
  journal =	 {Neural Computing and Applications},
  year =	 2001,
  month = 4,
  volume =	 10,
  number =	 1,
  pages =	 {39--47},
  abstract =	 {Several state-of-the-art techniques: a neural
                  network, Bayesian neural network, support vector
                  machine and naive Bayesian classifier are
                  experimentally evaluated in discriminating
                  fluorescence in-situ hybridization (FISH)
                  signals. Highly accurate classification of signals
                  from real data and artefacts of two cytogenetic
                  probes (colours) is required for detecting
                  abnormalities in the data. More than 3,100 FISH
                  signals are classified by the techniques into colour
                  and as real or artefact with accuracies of around
                  98\% and 88\%, respectively. The results of the
                  comparison also show a trade-off between simplicity
                  represented by the naive Bayesian classifier and
                  high classification performance represented by the
                  other techniques.},
  linkpsgz =	 myftp # {comparison.ps.gz}
}

@InProceedings{Rowstron:sync01,
  author =	 {Anthony I. T. Rowstron and Neil D. Lawrence and
                  Christopher M. Bishop},
  title =	 {Probabilistic Modelling of Replica Divergence},
  booktitle =	 {Proceedings of the 8th Workshop on Hot Topics in
                  Operating Systems HOTOS (VIII)},
  OPTpages =	 {},
  year =	 2001,
  OPTeditor =	 {},
  OPTaddress =	 {},
  OPTpublisher = {},
  abstract =	 {It is common in distributed systems to replicate
                  data. In many cases this data evolves in a
                  consistent fashion and this evolution can be
                  modelled. A \emph{probabilistic model} of the
                  evolution allows us to estimate the divergence of
                  the replicas and can be used by the application to
                  alter its behaviour, for example to control
                  synchronisation times, to determine the propagation
                  of writes, and to convey to the user information
                  about how much the data may have evolved. In this
                  paper, we describe how the evolution of the data may
                  be modelled and outline how the probabilistic model
                  may be utilised in various applications,
                  concentrating on a news database example.},
  linkpdf =	 myftp # {hotos_sync.pdf},
  linkpsgz =	 myftp # {hotos_sync.ps.gz}
}

@InProceedings{Seeger:fast03,
  author =	 {Matthias Seeger and Christopher K. I. Williams and
                  Neil D. Lawrence},
  title =	 {Fast Forward Selection to Speed Up Sparse {G}aussian
                  Process Regression},
  crossref =	 {Bishop:aistats03},
  abstract =	 {We present a method for the sparse greedy
                  approximation of Bayesian Gaussian process
                  regression, featuring a novel heuristic for very
                  fast forward selection. Our method is essentially as
                  fast as an equivalent one which selects the
                  ``support'' patterns at random, yet it can
                  outperform random selection on hard curve fitting
                  tasks. More importantly, it leads to a sufficiently
                  stable approximation of the log marginal likelihood
                  of the training data, which can be optimised to
                  adjust a large number of hyperparameters
                  automatically. We demonstrate the model selection
                  capabilities of the algorithm in a range of
                  experiments. In line with the development of our
                  method, we present a simple view on sparse
                  approximations for GP models and their underlying
                  assumptions and show relations to other methods.},
  linkpsgz =	 myftp # {fastForward.ps.gz},
  group =	 {shefml,spgp}
}

@InProceedings{Tipping:variational03,
  author =	 {Michael E. Tipping and Neil D. Lawrence},
  title =	 {A Variational Approach to Robust {B}ayesian
                  Interpolation},
  pages =	 {229--238},
  crossref =	 {Molina:nnsp03},
  abstract =	 {This paper details a robust Bayesian interpolation
                  procedure for linear-in-the-parameter
                  models. Robustness is achieved via a Student-$t$
                  noise model, defined hierarchically in terms of an
                  inverse-Gamma prior distribution over individual
                  Gaussian observation variances. Variational
                  techniques are exploited to update this prior in
                  light of the data, while also inferring all other
                  model variables. The key to this approach is
                  flexibility; it can infer Gaussian noise where
                  appropriate but can adapt to accommodate
                  heavier-tailed distributions in the presence of
                  outliers.},
  linkpdf =	 shefftp # {robustBayesian.pdf},
  group =	 {shefml}
}

@InProceedings{Vermaak:variational03,
  author =	 {Jaco Vermaak and Neil D. Lawrence and Patrick
                  P\'erez},
  title =	 {Variational Inference for Visual Tracking},
  booktitle =	 pCVPR,
  publisher =	 ieeecomp,
  year =	 2003,
  pages =	 {773--780},
  volume =	 {I},
  abstract =	 {The likelihood models used in probabilistic visual
                  tracking applications are often complex non-linear
                  and/or non-Gaussian functions, leading to
                  analytically intractable inference. Solutions then
                  require numerical approximation techniques, of which
                  the particle filter is a popular choice. Particle
                  filters, however, degrade in performance as the
                  dimensionality of the state space increases and the
                  support of the likelihood decreases. As an
                  alternative to particle filters this paper
                  introduces a variational approximation to the
                  tracking recursion. The variational inference is
                  intractable in itself, and is combined with an
                  efficient importance sampling procedure to obtain
                  the required estimates. The algorithm is shown to
                  compare favourably with particle filtering
                  techniques on a synthetic example and two real
                  tracking problems. The first involves the tracking
                  of a designated object in a video sequence based on
                  its colour properties, whereas the second involves
                  contour extraction in a single image.},
  linkpdf =	 shefftp # {variationalTracking.pdf},
  group =	 {shefml}
}

@TechReport{lawrence:sparse02,
  title =	 {Sparse {B}ayesian Learning: The Informative Vector
                  Machine},
  author =	 {Neil D. Lawrence and Matthias Seeger and Ralf
                  Herbrich},
  institution =	 {Department of Computer Science, Sheffield, UK},
  note =	 {This document has now evolved into
                  \cite{Lawrence:ivmTech04}.},
  year =	 {2002}
}

@TechReport{lawrence:variationalguide02,
  author =	 {Neil D. Lawrence},
  title =	 {Variational Inference Guide},
  institution =	 {University of Sheffield},
  year =	 {2002},
  abstract =	 {This report is a brief introduction to variational
                  inference for Bayesian models from the perspective
                  of the Expectation Maximisation (EM) algorithm
                  \cite{Dempster:EM77}. We start with an overview of
                  the EM algorithm from the perspective of variational
                  inference and then we show how approximate inference
                  may also be performed. We discuss briefly when
                  variational inference may be used and finally we
                  mention the variational importance sampler as an
                  alternative approach.},
  linkpsgz =	 shefftp # {variationalInference.ps.gz},
  linkpdf =	 shefftp # {variationalInference.pdf},
  group =	 {shefml}
}

@FailedPatent{Lawrence:patent03,
  author =	 {Neil D. Lawrence and Marta Milo},
  title =	 {Variational Importance Sampling},
  patentnumber = {0217827.5},
  country =      {U.K.},
  year =	 2003
}
@TechReport{Lawrence:matching04,
  author =	 {Neil D. Lawrence and Guido Sanguinetti},
  title =	 {Matching Kernels through {K}ullback-{L}eibler
                  Divergence Minimisation},
  year =	 2004,
  OPTkey =	 {},
  OPTtype =	 {},
  institution =	 sheftech,
  number =	 {CS-04-12},
  linkpsgz =	 shefftp # {KLobjTech.ps.gz},
  OPTmonth =	 {},
  OPTnote =	 {},
  abstract =	 {In this paper we study the general constrained
                  minimisation of Kullback-Leibler (KL) divergences
                  between two zero mean Gaussian distributions. We
                  reduce the problem to an equivalent minimisation
                  involving the eigenvectors of the two kernel
                  matrices, and provide explicit solutions in some
                  cases. We then focus, as an example, on the
                  important case of constraining the approximating
                  matrix to be block diagonal. We prove a stability
                  result on the approximating matrix, and speculate on
                  how these results may be used to give further
                  theoretical foundation to widely used techniques
                  such as spectral clustering.}
}

@Article{Sanguinetti:chipdyno06,
  author =	 {Guido Sanguinetti and Magnus Rattray and Neil D. Lawrence},
  title =	 {A probabilistic dynamical model for quantitative
                  inference of the regulatory mechanism of
                  transcription},
  journal =	 bioinf,
  year =	 2006,
  volume =	 22,
  number =	 14,
  pages =	 {1753--1759},
  pmid =	 16632490,
  doi =		 {10.1093/bioinformatics/btl154},
  linkpdf =
                  {http://bioinformatics.oxfordjournals.org/cgi/reprint/22/14/1753},
  label2 =	 {Supplementary Material},
  link2 =	 softwarehttp # {chipdyno/supplementary.pdf},
  linksoftware = sheffieldgit # {chipdyno/},
  abstract =	 {{\bf Motivation:} Quantitative estimation of the
                  regulatory relationship between transcription
                  factors and genes is a fundamental stepping stone
                  when trying to develop models of cellular
                  processes. This task, however, is difficult for a
                  number of reasons: transcription factors' expression
                  levels are often low and noisy, and many
                  transcription factors are post-transcriptionally
                  regulated. It is therefore useful to infer the
                  activity of the transcription factors from the
                  expression levels of their target genes.\\\\ {\bf
                  Results:} We introduce a novel probabilistic model
                  to infer transcription factor activities from
                  microarray data when the structure of the regulatory
                  network is known. The model is based on regression,
                  retaining the computational efficiency to allow
                  genome-wide investigation, but is rendered more
                  flexible by sampling regression coefficients
                  independently for each gene. This allows us to
                  determine the strength with which a transcription
                  factor regulates each of its target genes, therefore
                  providing a quantitative description of the
                  transcriptional regulatory network. The
                  probabilistic nature of the model also means that we
                  can associate credibility intervals to our estimates
                  of the activities. We demonstrate our model on two
                  yeast data sets. In both cases the network structure
                  was obtained using Chromatine Immunoprecipitation
                  data. We show how predictions from our model are
                  consistent with the underlying biology and offer
                  novel quantitative insights into the regulatory
                  structure of the yeast cell.\\\\ {\bf Availability:}
                  MATLAB code is available from
                  \url{http://umber.sbs.man.ac.uk/resources/puma}.},
  group =	 {gene networks,shefml,puma},
}

@InProceedings{Lawrence:transcriptionalGP06,
  author =	 {Neil D. Lawrence and Guido Sanguinetti and Magnus
                  Rattray},
  title =	 {Modelling transcriptional regulation using
                  {G}aussian Processes},
  crossref =	 {Schoelkopf:nips06},
  pages =	 {785--792},
  linksoftware = softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpsim.pdf},
  linkpsgz =	 shefftp # {gpsim.ps.gz},
  abstract =	 {Modelling the dynamics of transcriptional processes
                  in the cell requires the knowledge of a number of
                  key biological quantities. While some of them are
                  relatively easy to measure, such as mRNA decay rates
                  and mRNA abundance levels, it is still very hard to
                  measure the active concentration levels of the
                  transcription factor proteins that drive the process
                  and the sensitivity of target genes to these
                  concentrations. In this paper we show how these
                  quantities for a given transcription factor can be
                  inferred from gene expression levels of a set of
                  known target genes. We treat the protein
                  concentration as a latent function with a Gaussian
                  Process prior, and include the sensitivities, mRNA
                  decay rates and baseline expression levels as
                  hyperparameters. We apply this procedure to a human
                  leukemia dataset, focusing on the tumour repressor
                  p53 and obtaining results in good accordance with
                  recent biological studies.},
  group =	 {gene networks,shefml,puma},
  errata1 =	 {Equation after equation (5): the square outside the
                  $\exp(\gamma_k)$ term should be inside the
                  bracket. Same applies to equation after equation
                  (6)},
  errataCredit1 ={Pei Gao and David Luengo},
  errata2 =	 {Equation after equation (5): there is a missing $t$
                  on the second line of the equation after $D_j$.},
  errataCredit2 ={Pei Gao},
  errata3 =	 {Equation (10): There is a missing log on the left
                  hand side of the equation.},
  errataCredit3 ={Pei Gao},
  errata4 =	 {Equation (10): The sign before log(\sigma_{ji}^2)
                  should be positive, not negative.},
  errataCredit4 ={Pei Gao},
  errata5 =	 {Equation (7): We missed the mean function which
                  should be subtracted from the genes observations,
                  $\mathbf{x}$, to get the posterior mean
                  prediction. It was also misimplemented in the
                  original code, but since everything needs to be
                  offset to fit the earlier results we didn't
                  notice. It is correct in the later journal paper
                  \cite{Gao:latent08}},
  errataCredit5 ={Pei Gao and James Anderson}
}

@Article{Milo:probabilistic03,
  author =	 {Marta Milo and Alireza Fazeli and Mahesan Niranjan
                  and Neil D. Lawrence},
  title =	 {A Probabilistic Model for the Extraction of
                  Expression Levels from Oligonucleotide Arrays},
  journal =	 {Biochemical Transations},
  year =	 2003,
  month = 12,
  volume =	 31,
  number =	 6,
  pages =	 {1510--1512},
  abstract =	 {In this work we present a probabilistic model to
                  estimate summaries of Affymetrix GeneChip probe
                  level data. Comparisons with two different models
                  were made both on a publicly available dataset and
                  on a study performed in our laboratory, showing that
                  our model performs better for consistency of fold
                  change.},
  group =	 {shefml,mig,puma},
  linkpdf =	 shefftp # {probabilisticOligo.pdf}
}

@TechReport{Lawrence:gplvmtut06,
  author =	 {Neil D. Lawrence},
  title =	 {The {G}aussian Process Latent Variable Model},
  institution =	 sheftech,
  year =	 2006,
  number =	 {CS-06-03},
  abstract =	 {The Gaussian process latent variable model (GP-LVM)
                  is a recently proposed probabilistic approach to
                  obtaining a reduced dimension representation of a
                  data set. In this tutorial we motivate and describe
                  the GP-LVM, giving reviews of the model itself and
                  some of the concepts behind it.},
  linkpdf =	 shefftp # {gplvmTutorial.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  group =	 {shefml,gplvm,motion,dimensional reduction}
}

@Poster{Lawrence:varoptexit07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Variational Optimisation by Marginal Matching},
  venue =  	 {NIPS 07 Workshop on Approximate Bayesian Inference in Continuous/Hybrid Models, Whistler, Canada},
  linkpdf =	 shefftp # {epvarPoster.pdf},
  label1 =	 {Spotlight},
  link1 =	 shefftp # {epvarSpotlight.pdf},
  label2 =	 {Spotlight Video},
  link2 =	 {http://videolectures.net/abi07_lawrence_vap/snippet/},
  year =  	 2007,
  month =  	 12,
  day =  	 7,
  group =  	 {ep}
}


@Talk{Lawrence:rss16b,
  author = {Neil D. Lawrence},
  title = {Computational Perspectives: Fairness and Awareness in the Analysis of Data},
  abstract = {What is data science? An new name for something old perhaps. Nevertheless there is something new happening. Data is being acquired in ways that coudl never have been envisaged 100 years ago. This is presenting new challenges, and ones that no single field is equipped to face. As well as the need for new methodologies and theoretical underpinnings, modern data processing is having a direct effect on our citizens in real time. In this talk I'll suggest that data science provides a banner under which the computational and statistical sciences can unite to provide an unified response.},
  reveal = {2016-10-27-computational-perspectives-fairness-and-awareness-in-the-analysis-of-data.slides.html},
  year = 2016,
  month = 10,
  day = 27,
  venue = {Fisher Trust, Royal Society and London Mathematical Society Joint Meeting}
  }

@Talk{Lawrence:rss16a,
  author = {Neil D. Lawrence},
  title = {Data Science: Where Computation and Statistics Meet?},
  abstract = {What is data science? An new name for something old perhaps. Nevertheless there is something new happening. Data is being acquired in ways that could never have been envisaged 100 years ago. This is presenting new challenges, and ones that no single field is equipped to face. In this talk we will focus on three separate challenges for data science: 1. Paradoxes of the Data Society, 2. Quantifying the Value of Data, 3. Privacy, loss of control, marginalization. Each of these challenges has particular implications for data science and the interface between computation and statistics.  By addressing these challenges now we can ensure that the pitfalls of the data driven society are overcome allowing to reap the benefits.},
  reveal = {2016-09-06-data-science-where-computation-and-statistics-meet-.slides.html},
  url = {https://events.rss.org.uk/rss/frontend/reg/titem.csp?pageID=2373&eventID=1&eventID=1},
  year = 2016,
  month = 9,
  day = 6,
  venue = {Royal Statistical Society Conference, Manchester}
  }

@Talk{Lawrence:mlss16I,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to Gaussian Processes},
  abstract =     {},
  venue =  	 {MLSS, Cadiz},
  pdf =	 {gp_mlss16.pdf},
  year =  	 2016,
  month =  	 5,
  day =  	 12
}
@Talk{Lawrence:mlss16II,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to Gaussian Processes II},
  abstract =     {},
  venue =  	 {MLSS, Cadiz},
  pdf =	 {gp_mlss16.pdf},
  youtube = {xeP5Sh5VMoM},
  year =  	 2016,
  month =  	 5,
  day =  	 13
}

@Talk{Lawrence:mlss15bI,
  author =  	 {Neil D. Lawrence},
  title =        {{G}aussian Processes (Part I)},
  abstract =     {},
  venue =  	 {MLSS, TÃ¼bingen},
  linkpdf =	 shefhttp # {gp_mlss15b.pdf},
  year =  	 2015,
  youtube =      {S9RbSCpy_pg},
  month =  	 7,
  day =  	 16
}
@Talk{Lawrence:mlss15bII,
  author =  	 {Neil D. Lawrence},
  title =        {{G}aussian Processes (Part II)},
  abstract =     {},
  venue =  	 {MLSS, TÃ¼bingen},
  linkpdf =	 shefhttp # {gp_mlss15b.pdf},
  year =  	 2015,
  youtube =      {MxeQIKGEXb8},
  month =  	 7,
  day =  	 17
}
@Talk{Lawrence:mlss15bIII,
  author =  	 {Neil D. Lawrence},
  title =        {{G}aussian Processes (Part III)},
  abstract =     {},
  venue =  	 {MLSS, TÃ¼bingen},
  linkpdf =	 shefhttp # {gp_mlss15b.pdf},
  year =  	 2015,
  youtube =      {Ead4TivIOmU},
  month =  	 7,
  day =  	 18
}

@Talk{Lawrence:harvard15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {},
  venue =  	 {HIPS Group, SEAS, Harvard University},
  linkpdf =	 shefhttp # {deepgp_harvard15.pdf},
  year =  	 2015,
  month =  	 8,
  day =  	 {20}
}


@Talk{Lawrence:msrne15,
  author =  	 {Neil D. Lawrence},
  title =        {Personalized Health with {G}aussian Processes},
  abstract =     {Modern data connectivity gives us different views of the patient which need to be unified for truly personalized health care. I'll give an personal perspective on the type of methodological and social challenges we expect to arise in this this domain and motivate Gaussian process models as one approach to dealing with the explosion of data.},
  venue =  	 {Microsoft Research, New England},
  linkpdf =	 shefhttp # {personalized_msne15.pdf},
  year =  	 2015,
  month =  	 8,
  day =  	 {19}
}


@Talk{Lawrence:mlss15bc,
  author =  	 {Neil D. Lawrence},
  title =        {Latent Force Models: Bridging the Divide between Mechanistic and Data Modelling Paradigms},
  OPTabstract =     {},
  venue =  	 {MPI for Intelligent Systems, Stuttgart},
  linkpdf =	 shefhttp # {lfm_stuttgart15.pdf},
  year =  	 2015,
  month =  	 7,
  day =  	 {21}
}

@Talk{Lawrence:largeicml15,
  author =  	 {Neil D. Lawrence},
  title =        {Large Scale Learning in {G}aussian Processes},
  abstract =     {Gaussian process models view the kernel matrix as representing the covariance between data points. In a Gaussian process, the RKHS function is a mean of a posterior distribution over possible functions. Gaussian processes sustain uncertainty around this means and this leads to a posterior *covariance* function (or kernel) associated with the process. A complication for large scale Gaussian process models is the need to sustain the estimate for this covariance function. In this talk we'll review how this can be done probabilistically through a variational approach we know as 'variational compression'.
},
  venue =  	 {Large-Scale Kernel Learning Workshop @ICML2015},
  linkpdf =	 shefhttp # {parametric_icmllskw15.pdf},
  OPTipynb =        shefnbrepo_nohttp # {lab_classes/gprs/Low%20Rank%20Gaussian%20Processes.ipynb},
  year =  	 2015,
  month =  	 7,
  day =  	 {11}
}
@Talk{Welling:deeppanel15,
  author = {Max Welling and Demis Hassabis and Yoshua Bengio and Kevin Murphy and Yann LeCun and JÃ¼rgen Schmidhuber},
  title = {Panel Discussion},
  venue = {ICML Deep Learning Worshop, Lille, France},
  youtube = {EiStan9i8vA},
  year = 2015,
  month = 7,
  day = 11
}
@Talk{Lawrence:deepicml15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {Deep Learning Workshop @ICML2015},
  linkpdf =	 shefhttp # {deepgp_icmldeep15.pdf},
  OPTlinkipynb =    shefmlrepo_nohttp # {/deepGPy/blob/master/Nested%20Deep%20GPs.ipynb},
  youtube = {YHwnzd0i1XU},
  year =  	 2015,
  month =  	 7,
  day =  	 {11}
}

@Talk{Lawrence:edinburgh15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {2nd Deep Learning Workshop, Edinburgh},
  linkpdf =	 shefhttp # {deepgp_edinburgh15.pdf},
  linkipynb =	 shefmlrepo_nohttp # {/deepGPy/blob/master/Nested%20Deep%20GPs.ipynb},
  year =  	 2015,
  month =  	 6,
  day =  	 {9}
}

@Talk{Lawrence:nyu15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {Computer Science Colloquium, NYU},
  linkpdf =	 shefhttp # {deepgp_nyu15.pdf},
  linkipynb =	 shefmlrepo_nohttp # {/deepGPy/blob/master/Nested%20Deep%20GPs.ipynb},
  year =  	 2015,
  month =  	 5,
  day =  	 {11}
}


@Talk{Lawrence:kth15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {KTH Royal Institute of Technology, Sweden},
  linkpdf =	 shefhttp # {deepgp_kth15.pdf},
  OPTlinkipynb =        shefnbrepo_nohttp # {lab_classes/gprs/Low%20Rank%20Gaussian%20Processes.ipynb},
  year =  	 2015,
  month =  	 4,
  day =  	 {30}
}

@Talk{Lawrence:linkoping15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {IDA Machine Learning Seminars, Linkoping, Sweden},
  linkpdf =	 shefhttp # {deepgp_linkoping15.pdf},
  OPTlinkipynb =        shefnbrepo_nohttp # {lab_classes/gprs/Low%20Rank%20Gaussian%20Processes.ipynb},
  year =  	 2015,
  month =  	 4,
  day =  	 {29}
}

@Talk{Lawrence:mascotnum15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to handle the intractabilities. In this talk we review the variational bounds that are used under the framework of variational compression and give some initial results of deep Gaussian process models.},
  venue =  	 {Mascot Num 2015, St Etienne, France},
  linkpdf =	 shefhttp # {deepgp_mascotnum15.pdf},
  OPTlinkipynb =        shefnbrepo_nohttp # {lab_classes/gprs/Low%20Rank%20Gaussian%20Processes.ipynb},
  year =  	 2015,
  month =  	 4,
  day =  	 {8}
}




@Talk{Lawrence:mpi15,
  author =  	 {Neil D. Lawrence},
  title =        {Modelling in the Context of Massively Missing Data},
  abstract =     {In the age of large streaming data it seems appropriate to revisit the foundations of what we think of as data modelling. In this talk I'll argue that traditional statistical approaches based on parametric models and i.i.d. assumptions are inappropriate for the type of large scale machine learning we need to do in the age of massive streaming data sets. Particularly when we realise that regardless of the size of data we have, it pales in comparison to the data we could have. This is the domain of \emph{massively missing data}. I'll be arguing for flexible non-parametric models as the answer. This presents a particular challenge, non parametric models require data storage of the entire data set, which presents problems for massive, streaming data. I will present a potential solution, but perhaps end with more questions than we started with.},
  venue =  	 {Max Planck Institute, TÃ¼bingen},
  linkpdf =	 shefhttp # {missingdata_tuebingen15.pdf},
  linkipynb =        shefnbrepo_nohttp # {lab_classes/gprs/Low%20Rank%20Gaussian%20Processes.ipynb},
  year =  	 2015,
  month =  	 3,
  day =  	 18
}


@Misc{Lawrence:informationbarons15,
  author =	 {Neil D. Lawrence},
  title =	 {The information barons threaten our autonomy and our privacy},
  howpublished = {The Guardian Media & Tech Network},
  year =	 {2015},
  url =	 {https://www.theguardian.com/media-network/2015/nov/16/information-barons-threaten-autonomy-privacy-online}
}


@Misc{Lawrence:digitaloligarchy15,
  author =	 {Neil D. Lawrence},
  title =	 {Beware the rise of the digital oligarchy},
  howpublished = {The Guardian Media & Tech Network},
  year =	 {2015},
  url =	 {https://www.theguardian.com/media-network/2015/mar/05/digital-oligarchy-algorithms-personal-data}
}

@Misc{Lawrence:trusts16,
  key =		 {},
  author =	 {Neil D. Lawrence},
  title =	 {Data Trusts Could Allay our Privacy Fears},
  howpublished = {The Guardian Media & Tech Network},
  url =	 {https://www.theguardian.com/media-network/2016/jun/03/data-trusts-privacy-fears-feudalism-democracy},
  year =	 {2016},
  OPTnote = 	 {},
  OPTannote = 	 {},
  OPTpubmedid =  {},
  OPTdoi = 	 {},
  OPTlinkpdf = 	 {},
  OPTlinkps = 	 {},
  OPTlinkpsgz =  {},
  OPTlinksoftware = {},
  OPTabstract =  {},
  OPTgroup = 	 {}
}

@Talk{Lawrence:datafarm15b,
  author = {Neil D. Lawrence},
  title =        {The Data Farm},
  abstract =     {Like Hansel and Gretelâs breadcrumbs into the forest we leave a data trail of data-crumbs wherever we go: social networks, mobile phones, hospital visits, credit cards and loyalty cards. Our every move is being watched! The data-crumbs are seeds of information but what results from them... is it a jungle with dangers lurking or a productive farmyard? And if our data is being farmed, where does all the produce go?\\\\This edition of the talk was given to an age group between 11 and 13.},
  venue =  	 {Birley Community College, Birley, Sheffield},
  linkpdf =	 shefhttp # {datafarm_schools15.pdf},
  year =  	 2015,
  month =  	 3,
  day =  	 13
}
@Talk{Lawrence:datafarm15a,
  author = {Neil D. Lawrence},
  title =        {The Data Farm},
  abstract =     {Like Hansel and Gretelâs breadcrumbs into the forest we leave a data trail of data-crumbs wherever we go: social networks, mobile phones, hospital visits, credit cards and loyalty cards. Our every move is being watched! The data-crumbs are seeds of information but what results from them... is it a jungle with dangers lurking or a productive farmyard? And if our data is being farmed, where does all the produce go?\\\\This edition of the talk was given to an age group between 8 and 10.},
  venue =  	 {Westwoodside Primary School, Nether Gate, Westwoodside, Doncaster},
  linkpdf =	 shefhttp # {datafarm_schools15.pdf},
  year =  	 2015,
  month =  	 3,
  day =  	 13
}

@Talk{Lawrence:nottingham15,
  author =  	 {Neil D. Lawrence},
  title =        {Data Science: A New Field or Just a Rebadging Exercise?},
  abstract =     {Scientific fields don't necessarily emerge because fundamental new knowledge is being generated, but often because a shift in the key questions that are facing us, and the tools that we have to answer them. The current information revolution is causing us to reassess our approach to data. Our mathematical and computational toolsets are co-evolving. The potential of very large interconnected data is placing urgent demands on our methodologies. In this talk, inspired by these challenges, I will give a personal perspective on what this means for those of us at the interface of Computer Science/Mathematics and Statistics. I'll attempt to do this not only in the context of modelling and analysis, but also in the context of how we deploy our conclusions for the benefit of wider society. Many of our current suite of methodologies were motivated by different needs, and I'll argue that it may now be time to return to the fundamental ideas from where these methodologies were inspired, but with a contemporary slant on the nature of data. My own perspective is that if what I describe *is* data science, then it does not stand as a field alone, but it represents a new and pressing set of questions that bridge the computational and mathematical sciences. Regardless of its phylogeny, exploring this interface through these questions will be mutually beneficial.},
  venue =  	 {School of Mathematical Sciences, University of Nottingham},
  linkpdf =	 shefhttp # {datascience_nottingham15.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2015,
  month =  	 03,
  day =  	 12
}

@Talk{Lawrence:what15,
  author =   	 {Neil D. Lawrence},
  title =  	 {The Rise of the Algorithm: What is the AI that we have created?},
  abstract =     {There have been fears voiced by Elon Musk and Stephen Hawking about the direction of artificial intelligent research. They worry about the creation of a sentient AI, one that might outwit us. However, the nature of the AI we have actually created is a long way distant from this. In this talk we will try and relate our models of artificial intelligence to models that have been proposed for the way humans think. The AI that Hawking and Musk fear is not yet here, but is the AI we have actually developed more or less disturbing than the vision they project?},
  venue = {Sheffield Festival of Science and Engineering},
  year = 2016,
  month = 3,
  day = 17
}
@Talk{Lawrence:notre15,
  author =   	 {Neil D. Lawrence},
  title =  	 {The Rise of the Algorithm: What is the AI that we have created?},
  abstract =     {There have been fears voiced by Elon Musk and Stephen Hawking about the direction of artificial intelligent research. They worry about the creation of a sentient AI, one that might outwit us. However, the nature of the AI we have actually created is a long way distant from this. In this talk we will try and relate our models of artificial intelligence to models that have been proposed for the way humans think. The AI that Hawking and Musk fear is not yet here, but is the AI we have actually developed more or less disturbing than the vision they project?},
  venue = {Notre Dame High School, Sheffield},
  ipynb = {2016-03-17_what-kind-of-ai.ipynb},
  reveal = {2016-03-17_what-kind-of-ai.slides.html},
  year = 2016,
  month = 3,
  day = 17
}

@Talk{Lawrence:birley15,
  author =   	 {Neil D. Lawrence},
  title =  	 {The Rise of the Algorithm: What is the AI that we have created?},
  abstract =     {There have been fears voiced by Elon Musk and Stephen Hawking about the direction of artificial intelligent research. They worry about the creation of a sentient AI, one that might outwit us. However, the nature of the AI we have actually created is a long way distant from this. In this talk we will try and relate our models of artificial intelligence to models that have been proposed for the way humans think. The AI that Hawking and Musk fear is not yet here, but is the AI we have actually developed more or less disturbing than the vision they project?},
  venue = {Birley Community College, Sheffield},
  ipynb = {2016-03-10_what-kind-of-ai.ipynb},
  reveal = {2016-03-10_what-kind-of-ai.slides.html},
  year = 2016,
  month = 3,
  day = 10
}

@Talk{Lawrence:impact15,
  author =   	 {Neil D. Lawrence},
  title =  	 {The Digital Oligarchy: Information, Knowledge and the Internet Era},
  venue =      {Right First Time, ControlPoint Event},
abstract =     {The data revolution is among us and the technical press is filled with stories of big data and artificial intelligence. What is driving this progress? In this talk we will argue that collection of data on its own is of little utility, it is interconnection of data that allows information to become knowledge. Businesses need to place data at the core of what they do to benefit from these techniques. 

The talk will be grounded in academic ideas of what information, knowledge and data are. But these concepts have practical utility that can influence decision making on where data sits within an organisation.},
  ipynb = {2015-10-07_controlPointTalk.ipynb},
  reveal = {2015-10-07_controlPointTalk.slides.html},
  year = 2015,
  month = 10,
  day = 8
}
@Talk{Lawrence:imperial15b,
  author =  	 {Neil D. Lawrence},
  title =  	 {Machine Learning Tutorial: Probabilistic Dimensionality Reduction {II}},
  venue =  	 {Imperial College, U.K.},
  pdf =	 {probdimII_imperial15.pdf},
  year =  	 2015,
  month =  	 10,
  day =  	 21,
  OPTlink1 = {http://panopto.imperial.ac.uk/Panopto/Pages/Viewer.aspx?id=06941fe2-13be-45f5-9f57-52bfffed0960},
  OPTlabel1 = {Video},
  abstract =   	 {In the second part of this tutorial we will develop non linear approaches to dimensionality reduction from the probabilistic perspective. Firstly we will briefly review a probabilistic perspectives on spectral approaches, and then we will build on the non-linear approaches we derived using Gaussian processes in the first part of the tutorial.}
}

@Talk{Lawrence:imperial15,
  author =  	 {Neil D. Lawrence},
  title =  	 {Machine Learning Tutorial: Probabilistic Dimensionality Reduction},
  venue =  	 {Imperial College, U.K.},
  linkpdf =	 shefhttp # {probdim_imperial15.pdf},
  year =  	 2015,
  month =  	 3,
  day =  	 11,
  link1 = {http://panopto.imperial.ac.uk/Panopto/Pages/Viewer.aspx?id=06941fe2-13be-45f5-9f57-52bfffed0960},
  label1 = {Video},
  abstract =   	 {In this tutorial we will present probabilistic approaches to dimensionality reduction based on latent variable models. We will motivate dimensionality reduction and then start with principal component analysis and extend it to include non linear approaches to reducing the dimension of data.}
}

@Talk{Lawrence:fest15,
  author =  	 {Neil D. Lawrence},
  title =        {The Data Farm},
  abstract =     {Like Hansel and Gretelâs breadcrumbs into the forest we leave a data trail of data-crumbs wherever we go: social networks, mobile phones, hospital visits, credit cards and loyalty cards. Our every move is being watched! The data-crumbs are seeds of information but what results from them... is it a jungle with dangers lurking or a productive farmyard? And if our data is being farmed, where does all the produce go?},
  venue =  	 {Sheffield Festival of Engineering and Science, Jessop West Exhibition Space, Jessop West, University of Sheffield, 1 Upper Hanover Street, Sheffield S3 7RA},
  linkpdf =	 shefhttp # {datafarm_science15.pdf},
  linkipynb =        shefnbrepo_nohttp # {science_week/The%20Data%20Farm.ipynb},
  year =  	 2015,
  youtube = {s0EBVDAG2cg},
  month =  	 3,
  day =  	 5
}
@Talk{Lawrence:mlss15,
  author =  	 {Neil D. Lawrence},
  title =        {Introduction to {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {MLSS, Sydney},
  linkpdf =	 shefhttp # {gp_mlss15.pdf},
  OPTipynb =        shefnbrepo_nohttp # {lab_classes/gprs/Low%20Rank%20Gaussian%20Processes.ipynb},
  youtube = {pmeAgona_to},
  year =  	 2015,
  month =  	 2,
  day =  	 {21--22}
}




@Talk{Lawrence:radiant15,
  author =  	 {Neil D. Lawrence},
  title =        {The {NIPS} Experiment},
  OPTabstract =     {},
  venue =  	 {RADIANT Meeting, University of Zurich, Switzerland},
  year =  	 2015,
  month =  	 1,
  abstract = {The peer review process can be difficult to navigate for newcomers. In this informal talk we will review the results of the NIPS experiment, an experiment on the repeatability of peer review conducted for the 2014 conference. We will try to keep the presentation information to ensure questions can be asked. With luck it will give more insight into the processes that a program committee goes through when selecting papers.},
  linkipynb =        shefhttp_nohttp # {nips_radiant15.ipynb},
  blog = {2014-12-16-the-nips-experiment.md},
  ipynb = {nips_radiant15.ipynb},
  reveal = {nips_radiant15.slides.html},
  day =  	 30
}

@Talk{Lawrence:iit15,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {Gaussian} Processes},
  abstract =     {In this talk we describe how deep neural networks can be modified to produce deep Gaussian process models. The framework of deep Gaussian processes allow for unsupervised learning, transfer learning, semi-supervised learning, multi-task learning and principled handling of different data types (count data, binary data, heavy tailed noise distributions). The main challenge is to solve these models efficiently for massive data sets. That challenge is in reach through a new class of variational approximations known as variational compression. The underlying variational bounds are very similar to the objective functions for deep neural networks, giving the promise of efficient approaches to deep learning that are constructed from components with very well understood analytical properties.},
  venue =  	 {Instituto Italiano de Tecnologia, Genova, Italy},
  linkpdf =	 shefhttp # {deepgp_iit15.pdf},
  year =  	 2015,
  month =  	 1,
  day =  	 23
}

@Talk{Lawrence:warwick14,
  author =  	 {Neil D. Lawrence},
  title =        {Data Science: A New Field or Just a Rebadging Exercise?},
  abstract =     {Scientific fields don't necessarily emerge because fundamental new knowledge is being generated, but often because a shift in the key questions that are facing us, and the tools that we have to answer them. The current information revolution is causing us to reassess our approach to data. Our mathematical and computational toolsets are co-evolving. The potential of very large interconnected data is placing urgent demands on our methodologies. In this talk, inspired by these challenges, I will give a personal perspective on what this means for those of us at the interface of Computer Science/Mathematics and Statistics. I'll attempt to do this not only in the context of modelling and analysis, but also in the context of how we deploy our conclusions for the benefit of wider society. Many of our current suite of methodologies were motivated by different needs, and I'll argue that it may now be time to return to the fundamental ideas from where these methodologies were inspired, but with a contemporary slant on the nature of data. My own perspective is that if what I describe *is* data science, then it does not stand as a field alone, but it represents a new and pressing set of questions that bridge the computational and mathematical sciences. Regardless of its phylogeny, exploring this interface through these questions will be mutually beneficial.},
  venue =  	 {Department of Statistics, University of Warwick},
  linkpdf =	 shefhttp # {datascience_warwick14.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 11,
  day =  	 26
}
@Talk{Lawrence:rss14,
  author =  	 {Neil D. Lawrence},
  title =        {Statistical Computing: Python},
  OPTabstract =     {},
  venue =  	 {Royal Statistical Society, London, U.K.},
  OPTlinkpdf =	 shefhttp # {deepgp_ucl14b.pdf},
  OPTyoutube = {ulahro6DjyY},
  label1 =	 {Event Detail},
  link1 =	 {http://www.statslife.org.uk/events/eventdetail/284/-/-},
  label2 =	 {Slides (Regression, IPython Notebook)},
  link2 =	 {http://nbviewer.ipython.org/github/lawrennd/statslang/blob/master/python/statslang.ipynb},
  label3 =	 {Slides (Monte Carlo, IPython Notebook, written by Mike Croucher)},
  link3 =	 {http://nbviewer.ipython.org/github/lawrennd/MultiLangMonteCarlo/blob/master/MultiLangStats.ipynb},
  year =  	 2014,
  month =  	 11,
  day =  	 21
}
@Talk{Lawrence:ucl14c,
  author =  	 {Neil D. Lawrence},
  title =        {Approximate Inference in Deep {GP}s},
  abstract =     {In this talk we will review deep Gaussian process models and relate them to neural network models. We will then consider the details of how variational inference may be performed in these models. The approach is centred on "variational compression", an approach to variational inference that compresses information into an augmented variable space.

The aim of the deep Gaussian process framework is to enable probabilistic learning of multi-modal data. We will therefore end by highlighting directions for future research and discussing application of these models in domains such as personalised health.},
  venue =  	 {Gatsby Computational Neuroscience Unit, University College London, U.K.},
  linkpdf =	 shefhttp # {deepgp_ucl14b.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 10,
  day =  	 23
}

@Talk{Lawrence:ucl14b,
  author =  	 {Neil D. Lawrence},
  title =        {Deep {G}aussian Processes},
  abstract =     {In this talk we describe how deep neural networks can be
                 modified to produce deep Gaussian process models. The
                 framework of deep Gaussian processes allow for
                 unsupervised learning, transfer learning,
                 semi-supervised learning, multi-task learning and
                 principled handling of different data types (count
                 data, binary data, heavy tailed noise
                 distributions). The main challenge is to solve these
                 models efficiently for massive data sets. That
                 challenge is in reach through a new class of
                 variational approximations known as variational
                 compression. The underlying variational bounds are
                 very similar to the objective functions for deep
                 neural networks, giving the promise of efficient
                 approaches to deep learning that are constructed from
                 components with very well understood analytical
                 properties.},
  venue =  	 {UCL-Duke University Workshop on Sensing and Analysis of High-Dimensional Data},
  linkpdf =	 shefhttp # {deepgp_ucl14.pdf},
  videolectures = {sahd2014_lawrence_gaussian_processes},
  year =  	 2014,
  month =  	 9,
  day =  	 4
}
@Talk{Lawrence:uclid14,
  author = {Neil D. Lawrence},
  title = {Big Data and Open Data Science},
  abstract = {In this talk we will focus on the challenges that are arising through big data and focussing on potential solutions, both from a methodological side, but also in terms of the way that statistics and computer science need to respond to the challenges culturally.},
  venue =  	 {UCLID Workshop, University of Lancaster, UK},
  linkpdf =	 shefhttp # {opendata_uclid14.pdf},
  OPTyoutube = {ulahro6DjyY},
  blog = {2014-07-01-open-data-science.md},
  year =  	 2014,
  month =  	 7,
  day =  	 2
}
@Talk{Lawrence:edinburgh14,
  author =  	 {Neil D. Lawrence},
  title =        {Flexible Parametric Representations of Non Parametric Models},
  abstract =     {In the age of large streaming data it seems appropriate to revisit the foundations of what we think of as data modelling. In this talk I'll argue that traditional statistical approaches based on parametric models and i.i.d. assumptions are inappropriate for the type of large scale machine learning we need to do in the age of massive streaming data sets. I'll be arguing for flexible non-parametric models as the answer. This presents a particular challenge, non parametric models require data storage of the entire data set, which presents problems for massive, streaming data. I'll argue that recently proposed variational approximations allow us to retain the advantages of both non-parametric and parametric models within a consistent framework that performs an optimal compression of our data from an information gain perspective.},
  venue =  	 {Informatics Forum, University of Edinburgh, UK},
  linkpdf =	 shefhttp # {flexible_edinburgh14.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 5,
  day =  	 19
}

@Talk{Lawrence:ebi14a,
  author =  	 {Neil D. Lawrence},
  title =        {Gaussian Processes for Dynamic Modelling},
  venue =  	 {The Systems Biology Modelling Cycle, EBI, Hinxton, UK},
  linkpdf =	 shefhttp # {gp_ebi.pdf},
  linkipynb =	 shefnbrepo_nohttp # {compbio/TomancakDataWithGPy.ipynb},
  year =  	 2014,
  month =  	 5,
  day =  	 13
}

@Talk{Lawrence:ebi14b,
  author =  	 {Neil D. Lawrence},
  title =        {Visualizing Biological Data with {G}aussian Processes},
  venue =  	 {The Systems Biology Modelling Cycle, EBI, Hinxton, UK},
  linkpdf =	 shefhttp # {gplvm_ebi14.pdf},
  linkipynb =        shefnbrepo_nohttp # {compbio/SingleCellDataWithGPy.ipynb},
  year =  	 2014,
  month =  	 5,
  day =  	 13
}
@Talk{Lawrence:mlss14,
  author =  	 {Neil D. Lawrence},
  title =        {What is Machine Learning? A Probabilistic Perspective (Part I)},
  venue =  	 {MLSS, Reykjavik, Iceland},
  linkpdf =	 shefhttp # {what_mlss14.pdf},
  youtube = {rcZHO2Lyd8Q},
  year =  	 2014,
  month =  	 4,
  day =  	 26
}
@Talk{Lawrence:mlss14b,
  author =  	 {Neil D. Lawrence},
  title =        {What is Machine Learning? A Probabilistic Perspective (Part II)},
  venue =  	 {MLSS, Reykjavik, Iceland},
  linkpdf =	 shefhttp # {what_mlss14.pdf},
  youtube = {4FAZdCcj3MA},
  year =  	 2014,
  month =  	 4,
  day =  	 26
}


@Talk{Lawrence:curie14,
  author =  	 {Neil D. Lawrence},
  title =        {Applications of {G}aussian Processes in Computational Biology},
  abstract =     {In this talk we will give a brief overview of Gaussian processes and a quick review of how they can be applied to solve questions in computational biology. In particular we will show how we can construct covariance functions to solve simple tasks (like differential expression) or more complex tasks (like unpicking regulatory networks).},
  venue =  	 {Institute Curie, Paris, France},
  linkpdf =	 shefhttp # {gp_curie14.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 4,
  day =  	 3
}

@Talk{Lawrence:smile14,
  author =  	 {Neil D. Lawrence},
  title =        {Flexible Parametric Representations of Non Parametric Models},
  abstract =     {In the age of large streaming data it seems appropriate to revisit the foundations of what we think of as data modelling. In this talk I'll argue that traditional statistical approaches based on parametric models and i.i.d. assumptions are inappropriate for the type of large scale machine learning we need to do in the age of massive streaming data sets. I'll be arguing for flexible non-parametric models as the answer. This presents a particular challenge, non parametric models require data storage of the entire data set, which presents problems for massive, streaming data. I'll argue that recently proposed variational approximations allow us to retain the advantages of both non-parametric and parametric models within a consistent framework that performs an optimal compression of our data from an information gain perspective.},
  venue =  	 {Statistical Machine Learning in Paris, France},
  linkpdf =	 shefhttp # {flexible_smile14.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 4,
  day =  	 3
}

@Talk{Lawrence:facebook14,
  author =  	 {Neil D. Lawrence},
  title =  	 {Modelling with Massively Missing Data},
  abstract =     {Supervised deep learning techniques now dominate in terms of performance for complex classification tasks such as ImageNet. For these, the set of inputs (features) and targets (labels) are typically well defined in advance. However, for many tasks in artificial intelligence the questions that need to be answered evolve, alongside the features that we can acquire. For example, imagine we wish to infer the health status of individuals by building population scale models based on clinical data. For most people in the population most of the data will be missing because clinical tests are not applied to patients as a matter of course. Indeed, some of the features we may wish to use in our model may not even exist when our model is first designed (e.g. emerging clinical tests and treatments). We refer to this scenario as 'massively missing data'. It is a scenario humans are faced with every day. Almost all of the time we are missing almost all of the data. And yet we have no difficulty assimilating disparate pieces of information from a wide range of sources to draw inferences about our world. Implementing machine learning systems that can replicate this characteristic requires model architectures that can be adapted at 'runtime' as the data evolves, we don't want to be limited by decisions made at 'design time' when perhaps a more limited feature set existed. This poses particular challenges that we will address in this talk.},
  venue =  	 {Facebook, Menlo Park, CA},
  linkpdf =	 shefhttp # {missing_facebook14.pdf},
  year =  	 2014,
  month =  	 3,
  day =  	 20
}

@Talk{Lawrence:manizales14,
  author =  	 {Neil D. Lawrence},
  title =  	 {Personalized Health with {G}aussian Processes},
  abstract =     {Modern data connectivity gives us different views of the patient which need to be unified for truly personalized health care. I'll give an personal perspective on the type of methodological challenges we expect to arise in this this domain and motivate Gaussian process models as one approach to dealing with the explosion of data.},
  venue =  	 {Universidad Nacional de Colombia, Sede Manizales, Colombia},
  linkpdf =	 shefhttp # {personalized_health_manizales14.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 2,
  day =  	 19
}

@Talk{Lawrence:ucl14,
  author =  	 {Neil D. Lawrence},
  title =        {Flexible Parametric Representations of Non Parametric Models},
  abstract =     {In the age of large streaming data it seems appropriate to revisit the foundations of what we think of as data modelling. In this talk I'll argue that traditional statistical approaches based on parametric models and i.i.d. assumptions are inappropriate for the type of large scale machine learning we need to do in the age of massive streaming data sets. I'll be arguing for flexible non-parametric models as the answer. This presents a particular challenge, non parametric models require data storage of the entire data set, which presents problems for massive, streaming data. I'll argue that recently proposed variational approximations allow us to retain the advantages of both non-parametric and parametric models within a consistent framework that performs an optimal compression of our data from an information gain perspective.},
  venue =  	 {Gatsby Computational Neuroscience Unit, University College London, U.K.},
  linkpdf =	 shefhttp # {parametric_ucl14.pdf},
  OPTyoutube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 2,
  day =  	 26
}

@Talk{Lawrence:cued14,
  author =  	 {Neil D. Lawrence},
  title =  	 {New Perspectives on Variational Approximations in {G}aussian Processes: Modelling Data},
  abstract =     {In this talk I'll introduce new perspectives on variational approximations. Many of the ideas may be widely applicable, but we will try to instantiate them in the context of Gaussian process models.\\\\

Although the variational material itself is reasonably technical, I'll try and start the talk by making general statements about data modelling. Then, in an effort to make the talk seem coherent, I'll make claims that the technical material which follows was inspired by the wider perspective I've given. Of course in practice, the technical material really emerged across a number of years during discussions with many people, and the general perspective has been retrofitted. Still, I'll be giving the talk amongst friends, so no one will mind too much if the story doesn't really fit together, and in fact it might be a good trigger for discussion. Speaking of which, I'll be looking forward to lots of audience participation, and such participation may take the talk in previously unplanned directions.\\\\

The talk will be given without the use of electronic aids.},
  venue =  	 {University of Cambridge, Engineering Department},
  linkpdf =	 shefhttp # {modeling_things.pdf},
  youtube = {ulahro6DjyY},
  year =  	 2014,
  month =  	 1,
  day =  	 21
}

@Talk{Lawrence:oxford14,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {G}aussian {P}rocesses},
  abstract =     {In this talk we will introduce deep Gaussian process (GP) models. Deep GPs are a deep probabilistic model based on Gaussian process mappings. The data is modelled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GPLVM). We will motivate these models by considering applications in personalized health.\\\\

We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples. In the seminar we will briefly review dimensionality reduction via Gaussian processes, before showing how this framework can be extended to build deep models.},
  venue =  	 {Oxford University Statistics Department},
  linkpdf =	 shefhttp # {deep_oxford14.pdf},
  mp3 =	 shefhttp # {deep_oxford14.mp3},
  year =  	 2014,
  month =  	 2,
  day =  	 6
}


@Talk{Lawrence:leeds13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Unravelling the Big Data Revolution},
  abstract =     {Modern data connectivity gives us massive uncurated data sets which present enormous challenges for modelling and inference. I'll review where I think this is taking mathematics and speculate on the methodological and social challenges that this revolution will entail with some final reflections on how it might effect the teaching curriculum.},
  venue =  	 {Maths Department, Teaching Away Day, University of Leeds},
  linkpdf =	 shefhttp # {unravelling_leeds13.pdf},
  year =  	 2013,
  month =  	 12,
  day =  	 18
}
@Talk{Lawrence:necs13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Unravelling the Data Revolution with Machine Learning},
  abstract =     {Modern data connectivity gives us different views of the patient which need to be unified for truly personalized health care. I'll review where I think this is taking medicine and speculate on the methodological and social challenges that this revolution will entail.},
  venue =  	 {Newcastle, Edinburgh, Cambridge and Sheffield academic Departments of Respiratory Medicine Meeting, Whitely Hall, Sheffield},
  linkpdf =	 shefhttp # {unravelling_necs13.pdf},
  year =  	 2013,
  month =  	 11,
  day =  	 14
}
@Talk{Lawrence:leahurst13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Personalized Health with {G}aussian Processes},
  abstract =     {Modern data connectivity gives us different views of the patient which need to be unified for truly personalized health care. I'll give an personal perspective on the type of methodological challenges we expect to arise in this this domain and motivate Gaussian process models as one approach to dealing with the explosion of data.},
  venue =  	 {Disease Mapping Workshop, Leahurst},
  linkpdf =	 shefhttp # {personalized_health_leahurst13.pdf},
  year =  	 2013,
  month =  	 11,
  day =  	 4
}
@Talk{Lawrence:e4l13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep Health: Machine Learning for Personalized Medicine},
  abstract =     {I'll give an overview of the methodological challenges we see arising in personalized medicine. These are associated with the explosion of data giving us different views of the patient which need to be unified for truly personalized health care.},
  venue =  	 {E4L Away Day},
  linkpdf =	 shefftp # {deep_health_e4l.pdf},
  year =  	 2013,
  month =  	 10,
  day =  	 3
}
@Talk{Lawrence:mlpm13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Approaches for Computational Biology and Medicine},
  abstract =     {In this talk I'll discuss some of the challenges in personalized medicine and consider some of the implications for machine learning models. I'll introduce the probabilistic approach to machine learning, with a particular focus on Gaussian models. Giving some examples of applications I'll discuss Bayesian approaches to regression modelling and lead into Gaussian process models.},
  venue =  	 {Machine Learning for Personalized Medicine Summer School},
  linkpdf =	 shefftp # {probabilistic_mlpm13.pdf},
  year =  	 2013,
  month =  	 9,
  day =  	 25
}

@Talk{Lawrence:msr13b,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Unifying Probabilistic Perspective on Spectral Approaches to Dimensionality Reduction},
  abstract =     {Spectral approaches to dimensionality reduction typically reduce the dimensionality of a data set through taking the eigenvectors of a Laplacian or a similarity matrix. Classical multidimensional scaling also makes use of the eigenvectors of a similarity matrix. In this talk we introduce a maximum entropy approach to designing this similarity matrix. The approach is closely related to maximum variance unfolding. Other spectral approaches such as locally linear embeddings and Laplacian eigenmaps also turn out to be closely related. Each method can be seen as a sparse Gaussian graphical model where correlations between data points (rather than across data features) are specified in the graph. This also suggests optimization via sparse inverse covariance techniques such as the graphical LASSO. The hope is that this unifying perspective will allow the relationships between these methods to be better understood and will also provide the groundwork for further research.},
  venue =  	 {Microsoft Research, Cambridge},
  linkpdf =	 shefftp # {spectral_msr13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 9,
  day =  	 5
}
@Talk{Lawrence:msr13a,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {Gaussian} Processes},
  abstract =     {In this talk we will introduce deep Gaussian process (GP) models. Deep
GPs are a deep belief network based on Gaussian process mappings. The
data is modeled as the output of a multivariate GP. The inputs to that
Gaussian process are then governed by another GP. A single layer model
is equivalent to a standard GP or the GP latent variable model
(GPLVM). We perform inference in the model by approximate variational
marginalization. This results in a strict lower bound on the marginal
likelihood of the model which we use for model selection (number of
layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient
descent for optimization. Our fully Bayesian treatment allows for the
application of deep models even when data is scarce. Model selection
by our variational bound shows that a five layer hierarchy is
justified even when modelling a digit data set containing only 150
examples. In the seminar we will briefly review dimensionality reduction
via Gaussian processes, before showing how this framework can be
extended to build deep models.},
  venue =  	 {Microsoft Research, Cambridge},
  linkpdf =	 shefftp # {deep_msr13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 9,
  day =  	 3
}

@Talk{Lawrence:ncaf13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {Gaussian} Processes},
  abstract =     {In this talk we will introduce deep Gaussian process (GP) models. Deep
GPs are a deep belief network based on Gaussian process mappings. The
data is modeled as the output of a multivariate GP. The inputs to that
Gaussian process are then governed by another GP. A single layer model
is equivalent to a standard GP or the GP latent variable model
(GPLVM). We perform inference in the model by approximate variational
marginalization. This results in a strict lower bound on the marginal
likelihood of the model which we use for model selection (number of
layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient
descent for optimization. Our fully Bayesian treatment allows for the
application of deep models even when data is scarce. Model selection
by our variational bound shows that a five layer hierarchy is
justified even when modelling a digit data set containing only 150
examples. In the seminar we will briefly review dimensionality reduction
via Gaussian processes, before showing how this framework can be
extended to build deep models.},
  venue =  	 {Natural Computing Applications Forum, University of Oxford},
  linkpdf =	 shefftp # {gplvm_ncaf13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 7,
  day =  	 4
}

@Talk{Lawrence:manchester13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep Health},
  OPTabstract =     {},
  venue =  	 {Manchester and Sheffield Machine Learning Meetings},
  linkpdf =	 shefftp # {deep_health_manchester13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 6,
  day =  	 17
}

@Talk{Lawrence:lfmIntro13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models: Introduction},
  OPTabstract =     {},
  venue =  	 {Latent Force Model Workshop, Sheffield},
  linkpdf =	 shefftp # {gp_gpss13_session_lfm.pdf},
  youtube =	 {XK91ax4LBH4},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 6,
  day =  	 13
}
@Talk{Lawrence:gpwsThree14,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Variable Models with {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {Gaussian Process Winter School, Sheffield},
  linkpdf =	 shefftp # {gp_gpws14_session3.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  youtube = {pAZxfwo6efg},
  year =  	 2014,
  month =  	 1,
  day =  	 15
}
@Talk{Lawrence:gpwsTwo14,
  author =  	 {Neil D. Lawrence},
  title =  	 {Fitting Covariance and Multi-output {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {Gaussian Process Winter School, Sheffield},
  linkpdf =	 shefftp # {gp_gpws14_session2.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  youtube = {6ziKkc0nruc},
  year =  	 2014,
  month =  	 1,
  day =  	 14
}
@Talk{Lawrence:gpwsOne14,
  author =  	 {Neil D. Lawrence},
  title =  	 {Introduction to {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {Gaussian Process Winter School, Sheffield},
  linkpdf =	 shefftp # {gp_gpws14_session3.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  youtube = {ewJ3AxKclOg},
  year =  	 2014,
  month =  	 1,
  day =  	 13
}
@Talk{Lawrence:gpssThree13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Unsupervised Learning with {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {Gaussian Process Summer School, Sheffield},
  linkpdf =	 shefftp # {gp_gpss13_session3.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 6,
  day =  	 12
}
@Talk{Lawrence:gpssTwo13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Multioutput {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {Gaussian Process Summer School, Sheffield},
  linkpdf =	 shefftp # {gp_gpss13_session2.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 6,
  day =  	 11
}
@Talk{Lawrence:gpssOne13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Introduction to {G}aussian Processes},
  OPTabstract =     {},
  venue =  	 {Gaussian Process Summer School, Sheffield},
  linkpdf =	 shefftp # {gp_gpss13_session1.pdf},
  youtube =	 {JSY2rha7qOw},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 6,
  day =  	 10
}

@Talk{Lawrence:cambridge13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {Gaussian} Processes},
  abstract =     {In this talk we will introduce deep Gaussian process (GP) models. Deep
GPs are a deep belief network based on Gaussian process mappings. The
data is modeled as the output of a multivariate GP. The inputs to that
Gaussian process are then governed by another GP. A single layer model
is equivalent to a standard GP or the GP latent variable model
(GPLVM). We perform inference in the model by approximate variational
marginalization. This results in a strict lower bound on the marginal
likelihood of the model which we use for model selection (number of
layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient
descent for optimization. Our fully Bayesian treatment allows for the
application of deep models even when data is scarce. Model selection
by our variational bound shows that a five layer hierarchy is
justified even when modelling a digit data set containing only 150
examples. In the seminar we will briefly review dimensionality reduction
via Gaussian processes, before showing how this framework can be
extended to build deep models.},
  venue =  	 {University of Cambridge, Engineering Department},
  linkpdf =	 shefftp # {gplvm_cambridge13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 5,
  day =  	 1
}

@Talk{Lawrence:sheffield13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep Learning: What is it and What are We doing About it?},
  abstract =     {In November last year, deep learning algorithms made the front page of the New York Times. What's special about these learning algorithms? What are they being used for and how are we using them in Sheffield? In this talk I'll explain what deep learning is, why it's considered exciting, and what the success stories are. I'll also explain what the problems with these learning systems and how we are trying to address these problems with our own class of deep architectures been developed in our group in Sheffield.\\

http://deeplearning.net/2012/12/13/deep-learning-algorithms-made-front-page-on-new-york-times/},
  venue =  	 {University of Sheffield},
  linkpdf =	 shefftp # {gplvm_sheffield13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 3,
  day =  	 20
}
@Talk{Lawrence:ucl13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {Gaussian} Processes},
  abstract =     {In this talk we will introduce deep Gaussian process (GP) models. Deep
GPs are a deep belief network based on Gaussian process mappings. The
data is modeled as the output of a multivariate GP. The inputs to that
Gaussian process are then governed by another GP. A single layer model
is equivalent to a standard GP or the GP latent variable model
(GPLVM). We perform inference in the model by approximate variational
marginalization. This results in a strict lower bound on the marginal
likelihood of the model which we use for model selection (number of
layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient
descent for optimization. Our fully Bayesian treatment allows for the
application of deep models even when data is scarce. Model selection
by our variational bound shows that a five layer hierarchy is
justified even when modelling a digit data set containing only 150
examples. In the seminar we will briefly review dimensionality reduction
via Gaussian processes, before showing how this framework can be
extended to build deep models.},
  venue =  	 {University College, London},
  linkpdf =	 shefftp # {gplvm_ucl13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 1,
  day =  	 30
}

@Talk{Lawrence:tuebingen13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {Gaussian} Processes},
  abstract =     {In this talk we will introduce deep Gaussian process (GP) models. Deep
GPs are a deep belief network based on Gaussian process mappings. The
data is modeled as the output of a multivariate GP. The inputs to that
Gaussian process are then governed by another GP. A single layer model
is equivalent to a standard GP or the GP latent variable model
(GPLVM). We perform inference in the model by approximate variational
marginalization. This results in a strict lower bound on the marginal
likelihood of the model which we use for model selection (number of
layers and nodes per layer). Deep belief networks are typically
applied to relatively large data sets using stochastic gradient
descent for optimization. Our fully Bayesian treatment allows for the
application of deep models even when data is scarce. Model selection
by our variational bound shows that a five layer hierarchy is
justified even when modelling a digit data set containing only 150
examples. In the seminar we will briefly review dimensionality reduction
via Gaussian processes, before showing how this framework can be
extended to build deep models.},
  venue =  	 {Max Planck Institute, TÃ¼bingen, Germany},
  linkpdf =	 shefftp # {gplvm_tuebingen13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 3,
  day =  	 11
}

@Talk{Lawrence:tuebingen_var13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Variational {Gaussian} Processes},
  abstract =     {In this talk we will review the variational approximation to Gaussian processes which enables Bayesian learning of latent variables. We will focus in particular on a new explanation of the variational approach that also leads to stochastic variational inference for GPs.},
  venue =  	 {Max Planck Institute, TÃ¼bingen, Germany},
  linkpdf =	 shefftp # {gpvar_tuebingen13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 3,
  day =  	 11
}


@Talk{Lawrence:aalto13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Deep {Gaussian} Processes},
    abstract =   {In this talk we will introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GPLVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples. In the seminar we will first review dimensionality reduction via Gaussian processes, before showing how this framework can be extended to build deep models.},
  venue =  	 {Aalto University, Finland},
  linkpdf =	 shefftp # {gplvm_aalto13.pdf},
  label1 =	 {Software},
  link1 =	 softwarehttp # {hsvargplvm/},
  year =  	 2013,
  month =  	 1,
  day =  	 24
}



@Talk{Lawrence:reproducible13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Reproducible Research: {Lessons} from Machine Learning},
  OPTabstract =   {},
  venue =  	 {RADIANT Kick-off Meeting, Manchester, UK},
  linkpdf =	 shefftp # {reproducible_manchester13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 1,
  day =  	 15
}

@Talk{Lawrence:infection13,
  author =  	 {Neil D. Lawrence},
  title =  	 {Machine Learning and the Life Sciences: from Modelling to Medicine},
    abstract =   {},
  venue =  	 {Department of Infection and Immunity, University of Sheffield},
  linkpdf =	 shefftp # {ml_infectionResearch13.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2013,
  month =  	 1,
  day =  	 11
}

@Talk{Lawrence:inaugural12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Life, the Universe and Machine Learning},
    abstract =   {What is Machine Learning? Why is it useful for us? Machine learning algorithms are the engines that are driving forward an intelligent internet. They are allowing us to uncover the causes of cancer and helping us understand the way the universe is put together. They are suggesting who your friends are on facebook, enabling driverless cars and causing flagging potentially fraudulent transactions on your credit card. To put it simply, machine learning is about understanding data.

<p>In this lecture I will try and give a sense of the challenges we face in machine learning, with a particular focus on those that have inspired my research. We will look at applications of data modelling from the early 18th century to the present, and see how they relate to modern machine learning. There will be a particular focus on dealing with <i>uncertainty</i>: something humans are good at, but an area where computers have typically struggled. We will emphasize the role of uncertainty in data modelling and hope to persuade the audience that correct handling of uncertainty may be one of the keys to intelligent systems.},
  venue =  	 {St George's Church Lecture Theatre, University of Sheffield},
  OPTlinkpdf =	 shefftp # {gp_ucla12_introduction.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2012,
  month =  	 9,
  day =  	 6
}


@Talk{Lawrence:ucla12a,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Brief Introduction to {G}aussian Processes},
    abstract =   {Gaussian processes are non-parametric probabilistic models for function representation. In this tutorial we give a brief introduction to Gaussian process models. Using simple examples we show how, with particular choices for covariance functions (analagous to a kernel matrix in kernel methods), we can perform inference about functions using only data sampled from those functions. We give overview how the probabilistic interpretation allows us to fit the parameters of the covariance function.},
  venue =  	 {UCLA},
  linkpdf =	 shefftp # {gp_ucla12_introduction.pdf},
  OPTlabel1 =	 {Software},
  OPTlink1 =	 sheffieldgit # {multigp/},
  year =  	 2012,
  month =  	 7,
  day =  	 27
}
@Talk{Lawrence:scienceweek_edwards13,
  author =  	 {Neil D. Lawrence},
  title =  	 {How the Planets Affect Our Daily Lives: A Brief History of Uncertainty},
  venue =        {King Edward's School, Sheffield},
  year =  	 2013,
  month =  	 3,
  url = {},
  linkpdf =	 shefftp # {planets_friends_edwards13.pdf},
  OPTmp3 =          shefftp # {120206_ode_cruk12.mp3},
  day =  	 21,
  abstract =     {Within the last 400 years scientists became able to predict the future. Crystal balls were replaced with computation. Uncertainty met mathematics. This talk gives a brief history of uncertainty and prediction. You will find out how planets affect who your Facebook friends are.},
  agegroup =     {Y12-13}
}

@Talk{Lawrence:scienceweek_birley13,
  author =  	 {Neil D. Lawrence},
  title =  	 {How the Planets Affect Our Daily Lives: A Brief History of Uncertainty},
  venue =        {Birley Community College, Sheffield},
  year =  	 2013,
  month =  	 3,
  url = {},
  linkpdf =	 shefftp # {planets_friends13_birley.pdf},
  OPTmp3 =          shefftp # {120206_ode_cruk12.mp3},
  day =  	 18,
  abstract =     {Within the last 400 years scientists became able to predict the future. Crystal balls were replaced with computation. Uncertainty met mathematics. This talk gives a brief history of uncertainty and prediction. You will find out how planets affect who your Facebook friends are.},
  agegroup =     {Y10}
}
@Talk{Lawrence:scienceweek_wilfrids13,
  author =  	 {Neil D. Lawrence},
  title =  	 {How the Planets Affect Our Daily Lives: A Brief History of Uncertainty},
  venue =        {St Wilfrid's Primary School, Sheffield},
  year =  	 2013,
  month =  	 3,
  url = {},
  linkpdf =	 shefftp # {planets_friends13_wilfrids.pdf},
  OPTmp3 =          shefftp # {120206_ode_cruk12.mp3},
  day =  	 19,
  abstract =     {Within the last 400 years scientists became able to predict the future. Crystal balls were replaced with computation. Uncertainty met mathematics. This talk gives a brief history of uncertainty and prediction. You will find out how planets affect who your Facebook friends are.},
  agegroup =     {Y4-5}
}


@Talk{Lawrence:ucla12b,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Expression Data},
  venue =        {UCLA},
  year =  	 2012,
  month =  	 7,
  url = {http://talks.cam.ac.uk/talk/index/34517},
  linkpdf =	 shefftp # {gp_ucla12_modelBased.pdf},
  OPTmp3 =          shefftp # {120206_ode_cruk12.mp3},
  day =  	 27,
  abstract =     {A simple approach to target identification through
                 gene expression studies has been to cluster the
                 expression profiles and look for coregulated genes
                 within clusters. Within systems biology mechanistic
                 models of gene expression are typically constructed
                 through differential equations. mRNAâs production is
                 taken to be proportional to transcription factor
                 activity (with the proportionality given by the
                 sensitivity) and the mRNA is assumed to decay at a
                 particular rate. The assumption that coregulated
                 genes have similar profiles is equivalent to assuming
                 both the decay and the sensitivity are high.\\\\

                 Typically researchers either use a data driven
                 approach (such as clustering) or a model based
                 approach (such as differential equations). In this
                 talk we advocate hybrid techniques which have aspects
                 of the mechanistic and data driven models. We combine
                 simple differential equation models with Gaussian
                 process priors to make probabilistic models with
                 mechanistic underpinnings. We show applications in
                 target identification from mRNA measurements.}
}

@Talk{Lawrence:pathSoc12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bridging the Gap Between Computational Biology and Systems Biology},
  OPTabstract =     {},
  venue =  	 {Pathologists Society Summer Meeting, Sheffield},
  linkpdf =	 shefftp # {ode_pathSoc12.pdf},
  year =  	 2012,
  month =  	 7,
  day =  	 4
}


@Talk{Lawrence:icmlVector12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Kernels for Vector Valued Functions},
  abstract =     {In this talk we review kernels for vector valued functions
                 from the perspective of Gaussian processes. Deriving
                 a multiple output Gaussian process from the
                 perspective of a linear dynamical system (Kalman
                 Filter) we introduce the Intrinsic Coregionalization
                 Model and the Linear Model of Coregionalization. We
                 discuss how they relate to multi-task learning with
                 GPs and the Semi Parametric Latent Factor
                 model. Finally, we will introduce convolutional
                 process models from the perspective of the latent
                 force model.},
  venue =  	 {ICML Workshop on ``Object, functional and structured data: towards next generation kernel-based methods''},
  linkpdf =	 shefftp # {mok_icmlWorkshop12.pdf},
  year =  	 2012,
  month =  	 6,
  day =  	 30
}



@Talk{Lawrence:cvpr12_2,
  author =  	 {Neil D. Lawrence},
  title =  	 {Everything You Want to Know About {G}aussian Processes: Multioutput Covariances and Mechanistic Models},
  OPTabstract =   {},
  venue =  	 {CVPR Tutorial, Providence, RI, USA},
  linkpdf =	 shefftp # {gp_cvpr12_session2.pdf},
  note = {Tutorial with Raquel Urtasun.},
  year =  	 2012,
  month =  	 6,
  day =  	 16
}


@Talk{Lawrence:cvpr12_1,
  author =  	 {Neil D. Lawrence},
  title =  	 {Everything You Want to Know About {G}aussian Processes: {G}aussian Process Regression},
  OPTabstract =   {},
  venue =  	 {CVPR Tutorial, Providence, RI, USA},
  linkpdf =	 shefftp # {gp_cvpr12_session1.pdf},
  label1 =	 {Software},
  link1 =	 sheffieldgit # {GPmat/},
  note = {Tutorial with Raquel Urtasun.},
  year =  	 2012,
  month =  	 6,
  day =  	 16
}


@Talk{Lawrence:biopredyn12_1,
  author =  	 {Neil D. Lawrence},
  title =  	 {{G}aussian Processes in Computational Biology Tutorial: Multioutput {G}aussian Processes and Mechanistic Models},
    OPTabstract =   {},
  venue =  	 {BioPreDyn Workshop, CRG, Barcelona, Spain},
  linkpdf =	 shefftp # {gp_biopredyn12_session1.pdf},
  label1 =	 {Software},
  link1 =	 sheffieldgit # {multigp/},
  year =  	 2012,
  month =  	 6,
  day =  	 12
}

@Talk{Lawrence:biopredyn12_2,
  author =  	 {Neil D. Lawrence},
  title =  	 {{G}aussian Processes in Computational Biology Tutorial: Session 2},
  OPTabstract =   {},
  venue =  	 {BioPreDyn Workshop, CRG, Barcelona, Spain},
  linkpdf =	 shefftp # {gp_biopredyn12_session2.pdf},
  year =  	 2012,
  month =  	 6,
  day =  	 12
}


@Talk{Lawrence:liverpool12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models: Bridging the Divide between Mechanistic and Data Modelling Paradigms},
    abstract =   {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning and statistical approaches are typically data
                 driven---perhaps through regularized function
                 approximation.

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms.},
  venue =  	 {Computer Science Department, University of Liverpool, U.K.},
  linkpdf =	 shefftp # {lfm_liverpool12.pdf},
  year =  	 2012,
  month =  	 5,
  day =  	 2
}


@Talk{Lawrence:mlssOne12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Dimensionality Reduction: Motivation and Linear Models},
  venue =        {La Palma, Canary Islands},
  linkpdf =	 shefftp # {mlss12_session1.pdf},
  OPTmp3 =          shefftp # {120411_mlss12_session1.mp3},
  youtube =	 {RmjMLeYXDnI},
  videolectures = {mlss2012_lawrence_dimensionality_reduction},
  year =  	 2012,
  month =  	 04,
  day =  	 11,
  OPTabstract =	 {}
}
@Talk{Lawrence:mlssTwo12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Spectral Approaches to Dimensionality Reduction},
  venue =        {La Palma, Canary Islands},
  linkpdf =	 shefftp # {mlss12_session2.pdf},
  OPTmp3 =          shefftp # {120412_mlss12_session2.mp3},
  videolectures = {mlss2012_lawrence_dimensionality_reduction},
  youtube =	 {9i6o0hiG510},
  year =  	 2012,
  month =  	 04,
  day =  	 12,
  OPTabstract =	 {}
}
@Talk{Lawrence:mlssThree12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Nonlinear Probabilistic Dimensionality Reduction},
  venue =        {La Palma, Canary Islands},
  linkpdf =	 shefftp # {mlss12_session3.pdf},
  videolectures = {mlss2012_lawrence_dimensionality_reduction},
  OPTmp3 =          shefftp # {120413_mlss12_session3.mp3},
  youtube =	 {lEn1exknKFc},
  year =  	 2012,
  month =  	 04,
  day =  	 13,
  OPTabstract =	 {}
}
@Talk{Lawrence:mlssFour12,
  author =  	 {Neil D. Lawrence},
  title =  	 {What is Machine Learning?},
  venue =        {La Palma, Canary Islands},
  linkpdf =	 shefftp # {mlss12_session4.pdf},
  OPTmp3 =          shefftp # {120413_mlss12_session3.mp3},
  youtube =	 {7xALxBJ8Wgs},
  videolectures = {mlss2012_lawrence_machine_learning},
  year =  	 2012,
  month =  	 04,
  day =  	 15,
  OPTabstract =	 {}
}

@Talk{Lawrence:rank12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models: Combining the Mechanistic and Data Driven
 Modelling Paradigms},
  venue =        {Rank Prize Workshop, Grasmere, Lake District},
  linkpdf =	 shefftp # {lfm_rank12.pdf},
  mp3 =          shefftp # {120328_lfm_rank12.mp3},
  youtube =      {nSN-wEvwYCA},
  year =  	 2012,
  month =  	 03,
  day =  	 28,
  abstract =	 {The main focus of machine learning is to combine data with assumptions
that reflect our belief about the regularity of the world. This, then,
allows us to generalize and make new predictions for `test data'.
Relative to other modelling paradigms such as those found in physics
that are based on mechanistic understandings of the world, models in
machine learning typically make only weak assumptions about data.\\\\

In this talk, we argue that these weak assumptions are also
mechanistic in nature. In particular, a very common assumption is
smoothness, which can arise through the heat equation or other models
of diffusion. Our assumption of smoothness reflects our belief in an
underlying physical world in which smoothness is the norm. Strong
mechanistic models, such as those used in computational fluid
dynamics, climate etc. typically impose much more rigid constraints on
the data and are often inappropriate for machine learning tasks where
the model needs to be adaptive and should still perform well even when
our mechanistic assumptions are not completely fulfilled. These strong
mechanistic frameworks can, however, incorporate regularities beyond
smoothness. Systems with inertia exhibit resonance and oscillation and
these can be easily incorporated with strong mechanistic assumptions.\\\\

We believe that the area between the strong and weak mechanistic
paradigms should be a focus for much more research. For many
interesting datasets we need adaptive models which include mechanistic
assumptions. The latent force modeling paradigm is one way of
approaching this which relies on the combination of differential
equation systems which are driven, or have their initial or boundary
conditions set, by Gaussian processes. The Gaussian processes provide
the necessary adaptability and the differential equation encodes
mechanistic assumptions. In this talk we introduce the model and
demonstrate results in motion capture date and, given time,
computational biology.}
}
@Talk{Lawrence:oxfordLatent12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent force models: Combining Probabilistic and Mechanistic Modelling},
  venue =        {Robotics Research Group Seminar, Department of Engineering Science, University of Oxford},
  linkpdf =	 shefftp # {lfm_oxford12.pdf},
  mp3 =          shefftp # {120213_lfm_oxford12.mp3},
  year =  	 2012,
  month =  	 02,
  day =  	 13,
  abstract =	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Statistical
                 and machine learning approaches are typically data
                 driven-perhaps through regularized function
                 approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take. Physics based approaches can be seen as
                 strongly mechanistic, the mechanistic assumptions are
                 hard encoded into the model. Data-driven approaches
                 do incorporate assumptions that might be seen as
                 being derived from some underlying mechanism, such as
                 smoothness. In this sense they are weakly
                 mechanistic.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models. By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. A moderately mechanistic
                 approach. We show an application in modelling of
                 human motion capture data.}

}
@Talk{Lawrence:oxfordUnify12,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Unifying Review of Spectral Methods for Dimensionality Reduction},
  venue =        {Robotics Research Group Tutorial, Department of Engineering Science, University of Oxford},
  linkpdf =	 shefftp # {spectral_oxford12.pdf},
  linkmp3 =	 shefftp # {120213_spectral_oxford12.pdf},
  year =  	 2012,
  month =  	 2,
  day =  	 13,
  abstract =	 {In this tutorial we will review spectral approaches to
                 dimensionality reduction, introducing a unifying
                 probabilistic perspective. Our unifying perspective
                 is based on the maximum entropy principle and the
                 resulting probabilistic models are based on GRFs. We
                 will review maximum variance unfolding, Laplacian
                 eigenmaps, locally linear embeddings and
                 Isomap. Under the framework, these approaches can be
                 divided into those that preserve local distances and
                 those that don't.  For two small data sets we show
                 that local distance preserving methods tend to
                 perform better. Finally we use the unifying framework
                 to relate these approaches to the Gaussian process
                 latent variable model.}
}

@Talk{Lawrence:cruk12,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Expression Data},
  venue =        {Cambridge Research Institute, Cancer Research UK},
  year =  	 2012,
  month =  	 2,
  url = {http://talks.cam.ac.uk/talk/index/34517},
  linkpdf =	 shefftp # {ode_cruk12.pdf},
  mp3 =          shefftp # {120206_ode_cruk12.mp3},
  day =  	 6,
  abstract =     {A simple approach to target identification through
                 gene expression studies has been to cluster the
                 expression profiles and look for coregulated genes
                 within clusters. Within systems biology mechanistic
                 models of gene expression are typically constructed
                 through differential equations. mRNAâs production is
                 taken to be proportional to transcription factor
                 activity (with the proportionality given by the
                 sensitivity) and the mRNA is assumed to decay at a
                 particular rate. The assumption that coregulated
                 genes have similar profiles is equivalent to assuming
                 both the decay and the sensitivity are high.\\\\

                 Typically researchers either use a data driven
                 approach (such as clustering) or a model based
                 approach (such as differential equations). In this
                 talk we advocate hybrid techniques which have aspects
                 of the mechanistic and data driven models. We combine
                 simple differential equation models with Gaussian
                 process priors to make probabilistic models with
                 mechanistic underpinnings. We show applications in
                 target identification from mRNA measurements.}
}

@Talk{Lawrence:BioPreDyn11,
  author =  	 {Neil D. Lawrence},
  title =  	 {},
  venue =        {BioPreDyn Launch Workshop, CRG, Barcelona},
  year =  	 2011,
  month =  	 12,
  linkpdf =	 shefftp # {overview_biopredyn11.pdf},
  mp3 =          shefftp # {111212_overview_biopredyn11.mp3},
  day =  	 12,
  OPTabstract =   	 {}
}

@Talk{Lawrence:cambridge11,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Maximum Entropy Perspective on Spectral Dimensionality Reduction},
  abstract = 	 {Spectral approaches to dimensionality reduction typically reduce the dimensionality of a data set through taking the eigenvectors of a Laplacian or a similarity matrix. Classical multidimensional scaling also makes use of the eigenvectors of a similarity matrix. In this talk we introduce a maximum entropy approach to designing this similarity matrix. The approach is closely related to maximum variance unfolding. Other spectral approaches, e.g. locally linear embeddings, turn out to also be closely related. These  methods can be seen as a sparse Gaussian graphical model where correlations between data points (rather than across data features) are specified in the graph. The hope is that this unifying perspective will allow the relationships between these methods to be better understood and will also provide the groundwork for further research.},
  venue =  	 {Machine Learning @ CUED, University of Cambridge, U.K.},
  linkpdf =	 shefftp # {spectral_cambridge11.pdf},
  mp3 =          shefftp # {111116_ode_cambridge11.mp3},
  youtube =      {2XM2tS6TKhA},
  year =  	 2011,
  month =  	 11,
  day =  	 16
}
@Talk{Lawrence:liverpool11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Expression Data},
    venue =  	 {Gene Expression Profiling Workshop, Liverpool, UK},
  linkpdf =	 shefftp # {ode_liverpool11.pdf},
  mp3 =          shefftp # {111012_ode_liverpool11.mp3},
  youtube =      {pGVMRyfulB0},
  year =  	 2011,
  month =  	 10,
  day =  	 12,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high. \\\\

                  Typically researchers either use a data driven
                  approach (such as clustering) or a model based
                  approach (such as differential equations). In this
                  talk we advocate hybrid techniques which have
                  aspects of the mechanistic and data driven
                  models. We combine simple differential equation
                  models with Gaussian process priors to make
                  probabilistic models with mechanistic
                  underpinnings. We show applications in target
                  identification from mRNA measurements.}
}


@Talk{Lawrence:abcd11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Between Systems and Data-driven Modeling for Computational Biology: Target Identification with {Gaussian} Processes},
    venue =  	 {ABCD2011, Ravenna, Italy},
  linkpdf =	 shefftp # {ode_abcd11.pdf},
  mp3 =          shefftp # {110910_ode_abcd11.mp3},
  year =  	 2011,
  month =  	 09,
  day =  	 10,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high. \\\\

                  Typically researchers either use a data driven
                  approach (such as clustering) or a model based
                  approach (such as differential equations). In this
                  talk we advocate hybrid techniques which have
                  aspects of the mechanistic and data driven
                  models. We combine simple differential equation
                  models with Gaussian process priors to make
                  probabilistic models with mechanistic
                  underpinnings. We show applications in target
                  identification from mRNA measurements.}
}

@Talk{Lawrence:bayes11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    abstract =   {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Statistical
                 and machine learning approaches are typically data
                 driven---perhaps through regularized function
                 approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take. Physics based approaches can be seen as
                 \emph{strongly mechanistic}, the mechanistic
                 assumptions are hard encoded into the
                 model. Data-driven approaches do incorporate
                 assumptions that might be seen as being derived from
                 some underlying mechanism, such as smoothness. In
                 this sense they are \emph{weakly mechanistic}.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. A \emph{moderately
                 mechanistic} approach. We show an application in
                 modelling of human motion capture data.},

  venue =  	 {Bayes 250 Workshop, University of Edinburgh, U.K.},
  linkpdf =	 shefftp # {lfm_bayes250.pdf},
  mp3 =          shefftp # {110906_lfm_bayes250.mp3},
  year =  	 2011,
  month =  	 9,
  day =  	 6,
  group =  	 {}
}


@Talk{Lawrence:dagstuhl11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Gaussian Processes and Probabilistic Models for Dimensionality Reduction},
  venue =  	 {Schloss Dagstuhl, Germany},
  year =  	 2011,
  month =  	 8,
  day =  	 25,
  mp3 =          shefftp # {110825_probDimRed_dagstuhl11.mp3},
  linkpdf =	 shefftp # {probDimRed_dagstuhl11.pdf},
  abstract =     {In this talk we present an overview of probabilistic
                 approaches to dimensionality reduction and
                 probabilistic interpretations of dimensionality
                 reduction. We start by reviewing spectral methods and
                 then turn to probabilistic PCA and the Gaussian
                 process latent variable model.},
  group =  	 {gplvm,lfm}
}


@Talk{Lawrence:krebs11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Expression Data},
  venue =  	 {Krebs Institute Symposium},
  year =  	 2011,
  month =  	 6,
  day =  	 7,
  abstract =     {A simple approach to target identification through gene
                 expression studies has been to cluster the expression
                 profiles and look for coregulated genes within
                 clusters. Within systems biology mechanistic models
                 of gene expression are typically constructed through
                 differential equations. mRNA's production is taken to
                 be proportional to transcription factor activity
                 (with the proportionality given by the sensitivity)
                 and the mRNA is assumed to decay at a particular
                 rate. The assumption that coregulated genes have
                 similar profiles is equivalent to assuming both the
                 decay and the sensitivity are high.\\\\

                 Typically researchers either use a data driven
                 approach (such as clustering) or a model based
                 approach (such as differential equations). In this
                 talk we advocate hybrid techniques which have aspects
                 of the mechanistic and data driven models. We combine
                 simple differential equation models with Gaussian
                 process priors to make probabilistic models with
                 mechanistic underpinnings. We show applications in
                 target identification from mRNA measurements.},
  group =  	 {gplvm,lfm}
}

@Talk{Lawrence:bonn11,
  author =  	 {Neil D. Lawrence},
  title =  	 {A unifying probabilistic perspective on spectral approaches to dimensionality reduction},
  venue =  	 {Hausdorff Research Institute for Mathematics, Workshop on Manifold Learning, University of Bonn},
  year =  	 2011,
  month =  	 5,
  day =  	 31,
  linkpdf =	 shefftp # {spectral_bonn11.pdf},
  abstract =     {Spectral approaches to dimensionality reduction typically
                 reduce the dimensionality of a data set through
                 taking the eigenvectors of a Laplacian or a
                 similarity matrix. Classical multidimensional scaling
                 also makes use of the eigenvectors of a similarity
                 matrix. In this talk we introduce a maximum entropy
                 approach to designing this similarity matrix. The
                 approach is closely related to maximum variance
                 unfolding. Other spectral approaches such as locally
                 linear embeddings and Laplacian eigenmaps also turn
                 out to be closely related. Each method can be seen as
                 a sparse Gaussian graphical model where correlations
                 between data points (rather than across data
                 features) are specified in the graph. This also
                 suggests optimization via sparse inverse covariance
                 techniques such as the graphical LASSO. The hope is
                 that this unifying perspective will allow the
                 relationships between these methods to be better
                 understood and will also provide the groundwork for
                 further research.}
}

@Talk{Lawrence:siena11b,
  author =  	 {Neil D. Lawrence},
  title =  	 {Advanced Use of {G}aussian Processes},
  venue =  	 {University of Siena, Italy},
  linkpdf =	 shefftp # {gpAdvanced.pdf},
  year =  	 2011,
  month =  	 4,
  day =  	 7,
  group =  	 {gplvm,lfm}
}

@Talk{Lawrence:siena11a,
  author =  	 {Neil D. Lawrence},
  title =  	 {Introduction to {G}aussian Processes},
  venue =  	 {Mathematics and Computer Science, University of Siena, Italy},
  linkpdf =	 shefftp # {gpReview.pdf},
  year =  	 2011,
  month =  	 4,
  day =  	 6,
  group =  	 {gp}
}

@Talk{Lawrence:exeter11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    abstract =   {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data
                 driven---perhaps through regularized function
                 approximation.

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 (given time) modelling of human motion capture data.},
  venue =  	 {Mathematics and Computer Science, University of Exeter, U.K.},
  linkpdf =	 shefftp # {lfm_exeter.pdf},
  year =  	 2011,
  month =  	 3,
  day =  	 16,
  group =  	 {gplvm}
}

@Talk{Lawrence:loughborough11,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle5,
  abstract = 	 gplvmAbstract5,
  venue =  	 {Department of Computer Science, University of Loughgborough, U.K.},
  linkpdf =	 shefftp # {loughborough_gplvm.pdf},
  label1 =	 {Bayesian GPLVM Software},
  link1 =	 sheffieldgit # {vargplvm/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2011,
  month =  	 3,
  day =  	 09,
  group =  	 {gplvm}
}
@Talk{Lawrence:edinburgh11,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Unifying Probabilistic Perspective on Spectral Approaches to Dimensionality Reduction},
  abstract = 	 {Spectral approaches to dimensionality reduction
                  typically reduce the dimensionality of a data set
                  through taking the eigenvectors of a Laplacian or a
                  similarity matrix. Classical multidimensional
                  scaling also makes use of the eigenvectors of a
                  similarity matrix. In this talk we introduce a
                  maximum entropy approach to designing this
                  similarity matrix. The approach is closely related
                  to maximum variance unfolding. Other spectral
                  approaches such as locally linear embeddings and
                  Laplacian eigenmaps also turn out to be closely
                  related. Each method can be seen as a sparse
                  Gaussian graphical model where correlations between
                  data points (rather than across data features) are
                  specified in the graph. This also suggests
                  optimization via sparse inverse covariance
                  techniques such as the graphical LASSO. The hope is
                  that this unifying perspective will allow the
                  relationships between these methods to be better
                  understood and will also provide the groundwork for
                  further research.},
  venue =  	 {ANC/DTC Seminar, School of Informatics, University of Edinburgh, U.K.},
  linkpdf =	 shefftp # {spectral_edinburgh11.pdf},
  year =  	 2011,
  month =  	 03,
  day =  	 01
}

@Talk{Lawrence:smpgd11,
  author =  	 {Neil D. Lawrence},
  title =  	 {Between Systems and Data-driven Modeling for Computational Biology: Target Identification with {Gaussian} Processes},
    venue =  	 {SMPGD2010, Paris, France},
  linkpdf =	 shefftp # {ode_smpgd11.pdf},
  year =  	 2011,
  month =  	 01,
  day =  	 27,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high.  \\\\ Typically researchers either use a data
                  driven approach (such as clustering) or a model
                  based approach (such as differential equations). In
                  this talk we advocate hybrid techniques which have
                  aspects of the mechanistic and data driven
                  models. We combine simple differential equation
                  models with Gaussian process priors to make
                  probabilistic models with mechanistic
                  underpinnings. We show applications in target
                  identification from mRNA measurements.}
}

@Talk{Lawrence:nipsw10,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Probabilistic Perspective on Spectral Dimensionality Reduction},
    venue =  	 {Challenges of Data Visualization Workshop, NIPS 2010, Whistler},
  linkpdf =	 shefftp # {spectral_nipsw10.pdf},
  year =  	 2010,
  month =  	 12,
  day =  	 11,
  abstract =   	 {Spectral approaches to dimensionality reduction
                  typically reduce the dimensionality of a data set
                  through taking the eigenvectors of a Laplacian or a
                  similarity matrix. Classical multidimensional
                  scaling also makes use of the eigenvectors of a
                  similarity matrix. In this talk we introduce a
                  maximum entropy approach to designing this
                  similarity matrix. The approach is closely related
                  to maximum variance unfolding and other spectral
                  approaches such as locally linear embeddings and
                  Laplacian eigenmaps also turn out to be closely
                  related. Each method can be seen as a sparse
                  Gaussian graphical model where correlations between
                  data points (rather than across data features) are
                  specified in the graph. This also suggests
                  optimization via sparse inverse covariance
                  techniques such as the graphical LASSO. The hope is
                  that this unifying perspective will allow the
                  relationships between these methods to be better
                  understood and will also provide the groundwork for
                  further research.}
}
@Talk{Lawrence:lfmSheffield10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    venue =  	 {Dynamics Research Group Seminar, Mechanical Engineering, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {lfm_sheffield10.pdf},
  year =  	 2010,
  month =  	 11,
  day =  	 04,
  abstract =   	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data
                 driven---perhaps through regularized function
                 approximation.

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 (given time) modelling of human motion capture data.}
}

@Talk{Lawrence:aalto10,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Probabilistic Perspective on Spectral Dimensionality Reduction},
    venue =  	 {Department of Statistics, Aalto University, Finland},
  linkpdf =	 shefftp # {spectral_aalto10.pdf},
  year =  	 2010,
  month =  	 10,
  day =  	 20,
  abstract =   	 {Spectral approaches to dimensionality reduction typically reduce the dimensionality of a data set through taking the eigenvectors of a Laplacian or a similarity matrix. Classical multidimensional scaling also makes use of the eigenvectors of a similarity matrix. In this talk we introduce a maximum entropy approach to designing this similarity matrix. The approach is closely related to maximum variance unfolding and other spectral approaches such as locally linear embeddings and Laplacian eigenmaps also turn out to be closely related. Each method can be seen as a sparse Gaussian graphical model where correlations between data points (rather than across data features) are specified in the graph. This also suggests optimization via sparse inverse covariance techniques such as the graphical LASSO. The hope is that this unifying perspective will allow the relationships between these methods to be better understood and will also provide the groundwork for further research.}
}

@Talk{Lawrence:aaai10,
  author =  	 {Neil D. Lawrence},
  title =  	 {A Probabilistic Perspective on Spectral Dimensionality Reduction},
    venue =  	 {AAAI Fall Symposium on Manifold Methods, U.S.A.},
  linkpdf =	 shefftp # {spectral_aaai10.pdf},
  year =  	 2010,
  month =  	 11,
  day =  	 11,
  abstract =   	 {Spectral approaches to dimensionality reduction
                  typically reduce the dimensionality of a data set
                  through taking the eigenvectors of a Laplacian or a
                  similarity matrix. Classical multidimensional
                  scaling also makes use of the eigenvectors of a
                  similarity matrix. In this talk we introduce a
                  maximum entropy approach to designing this
                  similarity matrix. The approach is closely related
                  to maximum variance unfolding and other spectral
                  approaches such as locally linear embeddings and
                  Laplacian eigenmaps also turn out to be closely
                  related. Each method can be seen as a sparse
                  Gaussian graphical model where correlations between
                  data points (rather than across data features) are
                  specified in the graph. This also suggests
                  optimization via sparse inverse covariance
                  techniques such as the graphical LASSO. The hope is
                  that this unifying perspective will allow the
                  relationships between these methods to be better
                  understood and will also provide the groundwork for
                  further research.}
}
@Talk{Lawrence:eurogene10,
  author =  	 {Neil D. Lawrence and Antti Honkela},
  title =  	 {Bayesian approaches to Transcription Factor Target Identification},
    venue =  	 {Course in Practical Systems Biology: Visualisation and Reverse engineering gene regulatory networks, EuroMediterranean University Centre of Ronzano, Bologna, Italy},
  linkpdf =	 shefftp # {gp_bologna10.pdf},
  year =  	 2010,
  month =  	 10,
  day =  	 10,
  abstract =   	 {A simple approach to target identification through
                  gene expression studies has been to cluster the
                  expression profiles and look for coregulated genes
                  within clusters. Within systems biology mechanistic
                  models of gene expression are typically constructed
                  through differential equations. mRNA's production is
                  taken to be proportional to transcription factor
                  activity (with the proportionality given by the
                  sensitivity) and the mRNA is assumed to decay at a
                  particular rate. The assumption that coregulated
                  genes have similar profiles is equivalent to
                  assuming both the decay and the sensitivity are
                  high.  \\\\

                  In this lecture we introduce Bayesian
                  approaches to target identification which make use
                  of sampling approaches to rank candidate lists of
                  targets. We will begin with an introduction to the
                  target identification problem and an overview of the
                  power of Bayesian approaches in solving it. We will
                  then consider how probabilistic models such as
                  Gaussian processes can be used for ranking potential
                  targets of a transcription factor. These models are
                  simple enough to allow genome wide target
                  identification, but rich enough to encode dynamical
                  behavior that, allowing us to identify putative
                  targets even when decay rates are low.}
}
@Talk{Lawrence:validation10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Making Implementations Available for the Research Community},
    venue =  	 {Validation in Statistics and Machine Learning, Weierstrass Institute, Berlin, Germany},
  linkpdf =	 shefftp # {reproducible_berlin10.pdf},
  year =  	 2010,
  month =  	 10,
  day =  	 6,
  abstract =   	 {Machine learning research is either inspired by a particular application, or by a general desire to make technology more ``inteligent''. In modern machine learning most methodological development is mathematically inspired and results in an algorithm for optimization or fitting of a model to data.

Design choices in implementation of an algorithm can have a significant effect on the quality of results. Decisions such as model initializaiton and data pre-processing are all part of the implementation. Necessarily, space constraints sometimes mean that such details are not included in the associated paper. It seems clear that the paper only tells part of the story. Implementations need to be made available at the time of submission of the paper, so that the full story may be followed. In our research group we have done this since 2001. In this talk I will make the arguments in favour of doing this universally and give personal experiences of the results.}
}
@Talk{Lawrence:phylogenetics10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
    venue =  	 {Functional Phylogenies: Evolutionary Inference for Functional Data, University of Oxford, U.K.},
  linkpdf =	 shefftp # {lfm_oxford10.pdf},
  year =  	 2010,
  month =  	 09,
  day =  	 27,
  abstract =   	 {Physics based approaches to data modeling involve constructing an
accurate mechanistic model of data, often based on differential
equations. Machine learning approaches are typically data
driven---perhaps through regularized function approximation.

These two approaches to data modeling are often seen as polar
opposites, but in reality they are two different ends to a spectrum of
approaches we might take.

In this talk we introduce latent force models. Latent force models are
a new approach to data representation that model data through unknown
forcing functions that drive differential equation models.  By
treating the unknown forcing functions with Gaussian process priors we
can create probabilistic models that exhibit particular physical
characteristics of interest, for example, in dynamical systems
resonance and inertia. This allows us to perform a synthesis of the
data driven and physical modeling paradigms. We will show applications
of these models in systems biology and (given time) modelling of human
motion capture data.}
}
@Talk{Lawrence:tutorialPRIB10,
  author =  	 {Neil D. Lawrence},
  title =  	 {{PRIB} Tutorial: {G}aussian Processes and Gene Regulation},
    venue =  	 {PRIB2010, Radboud University, Nijmegen, Netherlands},
  linkpdf =	 shefftp # {gp_prib10.pdf},
  year =  	 2010,
  month =  	 09,
  day =  	 22,
  abstract =   	 {Computational biology models are often missing information,
                 such as the concentration of biochemical species of
                 interest. One approach to dealing with this missing
                 information is to place a probabilistic prior over
                 the missing data. One possible choice for such a
                 prior is a Gaussian process.\\\\

                 In this tutorial we will give an introduction to
                 Gaussian processes. We will give simple examples of
                 Gaussian processes in regression and
                 interpolation. We will then show how Gaussian
                 processes can be incorporated with differential
                 equation models to give probabilistic models for
                 transcription. Such models can then be used to rank
                 potential targets of given transcription factors.}
}


@Talk{Lawrence:ibsb10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Between Systems and Data-driven Modeling for Computational Biology: Target Identification with {Gaussian} Processes},
    venue =  	 {IBSB2010, Kyoto University, Japan},
  linkpdf =	 shefftp # {ode_kyoto10.pdf},
  year =  	 2010,
  month =  	 07,
  day =  	 27,
  abstract =   	 {A simple approach to target identification through gene expression studies has been to cluster the expression profiles and look for coregulated genes within clusters. Within systems biology mechanistic models of gene expression are typically constructed through differential equations. mRNA's production is taken to be proportional to transcription factor activity (with the proportionality given by the sensitivity) and the mRNA is assumed to decay at a particular rate. The assumption that coregulated genes have similar profiles is equivalent to assuming both the decay and the sensitivity are high.

Typically researchers either use a data driven approach (such as clustering) or a model based approach (such as differential equations). In this talk we advocate hybrid techniques which have aspects of the mechanistic and data driven models. We combine simple differential equation models with Gaussian process priors to make probabilistic models with mechanistic underpinnings. We show applications in target identification from mRNA measurements.}
}

@Talk{Lawrence:inference10,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Inference Group, Cavendish Laboratory, University of Cambridge},
  linkpdf =	 shefftp # {lfm_inference10.pdf},
  year =  	 2010,
  month =  	 03,
  day =  	 01,
  abstract =   	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}
}


@Talk{Lawrence:tlsd09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Transfer Learning and Multiple Output Kernel Functions},
  venue =  	 {NIPS 09 Workshop on Transfer Learning for Structured Data},
  linkpdf =	 shefftp # {tlsd09.pdf},
  year =  	 2009,
  month =  	 12,
  day =  	 12,
  abstract =   	 {A standard Bayesian approach to transfer learning is to
                 construct hierarchical probabilistic models. Learning
                 tasks are typically related in the model through
                 conditional independencies of the
                 variables/parameters. Many of the variables are
                 unobserved. Marginalization of the unobserved
                 variables and Bayesian treatment of parameters
                 induces structure and correlations between the tasks.
                 Gaussian processes are prior distributions over
                 functions: kernel functions are the covariances
                 associated with these priors. A Gaussian process can
                 be set up to have multiple outputs. However, for
                 these outputs to have correlation between them a
                 covariance function that models correlations between
                 outputs is required. Equivalently we need to develop
                 multiple output kernel functions (also known as
                 multitask kernel functions, or structured output
                 kernels).  In this talk we will briefly review work
                 in creating multiple output kernels before focusing
                 on models represented by a convolution processes. We
                 will arrive at convolutional processes through
                 physical interpretations of our models. We will try
                 to illustrate these models with a range of real world
                 examples of both transfer learning and other
                 applications.}
}



@Talk{Lawrence:kcl09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Disordered Systems Group, Department of Mathematics, King's College London},
  linkpdf =	 shefftp # {lfm_kcl09.pdf},
  year =  	 2009,
  month =  	 11,
  day =  	 25,
  abstract =   	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}
}

@Talk{Lawrence:tigem09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Nonlinear Response in {G}aussian Process Models of Transcription},
  venue =  	 {Telethon Institute of Genetics and Medicine, Naples, Italy},
  linkpdf =	 shefftp # {tigem09.pdf},
  year =  	 2009,
  month =  	 10,
  day =  	 29,
  OPTabstract =   	 {}
}


@Talk{Lawrence:licsb09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Estimation of Multiple Transcription Factor Activities using ODEs and {G}aussian Processes},
  venue =  	 {Workshop on Learning and Inference in Computational and Systems Biology (LICSB)},
  videolectures =	 {licsb09_lawrence_emtf},
  year =  	 2009,
  month =  	 4,
  day =  	 1,
  OPTabstract =   	 {}
}
@Talk{Lawrence:napoli09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Gene Expression with {G}aussian Processes},
  venue =  	 {BioDN@work '09, Computational Biology \& Bioinformatics, University of Naples ``Federico II''},
  linkpdf =	 shefftp # {ode_napoli09.pdf},
  year =  	 2009,
  month =  	 10,
  day =  	 28,
  abstract =   	 {A simple approach to target identification through gene

                 expression studies has been to cluster the expression
                 profiles and look for coregulated genes within
                 clusters. Within systems biology mechanistic models
                 of gene expression are typically constructed through
                 differential equations. mRNA's production is taken to
                 be proportional to transcription factor activity
                 (with the proportionality given by the sensitivity)
                 and the mRNA is assumed to decay at a particular
                 rate. The assumption that coregulated genes have
                 similar profiles is equivalent to assuming both the
                 decay and the sensitivity are high.

                In this talk we advocate model based target
                identification. We develop a simple probabilistic
                models of transcription (and translation) which encode
                mRNA (or Transcription Factor) production and
                decay. Our models are simple enough to allow genome
                wide target identification, but rich enough to encode
                dynamical behavior that, allowing us to identify
                putative targets even when decay rates are low.}
}


@Talk{Lawrence:nyu09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Computer Science Colloquium, NYU, U.S.A.},
  linkpdf =	 shefftp # {lfm_nyu09.pdf},
  year =  	 2009,
  month =  	 10,
  day =  	 23,
  abstract =   	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}
}

@Talk{Lawrence:google09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models},
  venue =  	 {Google Research, New York, U.S.A.},
  linkpdf =	 shefftp # {lfm_google09.pdf},
  year =  	 2009,
  month =  	 10,
  day =  	 21,
  abstract =   	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning approaches are typically data driven---
                 perhaps through regularized function approximation.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}
}

@Talk{Lawrence:jhu09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Model Based Target Identification from Gene Expression with {G}aussian Processes},
  venue =  	 {School of Public Health, Johns Hopkins University, U.S.A.},
  linkpdf =	 shefftp # {ode_jhu09.pdf},
  year =  	 2009,
  month =  	 10,
  day =  	 19,
  abstract =   	 {A simple approach to target identification through gene
                 expression studies has been to cluster the expression
                 profiles and look for coregulated genes within
                 clusters. Within systems biology mechanistic models
                 of gene expression are typically constructed through
                 differential equations. mRNA's production is taken to
                 be proportional to transcription factor activity
                 (with the proportionality given by the sensitivity)
                 and the mRNA is assumed to decay at a particular
                 rate. The assumption that coregulated genes have
                 similar profiles is equivalent to assuming both the
                 decay and the sensitivity are high.

                In this talk we advocate model based target
                identification. We develop a simple probabilistic
                models of transcription (and translation) which encode
                mRNA (or Transcription Factor) production and
                decay. Our models are simple enough to allow genome
                wide target identification, but rich enough to encode
                dynamical behavior that, allowing us to identify
                putative targets even when decay rates are low.}
}

@Talk{Lawrence:newcastle09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Modelling with {G}aussian Processes},
  venue =  	 {School of Mathematics and Statistics, University of Newcastle, U.K.},
  linkpdf =	 shefftp # {lfm_newcastle09.pdf},
  year =  	 2009,
  month =  	 10,
  day =  	 9,
  abstract =   	 {Physics based approaches to data modeling involve
                 constructing an accurate mechanistic model of data,
                 often based on differential equations. Machine
                 learning typically focuses on data driven
                 approaches---perhaps through regularized function
                 approximations.\\\\

                 These two approaches to data modeling are often seen
                 as polar opposites, but in reality they are two
                 different ends to a spectrum of approaches we might
                 take.\\\\

                 In this talk we introduce latent force models. Latent
                 force models are a new approach to data
                 representation that model data through unknown
                 forcing functions that drive differential equation
                 models.  By treating the unknown forcing functions
                 with Gaussian process priors we can create
                 probabilistic models that exhibit particular physical
                 characteristics of interest, for example, in
                 dynamical systems resonance and inertia. This allows
                 us to perform a synthesis of the data driven and
                 physical modeling paradigms. We will show
                 applications of these models in systems biology and
                 modelling of human motion capture data.}
}




@Talk{Lawrence:inspire09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Inspire Workshop, Imperial College, U.K.},
  linkpdf =	 shefftp # {lfm_inspire09.pdf},
  year =  	 2009,
  month =  	 9,
  day =  	 24,
  OPTabstract =   	 {}
}

@Talk{Lawrence:warwick09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Efficient Multiple Output Convolution Processes for Multiple Task Learning},
  venue =  	 {Inf Workshop, University of Warwick, U.K.},
  linkpdf =	 shefftp # {lfm_warwick09.pdf},
  year =  	 2009,
  month =  	 9,
  day =  	 16,
  OPTabstract =   	 {}
}


@Talk{Lawrence:interspeech09,
  author =  	 {Neil D. Lawrence and Jon Barker},
  title =  	 {Dealing with High Dimensional Data with Dimensionality Reduction},
  venue =  	 {Interspeech Tutorial, Brighton, U.K.},
  linkpdf =	 shefftp # {interspeech09.pdf},
  year =  	 2009,
  month =  	 9,
  day =  	 6,
  OPTabstract =   	 {}
}


@Talk{Lawrence:lfm_slim09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models and Multiple Output {G}aussian Processes},
  venue =  	 {Statistics and Learning Interface Meeting, University of Manchester, U.K.},
  linkpdf =	 shefftp # {lfm_slim09.pdf},
  year =  	 2009,
  month =  	 7,
  day =  	 23,
  abstract =   	 {We are used to dealing with the situation where we have a
                 latent variable. Often we assume this latent variable
                 to be independently drawn from a distribution,
                 e.g. probabilistic PCA or factor analysis. This
                 simplification is often extended for temporal data
                 where tractable Markovian independence assumptions
                 are used (e.g. Kalman filters or hidden Markov
                 models). In this talk we will consider the more
                 general case where the latent variable is a forcing
                 function in a differential equation model. We will
                 show how for some simple ordinary differential
                 equations the latent variable can be dealt with
                 analytically for particular Gaussian process priors
                 over the latent force. In this talk we will introduce
                 the general framework and present results in systems
                 biology and motion capture.}
}

@Talk{Lawrence:lfm_cagliary09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Pattern Recognition Applications Group, Department of Electrical and Electronic Engineering, University of Cagliari, Italy},
  linkpdf =	 shefftp # {lfm_cagliari09.pdf},
  year =  	 2009,
  month =  	 7,
  day =  	 13,
  abstract =   	 {We are used to dealing with the situation where we have a
                 latent variable. Often we assume this latent variable
                 to be independently drawn from a distribution,
                 e.g. probabilistic PCA or factor analysis. This
                 simplification is often extended for temporal data
                 where tractable Markovian independence assumptions
                 are used (e.g. Kalman filters or hidden Markov
                 models). In this talk we will consider the more
                 general case where the latent variable is a forcing
                 function in a differential equation model. We will
                 firstly give a brief introduction to Gaussian
                 processes, then we will show how for some simple
                 ordinary differential equations the latent variable
                 can be dealt with analytically for particular
                 Gaussian process priors over the latent force. In
                 this talk we will introduce the general framework,
                 present results in systems biology and motion
                 capture.}
}




@Talk{Lawrence:tut09,
  author =  	 {Neil D. Lawrence},
  title =  	 {An Introduction to Systems Biology from a Machine Learning Perspective},
  venue =  	 {TISE Summer School, Tampere, Finland},
  linkpdf =	 shefftp # {tut.pdf},
  year =  	 2009,
  month =  	 6,
  day =  	 22,
  OPTabstract =   	 {}
}
@Talk{Lawrence:tutII09,
  author =  	 {Neil D. Lawrence},
  title =  	 {An Introduction to Systems Biology from a Machine Learning Perspective {II}},
  venue =  	 {TISE Summer School, Tampere, Finland},
  linkpdf =	 shefftp # {tut2.pdf},
  year =  	 2009,
  month =  	 6,
  day =  	 23,
  OPTabstract =   	 {}
}

@Talk{Lawrence:learning09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Non-linear Matrix Factorization with {G}aussian Processes},
  venue =  	 {Learning Workshop, Clearwater, Florida},
  linkpdf =	 shefftp # {snowbird09.pdf},
  year =  	 2009,
  month =  	 4,
  day =  	 14,
  OPTabstract =   	 {}
}

@Talk{Lawrence:emmds09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Non-linear Matrix Facorization with {G}aussian Proceses},
  venue =  	 {European Modern Massive Datasets Workshop, Denmark Techinical University, Copenhagen},
  linkpdf =	 shefftp # {emmds09.pdf},
  year =  	 2009,
  month =  	 7,
  day =  	 3,
  abstract =   	 {A popular approach to collaborative filtering is matrix
                 factorization. In this talk we consider the
                 ``probabilistic matrix factorization'' and by taking
                 a latent variable model perspective we show its
                 equivalence to Bayesian PCA. This inspires us to
                 consider probabilistic PCA and its non-linear
                 extension, the Gaussian process latent variable model
                 (GP-LVM) as an approach for probabilistic non-linear
                 matrix factorization. We apply out approach to
                 benchmark movie recommender data sets. The results
                 show better than previous state-of-the-art
                 performance.}
}
@Talk{Lawrence:python09,
  author =  	 {Neil D. Lawrence},
  title =  	 {Python in Machine Learning},
  venue =  	 {MLO Lunch, Kilburn 2.15},
  linkpdf =	 shefhttp # {mlolunch.pdf},
  year =  	 2009,
  month =  	 3,
  day =  	 25,
  abstract =     {Is Python a viable replacement for MATLAB?\\\\

                 We are incredibly reliant on MATLAB, but should we be
                 looking elsewhere for our ML programming needs? In
                 this ML lunch I will try and share my recent
                 experiences with Python and machine learning: good
                 and bad. The main questions I think we should be
                 considering are:\\\\

                 Should we be trying to move to Python for our
                 research?\\\\

                 Should we be using Python in our teaching?\\\\

                 I don't know the answer, but I'll try and use this
                 MLO lunch to start the debate!}
}

@Talk{Lawrence:bristol08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Intelligent Systems Seminars, University of Bristol, U.K.},
  linkpdf =	 shefftp # {lfm_bristol08.pdf},
  year =  	 2008,
  month =  	 10,
  day =  	 16,
  abstract =   	 {We are used to dealing with the situation where we
                 have a latent variable. Often we assume this latent
                 variable to be independently drawn from a
                 distribution, e.g. probabilistic PCA or factor
                 analysis. This simplification is often extended for
                 temporal data where tractable Markovian independence
                 assumptions are used (e.g. Kalman filters or hidden
                 Markov models). In this talk we will consider the
                 more general case where the latent variable is a
                 forcing function in a differential equation model. We
                 will firstly give a brief introduction to Gaussian
                 processes, then we will show how for some simple
                 ordinary differential equations the latent variable
                 can be dealt with analytically for particular
                 Gaussian process priors over the latent force. In
                 this talk we will introduce the general framework,
                 present results in systems biology.\\\\

                 Joint work with Magnus Rattray, Mauricio \'Alvarez, Pei
                 Gao, Antti Honkela, David Luengo, Guido Sanguinetti
                 and Michalis K. Titsias.},

  group =  	 {ode, gp}
}
@Talk{Lawrence:rss08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Inference in Ordinary Differential Equations with Latent Functions through {G}aussian Processes},
  venue =  	 {RSS Manchester Local Group},
  linkpdf =	 shefftp # {manchesterRss.pdf},
  year =  	 2008,
  month =  	 10,
  day =  	 8,
  abstract =   	 {In biochemical interaction networks is a key problem
                 in estimation of the structure and parameters of the
                 genetic, metabolic and protein interaction networks
                 that underpin all biological processes. We present a
                 framework for Bayesian marginalisation of these
                 latent chemical species through Gaussian process
                 priors. We demonstrate our general approach on three
                 different biological examples of single input motifs,
                 including both activation and repression of
                 transcription.  We focus in particular on the problem
                 of inferring transcription factor activity when the
                 concentration of active protein cannot easily be
                 measured. The uncertainty in the inferred
                 transcription factor activity can be integrated out
                 in order to derive a likelihood function that can be
                 used for the estimation of regulatory model
                 parameters.},
  group =  	 {ode, gp}
}

@Talk{Lawrence:ncaf08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Dynamics with {G}aussian Processes},
  venue =  	 {Natural Computing Applications Forum, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {dynamics_ncaf08.pdf},
  year =  	 2008,
  month =  	 9,
  day =  	 10,
  abstract =   	 {We are used to dealing with the situation where we have
                 a latent variable. Often we assume this latent
                 variable to be independently drawn from a
                 distribution, \emph{e.g.} probabilistic PCA or factor
                 analysis. This simplification is often extended for
                 temporal data where tractable Markovian independence
                 assumptions are used (\emph{e.g.} Kalman filters or
                 hidden Markov models). In this talk we will consider
                 the more general case where the latent variable is a
                 forcing function in a differential equation model. We
                 will firstly give a brief introduction to Gaussian
                 processes, then we will show how for some simple
                 ordinary differential equations the latent variable
                 can be dealt with analytically for particular
                 Gaussian process priors over the latent force. In
                 this talk we will introduce the general framework,
                 present results in systems biology.\\\\

                 Joint work with Magnus Rattray, Mauricio \'Alvarez, Pei
                 Gao, Antti Honkela, David Luengo, Guido Sanguinetti
                 and Michalis K. Titsias.},
  group =  	 {}
}

@Talk{Lawrence:mlmi08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Ambiguity Modelling in Latent Spaces},
  venue =  	 {Machine Learning for Multimodal Interaction, Utrecht, The Netherlands},
  linkpdf =	 shefftp # {ncca_mlmi08.pdf},
  linkvideo =	 {http://mmm.idiap.ch/pres-mlmi2008/results?query=//kleweldata/mlmi2008/2008-09-08_09h14},
  year =  	 2008,
  month =  	 9,
  day =  	 8,
  abstract =   	 {We are interested in the situation where we have two
                  or more representations of an underlying
                  phenomenon. In particular we are interested in the
                  scenario where the representation are
                  complementary. This implies that a single individual
                  representation is not sufficient to fully
                  discriminate a specific instance of the underlying
                  phenomenon, it also means that each representation
                  is an ambiguous representation of the other
                  complementary spaces. In this paper we present a
                  latent variable model capable of consolidating
                  multiple complementary representations. Our method
                  extends canonical correlation analysis by
                  introducing additional latent spaces that are
                  specific to the different representations, thereby
                  explaining the full variance of the
                  observations. These additional spaces, explaining
                  representation specific variance, separately model
                  the variance in a representation ambiguous to the
                  other. We develop a spectral algorithm for fast
                  computation of the embeddings and a probabilistic
                  model (based on Gaussian processes) for validation
                  and inference. The proposed model has several
                  potential application areas, we demonstrate its use
                  for multi-modal regression on a benchmark human pose
                  estimation data set.},
  group =  	 {}
}

@Talk{Lawrence:bark08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Force Models with {G}aussian Processes},
  venue =  	 {Bayesian Research Kitchen, Grasmere, Lake District, U.K.},
  linkpdf =	 shefftp # {lfm_bark08.pdf},
  linkvideo =	 {http://videolectures.net/bark08_lawrence_lfmwgp/},
  videolectures =	 {bark08_lawrence_lfmwgp},
  year =  	 2008,
  month =  	 9,
  day =  	 6,
  abstract =   	 {We are used to dealing with the situation where we have
                 a latent variable. Often we assume this latent
                 variable to be independently drawn from a
                 distribution, \emph{e.g.} probabilistic PCA or factor
                 analysis. This simplification is often extended for
                 temporal data where tractable Markovian independence
                 assumptions are used (\emph{e.g.} Kalman filters or
                 hidden Markov models).\\\\

                 In this talk we will consider the more general case
                 where the latent variable is a forcing function in a
                 differential equation model. We will show how for
                 some simple ordinary differential equations the
                 latent variable can be dealt with analytically for
                 particular Gaussian process priors over the latent
                 force. In this talk we will introduce the general
                 framework, present results in systems biology and
                 preview extensions.\\\\

                 Joint work with Magnus Rattray, Mauricio \'Alvarez, Pei
                 Gao, Antti Honkela, David Luengo, Guido Sanguinetti
                 and Michalis K. Titsias.},
  group =  	 {}
}


@Talk{Lawrence:warwick08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Statistical inference in systems biology through {G}aussian processes and ordinary differential equations},
  venue =  	 {LICSB Workshop, University of Warwick, U.K.},
  linkpdf =	 shefftp # {warwick.pdf},
  year =  	 2008,
  month =  	 06,
  day =  	 17,
  abstract =   	 {In this talk we will summarise recent work from our group
                 in Manchester on inferring `latent biochemical
                 species' in biological systems using Gaussian
                 processes and differential equations. A key problem
                 in biological data is when particular biochemical
                 species of interest are not directly measurable. We
                 will show how the framework of Gaussian processes can
                 be brought to bear on the problem and values of
                 latent chemical species can be inferred given data
                 and a differential equation model.},
  group =  	 {}
}

@Talk{Lawrence:gpbayes08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Learning and Inference with {G}aussian Processes: An Overview of {B}ayesian Inference and {G}aussian Processes},
  venue =  	 {Data Modelling Series, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {gpAndBayes.pdf},
  year =  	 2008,
  month =  	 04,
  day =  	 01,
  group =  	 {}
}

@Talk{Lawrence:human08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Human Motion Modelling through Dimensional Reduction with {G}aussian Processes},
  venue =  	 {Hotel Golf, Bled, Slovenia},
  linkpdf =	 shefftp # {human.pdf},
  year =  	 2008,
  month =  	 01,
  day =  	 29,
  group =  	 {pascal}
}

@Talk{Lawrence:newton08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Human Motion Modelling with {G}aussian Processes},
  venue =  	 {Netwon Institute, Cambridge, U.K.},
  linkpdf =	 shefftp # {gplvm_newton.pdf},
  year =  	 2008,
  month =  	 02,
  day =  	 07,
  group =  	 {pascal}
}

@Talk{Lawrence:thematic08,
  author =  	 {Neil D. Lawrence},
  title =  	 {{TP1}: Leveraging Complex Prior Knowledge in Learning},
  venue =  	 {Hotel Golf, Bled, Slovenia},
  linkpdf =	 shefftp # {thematic.pdf},
  year =  	 2008,
  month =  	 01,
  day =  	 28,
  group =  	 {pascal}
}

@Talk{Lawrence:data08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Dimensionality Reduction},
  abstract = 	 {We approach dimensionality reduction from the
                 perspective of multidimensional scaling. Starting
                 from the basics, we draw the relationship between
                 multidimensional scaling and principal component
                 analysis. From this background we briefly review
                 kernel PCA and Isomap. Finally, we consider the
                 problem of model selection using Gaussian
                 processes.},
  venue =  	 {EPSRC Winter School, University of Sheffield, Sheffield, U.K.},
  linkpdf =	 shefftp # {dataModellingWinter.pdf},
  linksoftware =	 softwarehttp # {dimred/},
  linkvideo =	 {http://videolectures.net/epsrcws08_lawrence_dr/},
  videolectures =	 {epsrcws08_lawrence_dr},
  year =  	 2008,
  month =  	 01,
  day =  	 24,
  group =  	 {spectral}
}

@Talk{Lawrence:msr07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Variables, Differential Equations and {G}aussian Processes},
  abstract = 	 {We are used to dealing with the situation where we have a
                 latent variable. Often we assume this latent variable
                 to be independently drawn from a distribution,
                 e.g. probabilistic PCA or factor analysis. This
                 simplification is often extended for temporal data
                 where tractable Markovian independence assumptions
                 are used (e.g. Kalman filters or hidden Markov
                 models). In this talk we will consider such models in
                 the context of a biological problem: inferring
                 transcription factor activities in simple
                 transcription networks. We will extend the simpler
                 formalisms described above to consider the case where
                 the latent variable is a 'latent function' and the
                 relationship with the observed data is described by a
                 linear differential equation. Through the use of a
                 Gaussian process prior over the latent function we
                 can perform inference tractably and learn parameters
                 of interest in the system.},
  venue =  	 {Microsoft Research, Cambridge, U.K.},
  linkpdf =	 shefftp # {lvDeGp.pdf},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {TIGRE Project Page},
  link2 =	 {http://sheffieldml.github.io/projects/tigre/},
  label3 =	 {GPSIM Software},
  link3 =	 softwarehttp # {gpsim/},
  label4 =	 {Demos Software},
  link4 =	 softwarehttp # {oxford/},
  year =  	 2007,
  month =  	 11,
  day =  	 12,
  group =  	 {gp,puma,gpsim}
}

@Talk{Lawrence:param07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Modelling Transcriptional Regulation with {G}aussian Processes},
  OPTabstract = 	 {},
  venue =  	 {Parameter Estimation Workshop, Manchester Interdisciplinary Biocentre, University of Manchester, U.K.},
  linkpdf =	 shefftp # {parameterEstimationTalk.pdf},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {TIGRE Project Page},
  link2 =	 {http://sheffieldml.github.io/projects/tigre/},
  label3 =	 {GPSIM Software},
  link3 =	 softwarehttp # {gpsim/},
  label4 =	 {Demos Software},
  link4 =	 softwarehttp # {oxford/},
  year =  	 2007,
  month =  	 11,
  day =  	 7,
  group =  	 {puma,gpsim}
}
@Talk{Lawrence:mbb07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Towards Computational Systems Biology with a Statistical Analysis Pipeline for Microarray Data},
  abstract = 	 {Since the human genome project began mathematical models
                 have become an integral part of biological data analysis.
                 The growth in data availability has necessitated their use
                 in summarization of the data (e.g. \emph{statistical}
                 approaches such as hierarchical clustering). Simultaneously,
                 as more has become understood about the mechanisms
                 underpinning particular pathways \emph{mechanistic}
                 models of interactions have become more widespread.\\\\


                 The data-driven statistical approach and the mechanistic
                 model approach each have their advantages. Data-driven models
                 can be used in genome wide analyses to 'fish' for genes that
                 were not known to be relevant but provide a critical role in
                 a pathway. Mechanistic models make real predictions about how
                 systems will respond given particular interventions. The two
                 approaches have interacted only loosely, often not through
                 interaction between the `mathematicians' but through indirect
                 interaction via the biologists. \\\\

                 In this talk we will follow describe a statistical analysis
                 `pipeline' for microarray data which handles the noise in
                 the data. As we proceed down the pipeline we will come closer
                 to mechanistic models of systems. We will finish with some
                 general thoughts about the contribution that a combined
                 statistical/mechanistic modelling approach can make.},
  venue =  	 {Department of Molecular Biology and Biotechnology, University of Sheffield, U.K.},
  linkpdf =	 shefftp # {puma_mbb.pdf},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {TIGRE Project Page},
  link2 =	 {http://sheffieldml.github.io/projects/tigre/},
  label3 = 	 {PUMA Software},
  link3 = 	 {http://bioconductor.org/packages/2.2/bioc/html/puma.html},
  year =  	 2007,
  month =  	 10,
  day =  	 31,
  group =  	 {puma,gpsim}
}
@Talk{Lawrence:inverse07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Latent Variable Modelling with {G}aussian Processes},
  abstract = 	 {In this talk we will briefly describe the Gaussian process
                 latent variable model, an approach to probabilistic
                 modelling of data through non-linear dimensional
                 reduction. The model takes a dual approach to
                 statistical inference and can be shown to generalise
                 PCA. We will briefly introduce the model and quickly
                 show some example applications.},
  venue =  	 {Workshop on Probabilistic formulation of the inverse problem and application to image reconstruction, Neuroscience Research Institute, University of Manchester, U.K.},
  linkpdf =	 shefftp # {gplvm_inverse_09_07.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2007,
  month =  	 9,
  day =  	 13,
  group =  	 {gplvm}
}

@Talk{Lawrence:uc3mivm07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Fast Sparse {G}aussian Process Methods: The Informative Vector Machine},
  abstract = 	 {Gaussian processes are a non parametric approach to learning
                 regression models. In this talk we will given a brief
                 review of the use of Gaussian processes for
                 regression. We will then introduce the informative
                 vector machine approach to learning Gaussian
                 processes for Classification on large scale data
                 sets. We will show extensions of the method including
                 multi-task learning, semi-supervised learning and
                 learning invariances.},
  venue =  	 {Dept of Signal Theory and Communications, Universidad Carlos III de Madrid, Spain},
  label1 =  	 {IVM Software},
  link1 =	 softwarehttp # {ivm/},
  label2 =  	 {IVM C++ Software},
  link2 =	 sheffieldgit # {GPc/},
  linkpdf =	 shefftp # {gpivm_07_07.pdf},
  year =  	 2007,
  month =  	 7,
  day = 3
}
@Talk{Lawrence:uc3tfa07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Inference for Modelling of Transcription Factor Activity},
  abstract = 	 {Accurate modelling of transcriptional processes in the
                 cell requires the knowledge of a number of key
                 biological quantities. In practice many of them are
                 difficult to measure in vivo. For example, it is very
                 hard to measure the active concentration levels of
                 the transcription factor proteins that drive the
                 process.\\\\

                 In this talk we will show how, by making use of
                 structural information about the interaction network
                 (e.g. arising form ChIP-chip data), transcription
                 factor activities can estimated using probabilistic
                 inference. We propose two different probabilistic
                 models: a simple linear model with Kalman filter
                 based dynamics for genome/transcriptome wide studies
                 and a differential equation based Gaussian process
                 model with a more physically realistic
                 parameterisation for smaller interaction networks.},
  venue =  	 {Dept of Signal Theory and Communications, Universidad Carlos III de Madrid, Spain},
  label1 =	 {GPSIM Software},
  link1 =	 softwarehttp # {gpsim/},
  label2 = 	 {TFA Software},
  link2 =	 sheffieldgit # {chipdyno/},
  label3 = 	 {TFA Software II},
  link3 =	 softwarehttp # {chipvar/},
  label4 = 	 {PUMA Software},
  link4 = 	 {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  linkpdf =	 shefftp # {puma_07_07.pdf},
  year =  	 2007,
  month =  	 7,
  day = 5
}


@Talk{Lawrence:uc3mgplvm07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Dept of Signal Theory and Communications, Universidad Carlos III de Madrid, Spain},
  linkpdf =	 shefftp # {gplvm_07_07.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2007,
  month =  	 7,
  day = 4,
  group =  	 {gplvm}
}

@Talk{Lawrence:mathbio07,
  author =  	 {Neil D. Lawrence},
  title =  	 {{G}aussian Processes for Inference in Biological Interaction Networks},
  abstract = 	 {In many biological applications key functions of interest,
                 such as chemical species concentrations, are
                 unobserved.  In this talk we will briefly introduce
                 Gaussian processes, which are probabilistic models of
                 functions. We will show how they can be used, in
                 combination with a simple differential equation
                 model, to estimate the concentration of a
                 transcription factor in a simple single input module
                 network motif.},
  venue =  	 {Exploring the Interface Between Mathematics and Bioscience, Manchester Interdisciplinary Biocentre, University of Manchester, U.K.},
  linksoftware =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpsim_07_04.pdf},
  year =  	 2007,
  month =  	 4,
  day = 4
}
@Talk{Lawrence:pesb07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Modelling Transcriptional Regulation with {G}aussian Processes},
  abstract = 	 {Modelling the dynamics of transcriptional processes in the
                 cell requires the knowledge of a number of key
                 biological quantities. While some of them are
                 relatively easy to measure, such as mRNA decay rates
                 and mRNA abundance levels, it is still very hard to
                 measure the active concentration levels of the
                 transcription factor proteins that drive the process
                 and the sensitivity of target genes to these
                 concentrations. In this paper we show how these
                 quantities for a given transcription factor can be
                 inferred from gene expression levels of a set of
                 known target genes. We treat the protein
                 concentration as a latent function with a Gaussian
                 process prior, and include the sensitivities, mRNA
                 decay rates and baseline expression levels as
                 hyperparameters. We apply this procedure to a human
                 leukemia dataset, focusing on the tumour repressor
                 p53 and obtaining results in good accordance with
                 recent biological studies.},
  venue =  	 {Parameter Estimation in Systems Biology, School of Computer Science, University of Manchester, U.K.},
  linksoftware =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpsim_07_03.pdf},
  videolectures = {pesb07_lawrence_mtr},
  linkvideo =	 {http://videolectures.net/pesb07_lawrence_mtr/},
  year =  	 2007,
  month =  	 3,
  day =   	 28
}

@Talk{Lawrence:icml07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Hierarchical {G}aussian Process Latent Variable Models},
  abstract = 	 {The Gaussian process latent variable model (GP-LVM) is a
                 powerful approach for probabilistic modelling of high
                 dimensional data through dimensional reduction. In
                 this paper we extend the GP-LVM through
                 hierarchies. A hierarchical model (such as a tree)
                 allows us to express conditional independencies in
                 the data as well as the manifold structure. We first
                 introduce Gaussian process hierarchies through a
                 simple dynamical model, we then extend the approach
                 to a more complex hierarchy which is applied to the
                 visualisation of human motion data sets.},
  venue =  	 {ICML, Corvallis, Oregon},
  linkpdf =	 shefftp # {hgplvm_07_06.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {hgplvm/},
  year =  	 2007,
  month =  	 6,
  day = 22,
  group =  	 {gplvm}
}
@Talk{Lawrence:ncrg07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Neural Computing Research Group, Aston University, U.K.},
  linkpdf =	 shefftp # {gplvm_07_02.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2007,
  month =  	 3,
  day =          9,
  group =  	 {gplvm}
}
@Talk{Lawrence:google07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Google Research, New York, N.Y., U.S.A.},
  linkvideo =  	 {http://video.google.com/videoplay?docid=-5127068978792458641},
  youtube = {DS853uA0u4I},
  linkpdf =	 shefftp # {gplvm_07_02.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2007,
  month =  	 2,
  day = 12,
  group =  	 {gplvm}
}
@Talk{Lawrence:csail07,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Boston, MA, U.S.A.},
  linkpdf =	 shefftp # {gplvm_07_02.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  label3 =	 {Seminar Page},
  link3 =	 {http://www.eecs.mit.edu/cgi-bin/calendar.cgi?page=AY06-07/data/106.dat},
  year =  	 2007,
  month =  	 2,
  day = 9,
  group =  	 {gplvm}
}
@Talk{Lawrence:manchesterGuest06,
  author =  	 {Neil D. Lawrence},
  title =  	 {Learning and Inference with {G}aussian Processes: An Overview of {G}aussian Processes and the {GP-LVM}},
  venue =  	 {University of Manchester, Machine Learning Course Guest Lecture},
  label1 =	 {Demos Software},
  link1 =        softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =        sheffieldgit # {GPmat/},
  linkpdf =	 shefftp # {gpGuestLecture.pdf},
  year =  	 2006,
  month =  	 11,
  day = 3,
  group =  	 {gplvm}
}
@Talk{Lawrence:latentFunc08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Inferring Latent Functions with {G}aussian Processes in Differential Equations},
  venue =  	 {Department of Electronics and Computer Science, University of Southampton, U.K.},
  linkpdf =	 shefftp # {gpSysBio.pdf},
  label1 = 	 {Seminar Page},
  link1 = 	 {http://www.isis.ecs.soton.ac.uk/seminars/?date=20080430},
  year =  	 2008,
  month =  	 4,
  day =   	 30,
  abstract =   	 {In this talk we will present recent work from Manchester in
                 inference of latent functions in differential
                 equations.  Simple computational models for systems
                 biology make use of ordinary differential equations
                 that are driven from an often unobserved input
                 function. We will describe how probabilistic
                 inference over these latent functions may be
                 performed through Gaussian process prior
                 distributions. We will describe the algorithms and
                 show results on toy problems and real biological
                 systems.},
  group =  	 {gp}
}
@Talk{Lawrence:sysbioIntroA08,
  author =  	 {Neil D. Lawrence},
  title =  	 {An Introduction to Systems Biology from a Machine Learning Perspective},
  venue =  	 {Max Planck Society, Ringberg Castle, Germany},
  linkpdf =	 shefftp # {mpi.pdf},
  OPTlabel1 = 	 {},
  OPTlink1 = 	 {},
  year =  	 2008,
  month =  	 5,
  day =   	 5,
  abstract =   	 {In this talk we will introduce some of the challenges in
                 systems biology and discuss the efforts being made to
                 address them using statistical inference. General
                 biological background will be interlaced with case
                 studies that illustrate the salient issues in systems
                 biology from the perspective of a machine learning
                 researcher.},
  group =  	 {sysbio}
}

@Talk{Lawrence:sysbioIntroB08,
  author =  	 {Neil D. Lawrence},
  title =  	 {Statistical Inference in Systems Biology through {G}aussian Processes and Ordinary Differential Equations},
  venue =  	 {Max Planck Society, Ringberg Castle, Germany},
  linkpdf =	 shefftp # {mpi2.pdf},
  OPTlabel1 = 	 {},
  OPTlink1 = 	 {},
  year =  	 2008,
  month =  	 5,
  day =   	 7,
  abstract =   	 {In this talk we will summarise recent work from our group
                 in Manchester on inferring `latent biochemical
                 species' in biological systems using Gaussian
                 processes and differential equations. A key problem
                 in biological data is when particular biochemical
                 species of interest are not directly measurable. We
                 will show how the framework of Gaussian processes can
                 be brought to bear on the problem and values of
                 latent chemical species can be inferred given data
                 and a differential equation model.},
  group =  	 {gp,sysbio}
}
@Talk{Lawrence:gpdc08,
  author =  	 {Neil D. Lawrence},
  title =  	 {{GP-LVM} for Data Consolidation},
  venue =  	 {NIPS Workshop on Learning from Multiple Sources},
  videolectures = 	 {lms08_lawrence_gpdc},
  year =  	 2008,
  month =  	 12,
  day =   	 20,
  abstract =   	 {},
  group =  	 {gp,sysbio}
}

@Talk{Lawrence:tuebingen06,
  author =  	 {Neil D. Lawrence},
  title =  	 pumaTitle1,
  abstract = 	 pumaAbstract1,
  venue =  	 {Max Planck Institute, TÃ¼bingen, Germany},
  label1 =	 {PUMA Project Page},
  link1 =	 {http://bioinf.man.ac.uk/resources/puma/},
  label2 =	 {NPPCA Software},
  link2 =	 softwarehttp # {nppca/},
  label3 =	 {ChipDyno Software},
  link3 =	 sheffieldgit # {chipdyno/},
  label4 =	 {ChipVar Software},
  link4 =	 softwarehttp # {chipvar/},
  label5 =	 {GP p53 Software},
  link5 =	 softwarehttp # {gpsim/},
  label6 = 	 {PUMA Software},
  link6 = 	 {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  linkpdf =	 shefftp # {puma_06_08.pdf},
  year =  	 2006,
  month =  	 8,
  day = 2
}
@Talk{Lawrence:intel06,
  author =  	 {Neil D. Lawrence},
  title =  	 gpTitle1,
  abstract = 	 gpAbstract1,
  venue =  	 {Intel Research, Seattle, U.S.A.},
  label1 =	 {p53 Software},
  link1 =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpintroTalk_06_08.pdf},
  linksoftware =	 softwarehttp # {oxford/},
  year =  	 2006,
  month =  	 8,
  day = 21,
}

@Talk{Lawrence:erice06,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Erice Workshop on Mathematics and Medical Diagnosis, Sicily, Italy},
  linkpdf =	 shefftp # {gplvm_06_07.pdf},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2006,
  month =  	 7,
  day = 11,
  group =  	 {gplvm}
}


@Talk{Lawrence:manchester06,
  author =  	 {Neil D. Lawrence},
  title =  	 gpTitle1,
  abstract = 	 gpAbstract1,

  venue =  	 {School of Computer Science, University of Manchester, U.K.},
  label1 =	 {p53 Software},
  link1 =	 softwarehttp # {gpsim/},
  linkpdf =	 shefftp # {gpintroTalk_06_06.pdf},
  linksoftware =	 softwarehttp # {oxford/},
  year =  	 2006,
  month =  	 6,
  day = 22
}
@Talk{Lawrence:icml06,
  author =  	 {Neil D. Lawrence},
  title =  	 {Local Distance Preservation in the GP-LVM through Back Constraints},
  venue =  	 {International Conference on Machine Learning, Pittsburgh, U.S.A.},
  linkpdf =	 shefftp # {backConstraintsBeamer.pdf},
  linksoftware =	 sheffieldgit # {GPmat/},
  year =  	 2006,
  month =  	 6,
  day = 27,
  group =  	 {gplvm}
}
@Talk{Sanguinetti:masamb06,
  author =  	 {Guido Sanguinetti},
  title =  	 {A probabilistic dynamical model for quantitative inference of the regulatory mechanism of transcription},
  venue =  	 {Mathematical and Statistical Aspects of Molecular Biology},
  linkpptgz =	 shefftp # {MASAMB06.ppt.gz},
  label1 =	 {Model One Software},
  link1 =	 sheffieldgit # {chipdyno/},
  label2 =	 {Model Two  Software},
  link2 =	 softwarehttp # {chipchip/},
  year =  	 2006,
  group =	 {shefml,puma,gene networks},
  month =  	 4,
  day = 10
}
@Talk{Lawrence:cued06,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle4,
  abstract = 	 gplvmAbstract4,
  venue =  	 {Information Engineering, University of Cambridge, U.K.},
  linkpptgz =	 shefftp # {gplvm_06_03.ppt.gz},
  label1 =	 {Demos Software},
  link1 =	 softwarehttp # {oxford/},
  label2 =	 {Main Software},
  link2 =	 sheffieldgit # {GPmat/},
  year =  	 2006,
  month =  	 3,
  day = 7,
  group =  	 {gplvm}
}
@Talk{Lawrence:oxford06,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle3,
  abstract = 	 gplvmAbstract3,
  venue =  	 {Computer Vision Reading Group, Visual Geometry Group, Department of Engineering Science, University of Oxford, U.K.},
  label1 =	 {PDF Slides},
  link1 =	 shefftp # {gplvmTutorialSlides.pdf},
  label2 =	 {PDF Notes},
  link2 =	 shefftp # {gplvmTutorial.pdf},
  label3 =	 {Demos Software},
  link3 =        softwarehttp # {oxford/},
  label4 =	 {Main Software},
  link4 =        sheffieldgit # {GPmat/},
  year =  	 2006,
  month =  	 1,
  day = 27,
  group =  	 {gplvm}
}
@Talk{Lawrence::uw05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {University of Washington, Seattle, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005,
  month = 12,
  day = 14,
  group =  	 {gplvm}
}
@Talk{Lawrence::msrred05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Microsoft Research, Redmond, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005,
  month =  	 12,
  day = 12,
  group =  	 {gplvm}
}
@Talk{Lawrence::ubc05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Electronic Arts Speaker Series, University of British Columbia, Canada},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005,
  month =  	 12,
  day = 2,
  group =  	 {gplvm}
}
@Talk{Lawrence::columbia05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Columbia University, New York, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005,
  month =  	 11,
  day = 29,
  group =  	 {gplvm}
}
@Talk{Lawrence::ibm05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {IBM Thomas J Watson Research Center, New York, U.S.A.},
  linkpptgz =	 shefftp # {gplvm_05_11.ppt.gz},
  year =  	 2005,
  month =  	 11,
  day = 28,
  group =  	 {gplvm}
}

@Talk{Lawrence:gatsby05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {Gatsby Computational Neuroscience Unit, University College London, U.K.},
  linkpptgz =	 shefftp # {gplvm_gatsby.ppt.gz},
  year =  	 2005,
  month =  	 11,
  day = 16,
  group =  	 {gplvm}
}


@Talk{Lawrence:idiap05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {IDIAP Research Institute, Martigny, Switzerland},
  linkpptgz =	 shefftp # {gplvm_epfl.ppt.gz},
  year =  	 2005,
  month =  	 11,
  day = 2,
  group =  	 {gplvm}
}
@Talk{Lawrence:epfl05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle2,
  abstract = 	 gplvmAbstract2,
  venue =  	 {School of Computer and Communication Sciences, Swiss Federal Institute of
                 Technology (EPFL), Lausanne, Switzerland},
  linkpptgz =	 shefftp # {gplvm_epfl.ppt.gz},
  year =  	 2005,
  month =  	 10,
  day = 31,
  group =  	 {gplvm}
}
@Talk{Lawrence:manchester05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,

  venue =  	 {School of Computer Science, University of Manchester, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005,
  month =  	 3,
  day = 9,
  group =  	 {gplvm}
}
@Talk{Lawrence:soton05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Department of Electronics and Computer Science, University of Southampton, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  label1 = 	 {Seminar Page},
  link1 = 	 {http://www.isis.ecs.soton.ac.uk/seminars/?date=20050511},
  year =  	 2005,
  month =  	 5,
  day = 11,
  group =  	 {gplvm}
}
@Talk{Lawrence:msr05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Microsoft Research, Cambridge, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005,
  month =  	 3,
  day = 15,
  group =  	 {gplvm}

}
@Talk{Lawrence:edinburgh05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Institute for Adaptive and Neural Computation, University of Ediburgh, U.K.},
  linkpptgz =	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005,
  month =  	 3,
  day = 1,
  group =  	 {gplvm}

}
@Talk{Lawrence:oxford05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  abstract = 	 gplvmAbstract1,
  venue =  	 {Robotics Research Group, Department of Engineering Science, University of Oxford, U.K.},
  linkpptgz = 	 shefftp # {long_gplvm.ppt.gz},
  year =  	 2005,
  month =  	 2,
  day = 21,
  group =  	 {gplvm}
}
@Talk{Lawrence:tuebingen05,
  author =  	 {Neil D. Lawrence},
  title =  	 gplvmTitle1,
  venue =  	 {Max Planck Institute, TÃ¼bingen, Germany},
  linkpptgz = 	 shefftp # {gplvm.ppt.gz},
  year =  	 2005,
  month =  	 8,
  day = 15,
  group =  	 {gplvm}
}

@Talk{Lawrence:msrb03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bayesian Processing of {cDNA} Microarray Images through
                 the Variational Importance Sampler},
  abstract =  	 {Each cell in the human body contains the same basic code
                 in the form of the genome, however cells have differentiated
                 roles which come about through different cells `expressing'
                 different genes.

                 Key insights into gene interactions can be studied through
                 measuring the level of expression of each gene at different
                 times. Gene expression levels can be obtained from cDNA
                 microarray experiments through the extraction of pixel
                 intensities from a scanned image of a slide.

                 In this talk we will start by briefly reviewing cDNA
                 microarray technology. We will then focus on one
                 problem that arises when processing these images:
                 human error in locating the position of the spots can
                 lead to variabilities in the extracted expression
                 levels. We will present a Bayesian approach to the image
                 processing which alleviates this problem. Our
                 approach makes use of a novel combination of
                 importance sampling and variational approximations.

                 Finally if there is time we will briefly show some examples
                 of the variational importance sampler applied to visual
                 tracking problems.},
  venue =  	 {Microsoft Research, Redmond, U.S.A.},
  year =  	 {2003},
  month =  	 12,
  day = 4
}

@Talk{Lawrence:ucbgplvm03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Non-linear Component Analysis through {G}aussian
                 Process Latent Variable Models},
  abstract =  	 {It is known that Principal Component Analysis has an
                 underlying probabilistic representation based on a
                 latent variable model. PCA is recovered when the
                 latent variables are integrated out and the
                 parameters of the model are optimised by maximum
                 likelihood. It is less well known that the dual
                 approach of integrating out the parameters and
                 optimising with respect to the latent variables also
                 leads to PCA. The marginalised likelihood in this
                 case takes the form of Gaussian process mappings,
                 with linear Covariance functions, from a latent space
                 to an observed space, which we refer to as a Gaussian
                 Process Latent Variable Model (GPLVM) \cite{Lawrence:gplvm03} . It is
                 straightforward to `non-linearise' this model by
                 substituting the linear covariance function for a
                 non-linear one. The result is a non-linear
                 probabilistic PCA model.

                 In this talk we will present a practical algorithm
                 for optimising the latent variables in a non-linear
                 GPLVM and discuss some relations with other
                 models. Finally we will present results from a
                 SIGGRAPH paper which uses the GPLVM to learn styles
                 in an inverse kinematics problem \cite{Grochow:styleik04}.},
  venue =  	 {University of California, Berkeley, U.S.A.},
  year =  	 {2004},
  month =  	 5,
  day = 6
}

@Talk{Lawrence:smlwgplvm03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Non-linear Component Analysis through {G}aussian
                 Process Latent Variable Models},
  abstract =  	 {It is known that Principal Component Analysis has an
                 underlying probabilistic representation based on a
                 latent variable model. Principal component analysis
                 (PCA) is recovered when the latent variables are
                 integrated out and the parameters of the model are
                 optimised by maximum likelihood. It is less well
                 known that the dual approach of integrating out the
                 parameters and optimising with respect to the latent
                 variables also leads to PCA. The marginalised
                 likelihood in this case takes the form of Gaussian
                 process mappings, with linear Covariance functions,
                 from a latent space to an observed space, which we
                 refer to as a Gaussian Process Latent Variable Model
                 (GPLVM). This dual probabilistic PCA is still a
                 linear latent variable model, but by looking beyond
                 the inner product kernel as a for a covariance
                 function we can develop a non-linear probabilistic
                 PCA.},
  venue =  	 {Sheffield Machine Learning Workshop, Sheffield, U.K.},
  linkvideo =  	 {mms://velblod2.ijs.si/pascal/2004/sheffield_04/lawrence_neil/lawrence_neil_00.wmv},
  linkvideo =  	 {mms://velblod2.ijs.si/pascal/2004/sheffield_04/lawrence_neil/lawrence_neil_00.wmv},
  year =  	 {2004},
  month =  	 9,
  day = 9,
  linkpptgz =   	 shefftp # {gplvm_smlw.ppt.gz}
}
@Talk{Lawrence:sussex03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bayesian Processing of {cDNA} Microarray Images},
  abstract =  	 {Gene expression levels are obtained from microarray
                 experiments through the extraction of pixel
                 intensities from a scanned image of the slide. It is
                 widely acknowledged that variabilities can occur in
                 expression levels extracted from the same images by
                 different users with the same software
                 packages. These inconsistencies arise due to
                 differences in the refinement of the placement of the
                 microarray 'grids'. We introduce a novel automated
                 approach to the refinement of grid placements that is
                 based upon the use of Bayesian inference for
                 determining the size, shape and positioning of the
                 microarray 'spots', capturing uncertainty that can be
                 passed to downstream analysis.

                 Our experiments demonstrate that variability between
                 users can be significantly reduced using the
                 approach. The automated nature of the approach also
                 saves hours of researchers' time normally spent in
                 refining the grid placement.},
  venue =  	 {The University of Sussex, Department of Cognitive Science,
                 Bioinformatics and Vision Seminars},
  year =  	 {2003},
  month =  	 6,
  day = 20
}
@Talk{Lawrence:manchester03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Bayesian Processing of {cDNA} Microarray Images},
  abstract =  	 {Gene expression levels are obtained from microarray

                 experiments through the extraction of pixel
                 intensities from a scanned image of the slide. It is
                 widely acknowledged that variabilities can occur in
                 expression levels extracted from the same images by
                 different users with the same software
                 packages. These inconsistencies arise due to
                 differences in the refinement of the placement of the
                 microarray `grids'. We introduce a novel automated
                 approach to the refinement of grid placements that is
                 based upon the use of Bayesian inference for
                 determining the size, shape and positioning of the
                 microarray `spots', capturing uncertainty that can be
                 passed to downstream analysis.

                 Our experiments demonstrate that variability between
                 users can be significantly reduced using the
                 approach. The automated nature of the approach also
                 saves hours of researchers' time normally spent in
                 refining the grid placement.

                 A MATLAB implementation of the algorithm is available
                 from
                 \url{http://inverseprobability.com/vis}.},
  venue =  	 {The University of Manchester, Department of Computer Science,
                 Bio-health sciences Seminars},
  year =  	 {2003},
  month =  	 5,
  day = 21
}
@Talk{Lawrence:gatsby07,
  author =  	 {Neil D. Lawrence},
  title =  	 {Probabilistic Inference for Modelling of Transcription Factor Activity},
  link1 =   	 {http://bioinf.manchester.ac.uk/resources/puma/},
  label1 =   	 {Project Page},
  label2 =	 {NPPCA Software},
  link2 =	 softwarehttp # {nppca/},
  label3 =	 {ChipDyno Software},
  link3 =	 sheffieldgit # {chipdyno/},
  label4 =	 {ChipVar Software},
  link4 =	 softwarehttp # {chipvar/},
  label5 =	 {GP p53 Software},
  link5 =	 softwarehttp # {gpsim/},
  label6 = 	 {PUMA Software},
  link6 = 	 {http://www.bioconductor.org/packages/2.0/bioc/html/puma.html},
  linkpdf =	 shefftp # {puma_07_06.pdf},
  abstract =  	 {Accurate modelling of transcriptional processes in the
                 cell requires the knowledge of a number of key
                 biological quantities. In practice many of them are
                 difficult to measure in vivo. For example, it is very
                 hard to measure the active concentration levels of
                 the transcription factor proteins that drive the
                 process.\\\\

                 In this talk we will show how, by making use of
                 structural information about the interaction network
                 (e.g. arising form ChIP-chip data), transcription
                 factor activities can estimated using probabilistic
                 inference. We propose two different probabilistic
                 models: a simple linear model with Kalman filter
                 based dynamics for genome/transcriptome wide studies
                 and a differential equation based Gaussian process
                 model with a more physically realistic
                 parameterisation for smaller interaction networks.},
  venue =  	 {Gatsby Computational Neuroscience Unit, University College London, U.K.},
  year =  	 {2007},
  month =  	 6,
  day = 13
}
@InCollection{Lawrence:gpncnm05,
  author =	 {Neil D. Lawrence and Michael I. Jordan},
  title =	 {Gaussian Processes and the Null-Category Noise
                  Model},
  crossref =	 {Chapelle:semisuper06},
  abstract =	 {With Gaussian process classifiers (GPC) we aim to
                  predict the posterior probability of the class
                  labels given an input data point, $p(y_i|x_i)$. In
                  general we find that this posterior distribution is
                  unaffected by unlabeled data points during
                  learning. Support vector machines are strongly
                  related to GPCs, but one notable difference is that
                  the decision boundary in an SVM can be influenced by
                  unlabeled data. The source of this discrepancy is
                  the SVM's margin: a characteristic which is not
                  shared with the GPC. The presence of the marchin
                  allows the support vector machine to seek low data
                  density regions for the decision boundary,
                  effectively allowing it to incorporate the cluster
                  assumption (see Chapter 6). In this chapter we
                  present the \emph{null category noise model}. A
                  probabilistic equivalent of the margin. By combining
                  this noise model with a GPC we are able to
                  incorporated the cluster assumption without
                  explicitly modeling the input data density
                  distributions and without a special choice of
                  kernel.},
  pages =	 {152--165},
  linkps =	 shefftp # {gpncnm.ps},
  label1 =	 {MATLAB Software},
  link1 =        sheffieldgit # {GPmat/},
  label2 =	 {C++ Software},
  link2 =	 sheffieldgit # {GPc/}
}

@InProceedings{Hifny:maxent05,
  author =	 {Yasser Hifny and Steve Renals and Neil D. Lawrence},
  title =	 {A Hybrid {MaxEnt/HMM} Based {ASR} System},
  booktitle =	 {Proceedings of Interspeech 2005 --- 9th European
                  Conference on Speech Communication and Technology},
  OPTpages =	 {},
  year =	 2005,
  OPTeditor =	 {},
  OPTvolume =	 {},
  OPTnumber =	 {},
  OPTseries =	 {},
  OPTaddress =	 {},
  day = {4--8},
  month =	 9,
  organization = {Lisbon, Portugal},
  abstract =	 {The aim of this work is to develop a practical
                  framework, which extends the classical Hidden Markov
                  Model (HMM) for continuous speech recognition based
                  on the Maximum Entropy (MaxEnt) principle. The
                  MaxEnt models can estimate the posterior
                  probabilities directly as with Hybrid NN/HMM
                  connectionist speech recogniton systems. In
                  particular, a new acoustic modelling based on
                  discriminative MaxEnt models is formulated and is
                  being developed to replace the generative Gaussian
                  Mixture Models (GMM) commonly used to model acoustic
                  variability. Initial experimental results using the
                  TIMIT phone task are reported.},
  linkpdf =	 shefftp # {hifny-eurospeech05.pdf}
}

@Article{Tipping:variational05,
  author =	 {Michael E. Tipping and Neil D. Lawrence},
  title =	 {Variational inference for {S}tudent-$t$ models:
                  Robust {B}ayesian interpolation and generalised
                  component analysis},
  journal =	 {Neurocomputing},
  year =	 2005,
  OPTkey =	 {},
  volume =	 69,
  OPTnumber =	 {},
  pages =	 {123--141},
  abstract =	 {We demonstrate how a variational approximation
                  scheme enables effective inference of key parameters
                  in probabilisitic signal models which employ the
                  Student-t distribution. Using the two scenarios of
                  previous termrobustnext term interpolation and
                  independent component analysis (ICA) as examples, we
                  illustrate the key feature of the approach: that the
                  form of the noise distribution in the interpolation
                  case, and the source distributions in the ICA case,
                  can be inferred from the data concurrent with all
                  other model parameters.},
  link1 =        {http://authors.elsevier.com/sd/article/S0925231205001694},
  label1 =	 {Access via Science Direct},
  OPTlink2 =	 {},
  OPTlabel2 =	 {}
}

@Talk{Lawrence:msr03,
  author =  	 {Neil D. Lawrence},
  title =  	 {Particle Filters, Variational methods and Importance Sampling},
  abstract =  	 {Particle filters allow tracking of systems with highly
                 non-linear, multi-modal posterior distributions,
                 however they are prone to failure when model
                 likelihoods are sharply peaked or state spaces are
                 high dimensional. This failure is caused by a
                 mismatch between the proposal distribution and the
                 true posterior. The number of particles of samples
                 then required to accurately represent the posterior
                 increases dramatically and with it the computational
                 demands of the algorithm.

                 By formulating the problem within the framework of
                 variational inference we derive an algorithm in which
                 the proposal naturally adapts to more accurately
                 reflect the true posterior. This is achieved by
                 replacing intractable moment evaluations, arising
                 from the highly non-linear nature of the likelihood
                 functions, with sample based approximations.

                 In this talk we shall first introduce the approach in
                 a static setting: Bayesian processing of cDNA
                 microarray images.  We will then add dynamics to the
                 model and demonstrate a marked improvement over
                 standard approaches on both synthetic and real-world
                 tracking examples.},
  venue =  	 {Machine Learning and Perception Group, Microsoft Research,
                 Cambridge, U.K.},
  year =  	 {2003},
  month =  	 3,
  day = 24
}
@TechReport{King:ppa05,
  author =	 {Nathaniel J. King and Neil D. Lawrence},
  title =	 {Variational Inference in {G}aussian Processes via
                  Probabilistic Point Assimilation},
  institution =	 sheftech,
  year =	 2005,
  number =	 {CS-05-06},
  linkpsgz =	 shefftp # {ppatech.ps.gz},
  link2 =	 softwarehttp # {kpca/},
  label2 =	 {Software},
  group =	 {shefml},
  abstract =	 {We introduce a novel variational approach
                  for approximate inference in Gaussian process (GP)
                  models. The key advantages of our approach are the
                  ease with which different noise models can be
                  incorporated and improved speed of convergence. We
                  refer to the algorithm as probabilistic point
                  assimilation (PPA). We introduce the algorithm
                  firstly using the `weight space' view and then
                  through its Gaussian process formulation. We
                  illustrate the approach on several benchmark data
                  sets.}
}

@PhdProject{Health:14,
  author = {Neil D. Lawrence},
title = {Machine Learning Methodologies for Personalized Health},
abstract = {It is now possible to have several perspectives on a patient. mHealth provides information derived from mobile phones. Full genotyping of patients is becoming affordable, providing information about genetic background. The phenotype of disease is becoming better characterised than ever before. Techniques such as transcriptome analysis allow a highly detailed characterization of the state of a tissue. Finally the UK government's midata initiative (and similar initiatives elsewhere) may eventually allow patients to provide information about their consumer spending habits as well as social network behaviour.\\\\

These different data modalities need to be combined into one model of the patients well being. There are major challenges with doing this: models need to be applied across millions of patients and for any given patient many information modalities will be missing. Addressing data of this type requires new machine learning methodologies. This project will focus on combining data from different modalities within the same probabilistic model.}
}
@PhdProject{Animation:10,
  author = {Neil D. Lawrence},
  title = {Animation by Machine Learning with Motion Capture Data},
  abstract = {This project is about using Machine Learning to model human motion for animation, with a particular focus on the demands of computer games. The idea is to learn what natural motion looks like, and then combine it with constraints to develop an animation sequence. The constraints could be animator imposed, or imposed by the computer game. A typical scenario might be that the player's character is required to interact with a character in the game, for example a player might be given an object in the game. The constraint could be that the hands of the player touch the hands of the character giving the object. With the current approach to animation (looking up a library of motion capture data) such a constraint is very difficult to fulfill as the required motion won't exactly match a sequence in the library. By modelling natural motion through machine learning, we should be able to generate a new sequence to satisfy the constraint.\\\\

The models of motion will be developed using Gaussian processes. In particular the project will make use of the "Gaussian Process Latent Variable Model" (Lawrence, 2003) which has already shown a lot of promise in this domain, and "Latent Force Models" (Alvarez et al, 2009) a recently developed approach to learning based on physics and probabilistic models. \\\\

The project will involve a large amount of mathematics, in particular advanced linear algebra and calculus.}
}

@PhdProject{Mechanistic:10,
  author = {Neil D. Lawrence},
  title = {Data Driven Mechanistic Models for Systems Biology},
  abstract = {Models of biological systems, such as gene networks, are useful in
extracting meaning from quantitative data obtained from specific biological systems. In this PhD proposal we are interested in how things actually work in real observable biological systems. To do this we will make use of machine learning to infer mechanistic models biological systems. \\\\

Biological systems are immensely complex, and extracting all the data necessary to characterize the system is often impossible. It is therefore important to exploit other sources of information when modelling biological systems. Once such source of information is "mechanistic models". These are models of the underlying physical properties of the system. In this project we will ensure that such physical models can be easily combined with data driven machine learning approaches, aiming to obtain the best of both worlds: mechanistic modelling and data driven machine learning models. \\\\

The project will involve a large amount of mathematics, in particular advanced linear algebra and calculus.}
}

@PhdProject{Disease:10,
  author = {Neil D. Lawrence},
  year = 2010,
  title = {Inferring Complex Hidden Causes of Disease through Probabilistic Models},
  abstract = {The increased ability to measure individuals' genetic profiles ( through single nucleotide polymorphisms), combined with the ability to characterize a disease activity through gene expression, and other biomarkers, should reveal more realistically complex webs of causal factors for disease. Understanding these causal factors would enable personalised interventions targeted to an individual's genetic, environmental (including concurrent treatments) and treatment preference profile. \\\\

This vision is challenging on two fronts: first, integration of different sources of (genomic) biological data is not straightforward; second, environments are not artificially controlled. In practice, disease occurrence and progression is often triggered by a combination of genetic and environmental factors. Environmental factors need to be assimilated with the genetic background (through the genomic data) and placed in a unified modelling framework to characterize the disease. \\\\

The objective of this project is to address these issues. Our aim is to perform statistical inference from these models to enable us to resolve the determinants of a given disease and its responses to treatments. The research will involve amalgamation of several different research areas, covering health, biology, computational and statistical inference. \\\\

The project will involve a large amount of mathematics, in particular probability theory and advanced linear algebra.}
}


@BscProject{Psychoc:14,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-1},
  year = 2014,
  title = {Psychoc: The Artificial Psychic},
  abstract = {In this project we will build an artificial psychic called psychoc. The artificial psychic will work by querying a user on preferences about life (e.g. movies) and making predictions about what type of person the user is. Psychoc could either be a web interface or a mobile phone app, but the main initial task will be to build psychoc's information engine. Initially psychoc won't be a very good artificial psychic (its information engine will be a little rusty), but over time psychoc should be able to make good predictions about people using only a little information. Software for the project will be written according to the principles of open data science.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}


@BscProject{CitizenMe:14,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-2},
  year = 2014,
  title = {Verifying Identity},
  abstract = {How can you tell if a user is who they say they are? How can you tell if they are a real person or a bot? Can you do it without having the user reveal their inforamtion to you. An individual has the right to privacy, but what if they abuse that right to commit fraud? In this project (in collaboration with a start up company) we will consider how machine learning can be used to balance the need of the individual for privacy agains the need of society to be able to validate identity. Our aim is to build distributed user indenity validation systems that do not require the user to reveal personal information. We will do this by designing intelligent, machine learning based, agents that validate a user's information locally on the telephone. The project may involve collaboration with a London based start up company operating in this area.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{FormulaOneData:12,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-3},
  year = 2014,
  title = {Machine Learning for Modelling Formula One Races},
  abstract = {The machine learning group is working with one of the leading forumla one teams in analysis of data generated in Formula One races with the aim of improving strategy. With this aim we are running one or more projects this year focussed on Formula One data. Formula one is a data intensive sport, information about the location of each team's car during the race is provided to the teams. Optimization of pit stop strategy can make the difference between winning and loosing the race.\\\\

There are commercial confidentiality issues over which areas will be studied, but interested students can discuss these areas directly with Professor Lawrence.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{AeroCycling:13,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-4},
  year = 2014,
  title = {Monitoring Aerodynamic Performance Real Time for Cycling},
  abstract = {A cyclist's aerodynamic position has a very strong affect on their performance. In professional cycling, extensive wind tunnel testing is used to hone a cyclist's performance. Such testing is, however, highly expensive. In this project you will use machine learning techniques alongside the physics of cycling to estimate the aerodynamic performance of a cyclist real time whilst on a bicycle. By combining an anemometer, a power meter and an understanding of the rider's kinetic and gravitational potential energy the power loss due to aerodynamic drag can be estimated. Software for the project will be written according to the principles of open data science.

\\\\Note that field experiments will require access to a road bicycle (power loss due to rolling resistance on a mountain bicycle is too large) and some form of GPS device (for preliminary experiments a smart phone is likely sufficient).

\\\\This project will suit students with strong analytical (mathematical) skills.}}



@BScProject{CyclingData:14,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-5},
  year = 2014,
  title = {Machine Learning for Fitness Monitoring with Python},
  abstract = {Technologies that were previously only available to elite athletes are becoming widespread. Now casual athletes can buy systems that monitor pace, heart rate and other information for under 300 pounds. This project will build a software tool for analysis of data of this type. Software for the project will be written according to the principles of open data science.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{PythonMocap:12,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-6},
  year = 2014,
  title = {Motion Capture Data Modelling in Python},
  abstract = {The Python programming language is becoming a de facto standard for implementation of machine learning algorithms. This project will develop tools for modelling of motion capture data in the Python programming language. Based on existing tools in MATLAB, the aim of the project will be to port the underlying machine learning techniques to the more powerful Python programming language. The end aim is to provide a simple tool for animators to model motion capture data and create new animations for computer games or the film industry.\\\\

A previous project has built a visualization framework for motion capture data, this year's project will focus on more advanced machine learning methodologies. The student will work with an ongoing software development framework being developed by the machine learning group. Software for the project will be written according to the principles of open data science.

\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{ActivityRecognition:12,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-7},
  year = 2014,
  title = {Gesture Recognition using Kinect and Python},
  abstract = {Kinect cameras provide true image and an associated depth image. In this project the focus will be on data from the Gesture Recognition Challenge for kinect: \url{http://www.kaggle.com/c/GestureChallenge/}. The student will participate in the challenge using state of the art machine learning techniques with the assistance of the Sheffield Machine Learning group. A gesture recognizer for the Kinect would enable a large range of new interfaces between the human and computer. Software for the project will be written according to the principles of open data science.


\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}

@BScProject{UnderstandingDepthFromImages:12,
  author = {Neil D. Lawrence},
  projectNumber = {NDL-8},
  year = 2014,
  title = {Learning Depth Perception using Kinect and Python},
  abstract = {Kinect cameras provide true image and an associated depth image. These two images are providing different information, yet a human can infer depth directly from an image. This project will focus on using machine learning techniques building on the machine learning groups python code to see what can be learnt about depth from images. The ultimate aim will be to reconstruct the depth in a real image by learning about depths from information provided by the Kinect camera. Software for the project will be written according to the principles of open data science. 


\\\\This project will suit students with strong analytical skills, there will be a focus on linear algebra and probabilistic inference in the software.}
}


