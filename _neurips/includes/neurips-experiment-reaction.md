\ifndef{neuripsExperimentReaction}
\define{neuripsExperimentReaction}

\editme

\subsection{Reaction After Experiment}

\notes{There seems to have been a lot of discussion of the result, both at the conference and on bulletin boards since. Such discussion is to be encouraged, and for ease of memory, it is worth pointing out that the approximate proportions of papers in each category can be nicely divided in to eigths as follows. Accept-Accept 1 in 8 papers, Accept-Reject 3 in 8 papers, Reject-Reject, 5 in 8 papers. This makes the statistics we've computed above: inconsistency 1 in 4 (25%) accept precision 1 in 2 (50%) reject precision 5 in 6 (83%) and agreed accept rate of 1 in 6 (20%). This compares with the accept rate of 1 in 4.}


* Public reaction after experiment [documented here](http://inverseprobability.com/2015/01/16/blogs-on-the-nips-experiment/)

* [Open Data Science](http://inverseprobability.com/2014/07/01/open-data-science/) (see Heidelberg Meeting)

* NIPS was run in a very open way. [Code](https://github.com/sods/conference) and [blog posts](http://inverseprobability.com/2014/12/16/the-nips-experiment/) all available! 

* Reaction triggered by [this blog post](http://blog.mrtz.org/2014/12/15/the-nips-experiment.html). 

\notes{Much of the discussion speculates on the number of consistent accepts in the process (using the main conference accept rate as a proxy). It therefore produces numbers that don't match ours above. This is because the computed accept rate of the individual committees is different from that of the main conference. This could be due to a bias for the duplicated papers, or statistical sampling error. We look at these questions below. First, to get the reader primed for thinking about these numbers we discuss some context for placing these numbers.}

\endif
