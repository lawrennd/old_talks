---
title: "Special Topics: Gaussian Processes"
venue: "University of Sheffield"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Sheffield
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
date: 2015-12-15
published: 2015-12-15
reveal: 2015-12-15-gaussian-processes.slides.html
ipynb: 2015-12-15-gaussian-processes.ipynb
youtube: "B2XhFoCehy8"
layout: talk
categories:
- notes
---


<div style="display:none">
  $${% include talk-notation.tex %}$$
</div>

<script src="/talks/figure-magnify.js"></script>
<script src="/talks/figure-animate.js"></script>
    
<div id="modal-frame" class="modal">
  <span class="close" onclick="closeMagnify()">&times;</span>
  <div class="modal-figure">
    <div class="figure-frame">
      <div class="modal-content" id="modal01"></div>
      <!--<img class="modal-content" id="object01">-->
    </div>
    <div class="caption-frame" id="modal-caption"></div>
  </div>
</div>	  

<!-- Front matter -->
<!-- Front matter -->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!--Back matter-->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<h2 id="review">Review</h2>
<ul>
<li>Last week: Logistic Regression and Generalised Linear Models</li>
<li>Introduced link functions and different transformations.</li>
<li>Showed examples in classification and mentioned possibilities for disease rate models.</li>
<li>This week:
<ul>
<li>Gaussian Processes: non parametric Bayesian modelling }</li>
</ul></li>
</ul>
<p>Over the last two sessions we’ve begun considering classification models and logistic regresssion. In particular, for naive Bayes, we considered a set of assumptions that allowed us to build a joint model of our data set. In particular for naive Bayes we specified</p>
<ol type="1">
<li>Data conditional independence.</li>
<li>Feature conditional independence.</li>
<li>Marginal likelihood of labels was Bernoulli distributed.</li>
</ol>
<p>This allowed us to specify the joint density of our labels and our input data, <span class="math inline">$p(\dataVector, \inputMatrix|\boldsymbol{\theta})$</span>. And we conditioned on the training data to make predictions about the test data.</p>
<h2 id="generalized-linear-models">Generalized Linear Models</h2>
<p>Logistic regression is part of a wider class of models known as <em>generalized linear models</em>. In these models we determine that some characteristic of the model is speicified by a function that is liniear in the parameters. So we might suggest that <br /><span class="math display">$$
\log \frac{p(\inputVector)}{1-p(\inputVector)} = \mappingFunction(\inputVector; \mappingVector)
$$</span><br /> where <span class="math inline">$\mappingFunction(\inputVector; \mappingVector)$</span> is a linear-in-the-parameters function (here the parameters are <span class="math inline">$\mappingVector$</span>, which is generally non-linear in the inputs. So far we have considered basis function models of the form <br /><span class="math display">$$
\mappingFunction(\inputVector) =
\mappingVector^\top \basisVector(\inputVector).
$$</span><br /> When we form a Gaussian process we do something that is slightly more akin to the naive Bayes approach, but actually is closely related to the generalized linear model approach.</p>
<h2 id="gaussian-processes-edit">Gaussian Processes <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-lectures.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-lectures.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>Models where we model the entire joint distribution of our training data, <span class="math inline">$p(\dataVector, \inputMatrix)$</span> are sometimes described as <em>generative models</em>. Because we can use sampling to generate data sets that represent all our assumptions. However, as we discussed in the sessions on  and , this can be a bad idea, because if our assumptions are wrong then we can make poor predictions. We can try to make more complex assumptions about data to alleviate the problem, but then this typically leads to challenges for tractable application of the sum and rules of probability that are needed to compute the relevant marginal and conditional densities. If we know the form of the question we wish to answer then we typically try and represent that directly, through <span class="math inline">$p(\dataVector|\inputMatrix)$</span>. In practice, we also have been making assumptions of conditional independence given the model parameters, <br /><span class="math display">$$
p(\dataVector|\inputMatrix, \mappingVector) =
\prod_{i=1}^{\numData} p(\dataScalar_i | \inputVector_i, \mappingVector)
$$</span><br /> Gaussian processes are <em>not</em> normally considered to be <em>generative models</em>, but we will be much more interested in the principles of conditioning in Gaussian processes because we will use conditioning to make predictions between our test and training data. We will avoid the data conditional indpendence assumption in favour of a richer assumption about the data, in a Gaussian process we assume data is <em>jointly Gaussian</em> with a particular mean and covariance, <br /><span class="math display">$$
\dataVector|\inputMatrix \sim \gaussianSamp{\mathbf{m}(\inputMatrix)}{\kernelMatrix(\inputMatrix)},
$$</span><br /> where the conditioning is on the inputs <span class="math inline">$\inputMatrix$</span> which are used for computing the mean and covariance. For this reason they are known as mean and covariance functions.</p>
<h2 id="prediction-across-two-points-with-gps-edit">Prediction Across Two Points with GPs <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gptwopointpred.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gptwopointpred.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">np.random.seed(<span class="dv">4949</span>)</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="im">import</span> pods</a></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;two_point_sample</span><span class="sc">{sample:0&gt;3}</span><span class="st">.svg&#39;</span>, </a>
<a class="sourceLine" id="cb4-2" data-line-number="2">                            <span class="st">&#39;../slides/diagrams/gp&#39;</span>, </a>
<a class="sourceLine" id="cb4-3" data-line-number="3">                            sample<span class="op">=</span>IntSlider(<span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-two-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-two&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="two-point-sample-one-two-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_2$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_2$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<ul>
<li>The single contour of the Gaussian density represents the <font color="black">joint distribution, <span class="math inline">$p(\mappingFunction_1, \mappingFunction_2)$</span></font></li>
</ul>
<div class="incremental">
<ul>
<li>We observe that <font color="black"><span class="math inline">$\mappingFunction_1=?$</span></font></li>
</ul>
</div>
<div class="incremental">
<ul>
<li><p>Conditional density: <font color="black"><span class="math inline">$p(\mappingFunction_2|\mappingFunction_1=?)$</span></font></p></li>
<li><p>Prediction of <span class="math inline">$\mappingFunction_2$</span> from <span class="math inline">$\mappingFunction_1$</span> requires <em>conditional density</em>.</p></li>
<li><p>Conditional density is <em>also</em> Gaussian. <br /><span class="math display">$$
p(\mappingFunction_2|\mappingFunction_1) = {\mathcal{N}\left(\mappingFunction_2|\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1,\kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}\right)}
$$</span><br /> where covariance of joint density is given by <br /><span class="math display">$$
\kernelMatrix= \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}\end{bmatrix}
$$</span><br /></p></li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;two_point_sample</span><span class="sc">{sample:0&gt;3}</span><span class="st">.svg&#39;</span>, </a>
<a class="sourceLine" id="cb6-2" data-line-number="2">                            <span class="st">&#39;../slides/diagrams/gp&#39;</span>, </a>
<a class="sourceLine" id="cb6-3" data-line-number="3">                            sample<span class="op">=</span>IntSlider(<span class="dv">13</span>, <span class="dv">13</span>, <span class="dv">17</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<div id="two-point-sample-13-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-13-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-13&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="two-point-sample-13-caption" class="caption-frame">
<p>Figure: Sample from the joint Gaussian model, points indexed by 1 and 8 highlighted.</p>
</div>
</div>
<div class="figure">
<div id="two-point-sample-one-eight-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
<div id="two-point-sample-one-eight-magnify" class="magnify" onclick="magnifyFigure(&#39;two-point-sample-one-eight&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="two-point-sample-one-eight-caption" class="caption-frame">
<p>Figure: The joint Gaussian over <span class="math inline">$\mappingFunction_1$</span> and <span class="math inline">$\mappingFunction_8$</span> along with the conditional distribution of <span class="math inline">$\mappingFunction_8$</span> given <span class="math inline">$\mappingFunction_1$</span></p>
</div>
</div>
<ul>
<li>The single contour of the Gaussian density represents the <font color="black">joint distribution, <span class="math inline">$p(\mappingFunction_1, \mappingFunction_8)$</span></font></li>
</ul>
</div>
<div class="incremental">
<ul>
<li>We observe a value for <font color="black"><span class="math inline">$\mappingFunction_1=-?$</span></font></li>
</ul>
</div>
<div class="incremental">
<ul>
<li><p>Conditional density: <font color="black"><span class="math inline">$p(\mappingFunction_5|\mappingFunction_1=?)$</span></font>.</p></li>
<li><p>Prediction of <span class="math inline">$\mappingFunctionVector_*$</span> from <span class="math inline">$\mappingFunctionVector$</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <br /><span class="math display">$$
p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|\kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector,\kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}\right)}
$$</span><br /> </large></p></li>
<li><p>Here covariance of joint density is given by <br /><span class="math display">$$
\kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
$$</span><br /></p></li>
<li><p>Prediction of <span class="math inline">$\mappingFunctionVector_*$</span> from <span class="math inline">$\mappingFunctionVector$</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <br /><span class="math display">$$
p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|{\boldsymbol{{\mu}}},\boldsymbol{\Sigma}\right)}
$$</span><br /> <br /><span class="math display">$$
{\boldsymbol{{\mu}}}= \kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector
$$</span><br /> <br /><span class="math display">$$\boldsymbol{\Sigma} = \kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}
$$</span><br /> </large></p></li>
<li><p>Here covariance of joint density is given by <br /><span class="math display">$$
\kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
$$</span><br /></p></li>
</ul>
</div>
<h2 id="marginal-likelihood-edit">Marginal Likelihood <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/gaussian-processes.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/gaussian-processes.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>To understand the Gaussian process we’re going to build on our understanding of the marginal likelihood for Bayesian regression. In the session on  we sampled directly from the weight vector, <span class="math inline">$\mappingVector$</span> and applied it to the basis matrix <span class="math inline">$\basisMatrix$</span> to obtain a sample from the prior and a sample from the posterior. It is often helpful to think of modeling techniques as <em>generative</em> models. To give some thought as to what the process for obtaining data from the model is. From the perspective of Gaussian processes, we want to start by thinking of basis function models, where the parameters are sampled from a prior, but move to thinking about sampling from the marginal likelihood directly.</p>
<h2 id="sampling-from-the-prior">Sampling from the Prior</h2>
<p>The first thing we’ll do is to set up the parameters of the model, these include the parameters of the prior, the parameters of the basis functions and the noise level.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># set prior variance on w</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2">alpha <span class="op">=</span> <span class="fl">4.</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="co"># set the order of the polynomial basis set</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4">degree <span class="op">=</span> <span class="dv">5</span></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"><span class="co"># set the noise variance</span></a>
<a class="sourceLine" id="cb7-6" data-line-number="6">sigma2 <span class="op">=</span> <span class="fl">0.01</span></a></code></pre></div>
<p>Now we have the variance, we can sample from the prior distribution to see what form we are imposing on the functions <em>a priori</em>.</p>
<p>Let’s now compute a range of values to make predictions at, spanning the <em>new</em> space of inputs,</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">def</span> polynomial(x, degree, loc, scale):</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">    degrees <span class="op">=</span> np.arange(degree<span class="op">+</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">    <span class="cf">return</span> ((x<span class="op">-</span>loc)<span class="op">/</span>scale)<span class="op">**</span>degrees</a></code></pre></div>
<p>now let’s build the basis matrices. First we load in the data</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="im">import</span> pods</a></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1">data <span class="op">=</span> pods.datasets.olympic_marathon_men()</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</a></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">loc <span class="op">=</span> <span class="fl">1950.</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2">scale <span class="op">=</span> <span class="fl">100.</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3">num_data <span class="op">=</span> x.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">num_pred_data <span class="op">=</span> <span class="dv">100</span> <span class="co"># how many points to use for plotting predictions</span></a>
<a class="sourceLine" id="cb12-5" data-line-number="5">x_pred <span class="op">=</span> np.linspace(<span class="dv">1880</span>, <span class="dv">2030</span>, num_pred_data)[:, <span class="va">None</span>] <span class="co"># input locations for predictions</span></a>
<a class="sourceLine" id="cb12-6" data-line-number="6">Phi_pred <span class="op">=</span> polynomial(x_pred, degree<span class="op">=</span>degree, loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</a>
<a class="sourceLine" id="cb12-7" data-line-number="7">Phi <span class="op">=</span> polynomial(x, degree<span class="op">=</span>degree, loc<span class="op">=</span>loc, scale<span class="op">=</span>scale)</a></code></pre></div>
<h2 id="weight-space-view">Weight Space View</h2>
<p>To generate typical functional predictions from the model, we need a set of model parameters. We assume that the parameters are drawn independently from a Gaussian density, <br /><span class="math display">$$
\weightVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
$$</span><br /> then we can combine this with the definition of our prediction function <span class="math inline">$\mappingFunction(\inputVector)$</span>, <br /><span class="math display">$$
\mappingFunction(\inputVector) =
\weightVector^\top \basisVector(\inputVector).
$$</span><br /> We can now sample from the prior density to obtain a vector <span class="math inline">$\weightVector$</span> using the function <code>np.random.normal</code> and combine these parameters with our basis to create some samples of what <span class="math inline">$\mappingFunction(\inputVector)$</span> looks like,</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="op">%</span>matplotlib inline</a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">num_samples <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">K <span class="op">=</span> degree<span class="op">+</span><span class="dv">1</span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_samples):</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">    z_vec <span class="op">=</span> np.random.normal(size<span class="op">=</span>(K, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">    w_sample <span class="op">=</span> z_vec<span class="op">*</span>np.sqrt(alpha)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">    f_sample <span class="op">=</span> np.dot(Phi_pred,w_sample)</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">    plt.plot(x_pred, f_sample)</a></code></pre></div>
<h2 id="function-space-view">Function Space View</h2>
<p>The process we have used to generate the samples is a two stage process. To obtain each function, we first generated a sample from the prior, <br /><span class="math display">$$
\weightVector \sim \gaussianSamp{\zerosVector}{\alpha \eye}
$$</span><br /> then if we compose our basis matrix, <span class="math inline">$\basisMatrix$</span> from the basis functions associated with each row then we get, <br /><span class="math display">$$
\basisMatrix = \begin{bmatrix}\basisVector(\inputVector_1) \\ \vdots \\
\basisVector(\inputVector_n)\end{bmatrix}
$$</span><br /> then we can write down the vector of function values, as evaluated at <br /><span class="math display">$$
\mappingFunctionVector = \begin{bmatrix} \mappingFunction_1
\\ \vdots \mappingFunction_n\end{bmatrix}
$$</span><br /> in the form <br /><span class="math display">$$
\mappingFunctionVector = \basisMatrix
\weightVector.
$$</span><br />}</p>
<p>Now we can use standard properties of multivariate Gaussians to write down the probability density that is implied over <span class="math inline">$\mappingFunctionVector$</span>. In particular we know that if <span class="math inline">$\weightVector$</span> is sampled from a multivariate normal (or multivariate Gaussian) with covariance <span class="math inline">$\alpha \eye$</span> and zero mean, then assuming that <span class="math inline">$\basisMatrix$</span> is a deterministic matrix (i.e. it is not sampled from a probability density) then the vector <span class="math inline">$\mappingFunctionVector$</span> will also be distributed according to a zero mean multivariate normal as follows, <br /><span class="math display">$$
\mappingFunctionVector \sim \gaussianSamp{\zerosVector}{\alpha \basisMatrix\basisMatrix^\top}.
$$</span><br /></p>
<p>The question now is, what happens if we sample <span class="math inline">$\mappingFunctionVector$</span> directly from this density, rather than first sampling <span class="math inline">$\weightVector$</span> and then multiplying by <span class="math inline">$\basisMatrix$</span>. Let’s try this. First of all we define the covariance as <br /><span class="math display">$$
\kernelMatrix = \alpha
\basisMatrix\basisMatrix^\top.
$$</span><br /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1">K <span class="op">=</span> alpha<span class="op">*</span>np.dot(Phi_pred, Phi_pred.T)</a></code></pre></div>
<p>Now we can use the <code>np.random.multivariate_normal</code> command for sampling from a multivariate normal with covariance given by <span class="math inline">$\kernelMatrix$</span> and zero mean,</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">10</span>):</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">    f_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">    plt.plot(x_pred.flatten(), f_sample.flatten())</a></code></pre></div>
<p>The samples appear very similar to those which we obtained indirectly. That is no surprise because they are effectively drawn from the same mutivariate normal density. However, when sampling <span class="math inline">$\mappingFunctionVector$</span> directly we created the covariance for <span class="math inline">$\mappingFunctionVector$</span>. We can visualise the form of this covaraince in an image in python with a colorbar to show scale.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">im <span class="op">=</span> ax.imshow(K, interpolation<span class="op">=</span><span class="st">&#39;none&#39;</span>)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">fig.colorbar(im)</a></code></pre></div>
<p>{This image is the covariance expressed between different points on the function. In regression we normally also add independent Gaussian noise to obtain our observations <span class="math inline">$\dataVector$</span>, <br /><span class="math display">$$
\dataVector = \mappingFunctionVector + \boldsymbol{\epsilon}
$$</span><br /> where the noise is sampled from an independent Gaussian distribution with variance <span class="math inline">$\dataStd^2$</span>, <br /><span class="math display">$$
\epsilon \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}.
$$</span><br /> we can use properties of Gaussian variables, i.e. the fact that sum of two Gaussian variables is also Gaussian, and that it’s covariance is given by the sum of the two covariances, whilst the mean is given by the sum of the means, to write down the marginal likelihood, <br /><span class="math display">$$
\dataVector \sim \gaussianSamp{\zerosVector}{\basisMatrix\basisMatrix^\top +\dataStd^2\eye}.
$$</span><br /> Sampling directly from this density gives us the noise corrupted functions,</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1">K <span class="op">=</span> alpha<span class="op">*</span>np.dot(Phi_pred, Phi_pred.T) <span class="op">+</span> sigma2<span class="op">*</span>np.eye(x_pred.size)</a>
<a class="sourceLine" id="cb18-2" data-line-number="2"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</a>
<a class="sourceLine" id="cb18-3" data-line-number="3">    y_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</a>
<a class="sourceLine" id="cb18-4" data-line-number="4">    plt.plot(x_pred.flatten(), y_sample.flatten())</a></code></pre></div>
<p>where the effect of our noise term is to roughen the sampled functions, we can also increase the variance of the noise to see a different effect,</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" data-line-number="1">sigma2 <span class="op">=</span> <span class="fl">1.</span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2">K <span class="op">=</span> alpha<span class="op">*</span>np.dot(Phi_pred, Phi_pred.T) <span class="op">+</span> sigma2<span class="op">*</span>np.eye(x_pred.size)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</a>
<a class="sourceLine" id="cb19-4" data-line-number="4">    y_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</a>
<a class="sourceLine" id="cb19-5" data-line-number="5">    plt.plot(x_pred.flatten(), y_sample.flatten())</a></code></pre></div>

<h2 id="non-degenerate-gaussian-processes-edit">Non-degenerate Gaussian Processes <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-degenerate-gps.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/non-degenerate-gps.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>The process described above is degenerate. The covariance function is of rank at most <span class="math inline">$\numHidden$</span> and since the theoretical amount of data could always increase <span class="math inline">$\numData \rightarrow \infty$</span>, the covariance function is not full rank. This means as we increase the amount of data to infinity, there will come a point where we can’t normalize the process because the multivariate Gaussian has the form, <br /><span class="math display">$$
\gaussianDist{\mappingFunctionVector}{\zerosVector}{\kernelMatrix} = \frac{1}{\left(2\pi\right)^{\frac{\numData}{2}}\det{\kernelMatrix}^\frac{1}{2}} \exp\left(-\frac{\mappingFunctionVector^\top\kernelMatrix \mappingFunctionVector}{2}\right)
$$</span><br /> and a non-degenerate kernel matrix leads to <span class="math inline">$\det{\kernelMatrix} = 0$</span> defeating the normalization (it’s equivalent to finding a projection in the high dimensional Gaussian where the variance of the the resulting univariate Gaussian is zero, i.e. there is a null space on the covariance, or alternatively you can imagine there are one or more directions where the Gaussian has become the delta function).</p>
<p>In the machine learning field, it was Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal 1994)</span> that realized the potential of the next step. In his 1994 thesis, he was considering Bayesian neural networks, of the type we described above, and in considered what would happen if you took the number of hidden nodes, or neurons, to infinity, i.e. <span class="math inline">$\numHidden \rightarrow \infty$</span>.</p>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="neal-infinite-priors-magnify" class="magnify" onclick="magnifyFigure(&#39;neal-infinite-priors&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="neal-infinite-priors-caption" class="caption-frame">
<p>Figure: Page 37 of <a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s 1994 thesis</a></p>
</div>
</div>
<p>In loose terms, what Radford considers is what happens to the elements of the covariance function, <br /><span class="math display">$$
  \begin{align*}
  \kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) &amp; = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)\\
  &amp; = \alpha \sum_k \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_j\right)
  \end{align*}
  $$</span><br /> if instead of considering a finite number you sample infinitely many of these activation functions, sampling parameters from a prior density, <span class="math inline">$p(\mappingVectorTwo)$</span>, for each one, <br /><span class="math display">$$
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \int \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}, \inputVector_j\right) p(\mappingVector^{(1)}) \text{d}\mappingVector^{(1)}
$$</span><br /> And that’s not <em>only</em> for Gaussian <span class="math inline">$p(\mappingVectorTwo)$</span>. In fact this result holds for a range of activations, and a range of prior densities because of the <em>central limit theorem</em>.</p>
<p>To write it in the form of a probabilistic program, as long as the distribution for <span class="math inline"><em>ϕ</em><sub><em>i</em></sub></span> implied by this short probabilistic program, <br /><span class="math display">$$
  \begin{align*}
  \mappingVectorTwo &amp; \sim p(\cdot)\\
  \phi_i &amp; = \activationScalar\left(\mappingVectorTwo, \inputVector_i\right), 
  \end{align*}
  $$</span><br /> has finite variance, then the result of taking the number of hidden units to infinity, with appropriate scaling, is also a Gaussian process.</p>
<h2 id="further-reading">Further Reading</h2>
<p>To understand this argument in more detail, I highly recommend reading chapter 2 of Neal’s thesis <span class="citation" data-cites="Neal:bayesian94">(Neal 1994)</span>, which remains easy to read and clear today. Indeed, for readers interested in Bayesian neural networks, both Raford Neal’s and David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay 1992)</span> remain essential reading. Both theses embody a clarity of thought, and an ability to weave together threads from different fields that was the business of machine learning in the 1990s. Radford and David were also pioneers in making their software widely available and publishing material on the web.</p>
<h2 id="gaussian-process">Gaussian Process</h2>
<p>In our  we sampled from the prior over paraemters. Through the properties of multivariate Gaussian densities this prior over parameters implies a particular density for our data observations, <span class="math inline">$\dataVector$</span>. In this session we sampled directly from this distribution for our data, avoiding the intermediate weight-space representation. This is the approach taken by <em>Gaussian processes</em>. In a Gaussian process you specify the <em>covariance function</em> directly, rather than <em>implicitly</em> through a basis matrix and a prior over parameters. Gaussian processes have the advantage that they can be <em>nonparametric</em>, which in simple terms means that they can have <em>infinite</em> basis functions. In the lectures we introduced the <em>exponentiated quadratic</em> covariance, also known as the RBF or the Gaussian or the squared exponential covariance function. This covariance function is specified by <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left( -\frac{\left\Vert \inputVector-\inputVector^\prime\right\Vert^2}{2\ell^2}\right),
$$</span><br /> where <span class="math inline">$\left\Vert\inputVector - \inputVector^\prime\right\Vert^2$</span> is the squared distance between the two input vectors <br /><span class="math display">$$
\left\Vert\inputVector - \inputVector^\prime\right\Vert^2 = (\inputVector - \inputVector^\prime)^\top (\inputVector - \inputVector^\prime) 
$$</span><br /> Let’s build a covariance matrix based on this function. First we define the form of the covariance function,</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s eq_cov mlai.py</a></code></pre></div>
<p>We can use this to compute <em>directly</em> the covariance for <span class="math inline">$\mappingFunctionVector$</span> at the points given by <code>x_pred</code>. Let’s define a new function <code>K()</code> which does this,</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s Kernel mlai.py</a></code></pre></div>
<p>Now we can image the resulting covariance,</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" data-line-number="1">kernel <span class="op">=</span> Kernel(function<span class="op">=</span>eq_cov, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">10.</span>)</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">K <span class="op">=</span> kernel.K(x_pred, x_pred)</a></code></pre></div>
<p>To visualise the covariance between the points we can use the <code>imshow</code> function in matplotlib.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">im <span class="op">=</span> ax.imshow(K, interpolation<span class="op">=</span><span class="st">&#39;none&#39;</span>)</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">fig.colorbar(im)</a></code></pre></div>
<p>Finally, we can sample functions from the marginal likelihood.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb24-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize(<span class="dv">8</span>, <span class="dv">5</span>))</a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</a>
<a class="sourceLine" id="cb24-3" data-line-number="3">    y_sample <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>np.zeros(x_pred.size), cov<span class="op">=</span>K)</a>
<a class="sourceLine" id="cb24-4" data-line-number="4">    ax.plot(x_pred.flatten(), y_sample.flatten())</a></code></pre></div>

<h2 id="bayesian-inference-by-rejection-sampling-edit">Bayesian Inference by Rejection Sampling <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>One view of Bayesian inference is to assume we are given a mechanism for generating samples, where we assume that mechanism is representing on accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world by discarding all those samples that are inconsistent with our prior. The <em>likelihood</em> defines mathematically what we mean by inconsistent with the prior. The higher the noise level in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are considered to be samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two sampling techniques known as <em>rejection sampling</em> and <em>importance sampling</em>. It is realized in practice in an approach known as <em>approximate Bayesian computation</em> (ABC) or likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because most samples will be inconsistent with the data and as a result the mechanism has to be operated many times to obtain a few posterior samples.</p>
<p>However, in the Gaussian process case, when the likelihood also assumes Gaussian noise, we can operate this mechanism mathematically, and obtain the posterior density <em>analytically</em>. This is the benefit of Gaussian processes.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="im">import</span> pods</a>
<a class="sourceLine" id="cb25-2" data-line-number="2"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</a></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1">pods.notebook.display_plots(<span class="st">&#39;gp_rejection_sample</span><span class="sc">{sample:0&gt;3}</span><span class="st">.png&#39;</span>, </a>
<a class="sourceLine" id="cb26-2" data-line-number="2">                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp&#39;</span>, </a>
<a class="sourceLine" id="cb26-3" data-line-number="3">                            sample<span class="op">=</span>IntSlider(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">1</span>))</a></code></pre></div>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for generating samples (the <em>prior</em>), and we discard all samples inconsistent with our data, leaving the samples of interest (the <em>posterior</em>). The Gaussian process allows us to do this analytically.</p>
</div>
</div>
<h2 id="gaussian-process-1">Gaussian Process</h2>
<p>The Gaussian process perspective takes the marginal likelihood of the data to be a joint Gaussian density with a covariance given by <span class="math inline">$\kernelMatrix$</span>. So the model likelihood is of the form, <br /><span class="math display">$$
p(\dataVector|\inputMatrix) =
\frac{1}{(2\pi)^{\frac{\numData}{2}}|\kernelMatrix|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\dataVector^\top \left(\kernelMatrix+\dataStd^2
\eye\right)^{-1}\dataVector\right)
$$</span><br /> where the input data, <span class="math inline">$\inputMatrix$</span>, influences the density through the covariance matrix, <span class="math inline">$\kernelMatrix$</span> whose elements are computed through the covariance function, <span class="math inline">$\kernelScalar(\inputVector, \inputVector^\prime)$</span>.</p>
<p>This means that the negative log likelihood (the objective function) is given by, <br /><span class="math display">$$
\errorFunction(\boldsymbol{\theta}) = \frac{1}{2} \log |\kernelMatrix|
+ \frac{1}{2} \dataVector^\top \left(\kernelMatrix +
\dataStd^2\eye\right)^{-1}\dataVector
$$</span><br /> where the <em>parameters</em> of the model are also embedded in the covariance function, they include the parameters of the kernel (such as lengthscale and variance), and the noise variance, <span class="math inline">$\dataStd^2$</span>. Let’s create a class in python for storing these variables.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s GP mlai.py</a></code></pre></div>
<h2 id="making-predictions">Making Predictions</h2>
<p>We now have a probability density that represents functions. How do we make predictions with this density? The density is known as a process because it is <em>consistent</em>. By consistency, here, we mean that the model makes predictions for <span class="math inline">$\mappingFunctionVector$</span> that are unaffected by future values of <span class="math inline">$\mappingFunctionVector^*$</span> that are currently unobserved (such as test points). If we think of <span class="math inline">$\mappingFunctionVector^*$</span> as test points, we can still write down a joint probability density over the training observations, <span class="math inline">$\mappingFunctionVector$</span> and the test observations, <span class="math inline">$\mappingFunctionVector^*$</span>. This joint probability density will be Gaussian, with a covariance matrix given by our covariance function, <span class="math inline">$\kernelScalar(\inputVector_i, \inputVector_j)$</span>. <br /><span class="math display">$$
\begin{bmatrix}\mappingFunctionVector \\ \mappingFunctionVector^*\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\
\kernelMatrix_\ast^\top &amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}}
$$</span><br /> where here <span class="math inline">$\kernelMatrix$</span> is the covariance computed between all the training points, <span class="math inline">$\kernelMatrix_\ast$</span> is the covariance matrix computed between the training points and the test points and <span class="math inline">$\kernelMatrix_{\ast,\ast}$</span> is the covariance matrix computed betwen all the tests points and themselves. To be clear, let’s compute these now for our example, using <code>x</code> and <code>y</code> for the training data (although <code>y</code> doesn’t enter the covariance) and <code>x_pred</code> as the test locations.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># set covariance function parameters</span></a>
<a class="sourceLine" id="cb28-2" data-line-number="2">variance <span class="op">=</span> <span class="fl">16.0</span></a>
<a class="sourceLine" id="cb28-3" data-line-number="3">lengthscale <span class="op">=</span> <span class="dv">8</span></a>
<a class="sourceLine" id="cb28-4" data-line-number="4"><span class="co"># set noise variance</span></a>
<a class="sourceLine" id="cb28-5" data-line-number="5">sigma2 <span class="op">=</span> <span class="fl">0.05</span></a>
<a class="sourceLine" id="cb28-6" data-line-number="6"></a>
<a class="sourceLine" id="cb28-7" data-line-number="7">kernel <span class="op">=</span> Kernel(eq_cov, variance<span class="op">=</span>variance, lengthscale<span class="op">=</span>lengthscale)</a>
<a class="sourceLine" id="cb28-8" data-line-number="8">K <span class="op">=</span> kernel.K(x, x)</a>
<a class="sourceLine" id="cb28-9" data-line-number="9">K_star <span class="op">=</span> kernel.K(x, x_pred)</a>
<a class="sourceLine" id="cb28-10" data-line-number="10">K_starstar <span class="op">=</span> kernel.K(x_pred, x_pred)</a></code></pre></div>
<p>Now we use this structure to visualise the covariance between test data and training data. This structure is how information is passed between test and training data. Unlike the maximum likelihood formalisms we’ve been considering so far, the structure expresses <em>correlation</em> between our different data points. However, just like the  we now have a <em>joint density</em> between some variables of interest. In particular we have the joint density over <span class="math inline">$p(\mappingFunctionVector, \mappingFunctionVector^*)$</span>. The joint density is <em>Gaussian</em> and <em>zero mean</em>. It is specified entirely by the <em>covariance matrix</em>, <span class="math inline">$\kernelMatrix$</span>. That covariance matrix is, in turn, defined by a covariance function. Now we will visualise the form of that covariance in the form of the matrix, <br /><span class="math display">$$
\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\ \kernelMatrix_\ast^\top
&amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}
$$</span><br /></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb29-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</a>
<a class="sourceLine" id="cb29-2" data-line-number="2">im <span class="op">=</span> ax.imshow(np.vstack([np.hstack([K, K_star]), np.hstack([K_star.T, K_starstar])]), interpolation<span class="op">=</span><span class="st">&#39;none&#39;</span>)</a>
<a class="sourceLine" id="cb29-3" data-line-number="3"><span class="co"># Add lines for separating training and test data</span></a>
<a class="sourceLine" id="cb29-4" data-line-number="4">ax.axvline(x.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">&#39;w&#39;</span>)</a>
<a class="sourceLine" id="cb29-5" data-line-number="5">ax.axhline(x.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">&#39;w&#39;</span>)</a>
<a class="sourceLine" id="cb29-6" data-line-number="6">fig.colorbar(im)</a></code></pre></div>
<p>There are four blocks to this color plot. The upper left block is the covariance of the training data with itself, <span class="math inline">$\kernelMatrix$</span>. We see some structure here due to the missing data from the first and second world wars. Alongside this covariance (to the right and below) we see the cross covariance between the training and the test data (<span class="math inline">$\kernelMatrix_*$</span> and <span class="math inline">$\kernelMatrix_*^\top$</span>). This is giving us the covariation between our training and our test data. Finally the lower right block The banded structure we now observe is because some of the training points are near to some of the test points. This is how we obtain ‘communication’ between our training data and our test data. If there is no structure in <span class="math inline">$\kernelMatrix_*$</span> then our belief about the test data simply matches our prior.</p>
<h2 id="conditional-density">Conditional Density</h2>
<p>Just as in naive Bayes, we first defined the joint density (although there it was over both the labels and the inputs, <span class="math inline">$p(\dataVector, \inputMatrix)$</span> and now we need to define <em>conditional</em> distributions that answer particular questions of interest. In particular we might be interested in finding out the values of the function for the prediction function at the test data given those at the training data, <span class="math inline">$p(\mappingFunctionVector_*|\mappingFunctionVector)$</span>. Or if we include noise in the training observations then we are interested in the conditional density for the prediction function at the test locations given the training observations, <span class="math inline">$p(\mappingFunctionVector^*|\dataVector)$</span>.</p>
<p>As ever all the various questions we could ask about this density can be answered using the <em>sum rule</em> and the <em>product rule</em>. For the multivariate normal density the mathematics involved is that of <em>linear algebra</em>, with a particular emphasis on the <em>partitioned inverse</em> or <a href="http://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion"><em>block matrix inverse</em></a>, but they are beyond the scope of this course, so you don’t need to worry about remembering them or rederiving them. We are simply writing them here because it is this <em>conditional</em> density that is necessary for making predictions.</p>
<p>The conditional density is also a multivariate normal, <br /><span class="math display">$$
\mappingFunctionVector^* | \mappingFunctionVector \sim \gaussianSamp{\meanVector_\mappingFunction}{\mathbf{C}_\mappingFunction}
$$</span><br /> with a mean given by <br /><span class="math display">$$
\meanVector_\mappingFunction = \kernelMatrix_*^\top \left[\kernelMatrix + \dataStd^2
\eye\right]^{-1} \dataVector
$$</span><br /> and a covariance given by <br /><span class="math display">$$
\mathbf{C}_\mappingFunction
= \kernelMatrix_{*,*} - \kernelMatrix_*^\top \left[\kernelMatrix + \dataStd^2
\eye\right]^{-1} \kernelMatrix_\ast.
$$</span><br /> Let’s compute what those posterior predictions are for the olympic marathon data.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s posterior_f mlai.py</a></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="co"># attach the new method to class GP():</span></a>
<a class="sourceLine" id="cb31-2" data-line-number="2">GP.posterior_f <span class="op">=</span> posterior_f</a></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb32-1" data-line-number="1">model <span class="op">=</span> GP(x, y, sigma2, exponentiated_quadratic, variance<span class="op">=</span>variance, lengthscale<span class="op">=</span>lengthscale)</a>
<a class="sourceLine" id="cb32-2" data-line-number="2">mu_f, C_f <span class="op">=</span> model.posterior_f(x_pred)</a></code></pre></div>
<p>where for convenience we’ve defined</p>
<p><br /><span class="math display">$$
\mathbf{A} = \left[\kernelMatrix + \dataStd^2\eye\right]^{-1}\kernelMatrix_*.
$$</span><br /></p>
<p>We can visualize the covariance of the <em>conditional</em>,</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb33-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</a>
<a class="sourceLine" id="cb33-2" data-line-number="2">im <span class="op">=</span> ax.imshow(C_f, interpolation<span class="op">=</span><span class="st">&#39;none&#39;</span>)</a>
<a class="sourceLine" id="cb33-3" data-line-number="3">fig.colorbar(im)</a></code></pre></div>
<p>and we can plot the mean of the conditional</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb34-1" data-line-number="1">plt.plot(x, y, <span class="st">&#39;rx&#39;</span>)</a>
<a class="sourceLine" id="cb34-2" data-line-number="2">plt.plot(x_pred, mu_f, <span class="st">&#39;b-&#39;</span>)</a></code></pre></div>
<p>as well as the associated error bars. These are given (similarly to the Bayesian parametric model from the last lab) by the standard deviations of the marginal posterior densities. The marginal posterior variances are given by the diagonal elements of the posterior covariance,</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb35-1" data-line-number="1">var_f <span class="op">=</span> np.diag(C_f)[:, <span class="va">None</span>]</a>
<a class="sourceLine" id="cb35-2" data-line-number="2">std_f <span class="op">=</span> np.sqrt(var_f)</a></code></pre></div>
<p>They can be added to the underlying mean function to give the error bars,</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb36-1" data-line-number="1">plt.plot(x, y, <span class="st">&#39;rx&#39;</span>)</a>
<a class="sourceLine" id="cb36-2" data-line-number="2">plt.plot(x_pred, mu_f, <span class="st">&#39;b-&#39;</span>)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">plt.plot(x_pred, mu_f<span class="op">+</span><span class="dv">2</span><span class="op">*</span>std_f, <span class="st">&#39;b--&#39;</span>)</a>
<a class="sourceLine" id="cb36-4" data-line-number="4">plt.plot(x_pred, mu_f<span class="dv">-2</span><span class="op">*</span>std_f, <span class="st">&#39;b--&#39;</span>)</a></code></pre></div>
<p>This gives us a prediction from the Gaussian process. Remember machine learning is <br /><span class="math display">data + model → prediction.</span><br /> Here our data is from the olympics, and our model is a Gaussian process with two parameters. The assumptions about the world are encoded entirely into our Gaussian process covariance. The GP covariance assumes that the function is highly smooth, and that correlation falls off with distance (scaled according to the length scale, <span class="math inline">ℓ</span>). The model sustains the uncertainty about the function, this means we see an increase in the size of the error bars during periods like the 1st and 2nd World Wars when no olympic marathon was held.</p>

<h2 id="the-importance-of-the-covariance-function">The Importance of the Covariance Function</h2>
<p>The covariance function encapsulates our assumptions about the data. The equations for the distribution of the prediction function, given the training observations, are highly sensitive to the covariation between the test locations and the training locations as expressed by the matrix <span class="math inline">$\kernelMatrix_*$</span>. We defined a matrix <span class="math inline"><strong>A</strong></span> which allowed us to express our conditional mean in the form, <br /><span class="math display">$$
\meanVector_\mappingFunction = \mathbf{A}^\top \dataVector,
$$</span><br /> where <span class="math inline">$\dataVector$</span> were our <em>training observations</em>. In other words our mean predictions are always a linear weighted combination of our <em>training data</em>. The weights are given by computing the covariation between the training and the test data (<span class="math inline">$\kernelMatrix_*$</span>) and scaling it by the inverse covariance of the training data observations, <span class="math inline">$\left[\kernelMatrix + \dataStd^2 \eye\right]^{-1}$</span>. This inverse is the main computational object that needs to be resolved for a Gaussian process. It has a computational burden which is <span class="math inline">$O(\numData^3)$</span> and a storage burden which is <span class="math inline">$O(\numData^2)$</span>. This makes working with Gaussian processes computationally intensive for the situation where <span class="math inline">$\numData&gt;10,000$</span>.</p>
<iframe width height src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
<h2 id="improving-the-numerics">Improving the Numerics</h2>
<p>In practice we shouldn’t be using matrix inverse directly to solve the GP system. One more stable way is to compute the <em>Cholesky decomposition</em> of the kernel matrix. The log determinant of the covariance can also be derived from the Cholesky decomposition.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s update_inverse mlai.py</a></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb38-1" data-line-number="1">GP.update_inverse <span class="op">=</span> update_inverse</a></code></pre></div>
<h2 id="capacity-control">Capacity Control</h2>
<p>Gaussian processes are sometimes seen as part of a wider family of methods known as kernel methods. Kernel methods are also based around covariance functions, but in the field they are known as Mercer kernels. Mercer kernels have interpretations as inner products in potentially infinite dimensional Hilbert spaces. This interpretation arises because, if we take <span class="math inline"><em>α</em> = 1</span>, then the kernel can be expressed as <br /><span class="math display">$$
\kernelMatrix = \basisMatrix\basisMatrix^\top 
$$</span><br /> which imples the elements of the kernel are given by, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime).
$$</span><br /> So we see that the kernel function is developed from an inner product between the basis functions. Mercer’s theorem tells us that any valid <em>positive definite function</em> can be expressed as this inner product but with the caveat that the inner product could be <em>infinite length</em>. This idea has been used quite widely to <em>kernelize</em> algorithms that depend on inner products. The kernel functions are equivalent to covariance functions and they are parameterized accordingly. In the kernel modeling community it is generally accepted that kernel parameter estimation is a difficult problem and the normal solution is to cross validate to obtain parameters. This can cause difficulties when a large number of kernel parameters need to be estimated. In Gaussian process modelling kernel parameter estimation (in the simplest case proceeds) by maximum likelihood. This involves taking gradients of the likelihood with respect to the parameters of the covariance function.</p>
<h2 id="gradients-of-the-likelihood">Gradients of the Likelihood</h2>
<p>The easiest conceptual way to obtain the gradients is a two step process. The first step involves taking the gradient of the likelihood with respect to the covariance function, the second step involves considering the gradient of the covariance function with respect to its parameters.</p>
<h2 id="overall-process-scale">Overall Process Scale</h2>
<p>In general we won’t be able to find parameters of the covariance function through fixed point equations, we will need to do gradient based optimization.</p>
<h2 id="capacity-control-and-data-fit">Capacity Control and Data Fit</h2>
<p>The objective function can be decomposed into two terms, a capacity control term, and a data fit term. The capacity control term is the log determinant of the covariance. The data fit term is the matrix inner product between the data and the inverse covariance.</p>
<p>Can we determine covariance parameters from the data?</p>
<p><br /><span class="math display">$$
\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\color{black} \det{\kernelMatrix}^{\frac{1}{2}}}}{\color{black}\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;{\color{black}-\frac{1}{2}\log\det{\kernelMatrix}}{\color{black}-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
$$</span><br /></p>
<p><br /><span class="math display">$$
\errorFunction(\parameterVector) = {\color{black} \frac{1}{2}\log\det{\kernelMatrix}} + {\color{black} \frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
$$</span><br /></p>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <br /><span class="math display">$$\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)$$</span><br /></p>
<p><span> <br /><span class="math display">$$\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top$$</span><br /></span></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb39-1" data-line-number="1">gpoptimizePlot1</a></code></pre></div>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">$\eigenvalueMatrix$</span> represents distance on axes. <span class="math inline">$\rotationMatrix$</span> gives rotation.
</td>
</tr>
</table>
<ul>
<li><span class="math inline">$\eigenvalueMatrix$</span> is <em>diagonal</em>, <span class="math inline">$\rotationMatrix^\top\rotationMatrix = \eye$</span>.</li>
<li>Useful representation since <span class="math inline">$\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2$</span>.</li>
</ul>
<!--
\only<1>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant1}}\only<2>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant2}}\only<3>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant3}}\only<4>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant4}}\only<5>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant5}}\only<6>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant6}}\only<7>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant7}}\only<8>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant8}}\only<9>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant9}}\only<10>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant10}}-->
<!--```{.python}
gpoptimizePlot3
```

\only<1>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic1}}\only<2>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic2}}\only<3>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic3}}-->
<div class="figure">
<div id="gp-optimise-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width="100%" style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width="100%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="gp-optimise-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-optimise&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="gp-optimise-caption" class="caption-frame">
<p>Figure: Variation in the data fit term, the capacity term and the negative log likelihood for different lengthscales.</p>
</div>
</div>
<h2 id="exponentiated-quadratic-covariance-edit">Exponentiated Quadratic Covariance <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/eq-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>The exponentiated quadratic covariance, also known as the Gaussian covariance or the RBF covariance and the squared exponential. Covariance between two points is related to the negative exponential of the squared distnace between those points. This covariance function can be derived in a few different ways: as the infinite limit of a radial basis function neural network, as diffusion in the heat equation, as a Gaussian filter in <em>Fourier space</em> or as the composition as a series of linear filters applied to a base function.</p>
<p>The covariance takes the following form, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)
$$</span><br /> where <span class="math inline">ℓ</span> is the <em>length scale</em> or <em>time scale</em> of the process and <span class="math inline"><em>α</em></span> represents the overall process variance.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)$$</span><br />
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class width="100%" data="../slides/diagrams/kern/eq_covariance.svg">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="eq-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;eq-covariance-plot&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="eq-covariance-plot-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function.</p>
</div>
</div>
<h2 id="olympic-marathon-data-edit">Olympic Marathon Data <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/olympic-marathon-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression modelling. The data consists of the pace of Olympic Gold Medal Marathon winners for the Olympics from 1896 to present. First we load in the data and plot.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb40-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb40-2" data-line-number="2"><span class="im">import</span> pods</a></code></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb41-1" data-line-number="1">data <span class="op">=</span> pods.datasets.olympic_marathon_men()</a>
<a class="sourceLine" id="cb41-2" data-line-number="2">x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</a>
<a class="sourceLine" id="cb41-3" data-line-number="3">y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</a>
<a class="sourceLine" id="cb41-4" data-line-number="4"></a>
<a class="sourceLine" id="cb41-5" data-line-number="5">offset <span class="op">=</span> y.mean()</a>
<a class="sourceLine" id="cb41-6" data-line-number="6">scale <span class="op">=</span> np.sqrt(y.var())</a></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb42-2" data-line-number="2"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a>
<a class="sourceLine" id="cb42-3" data-line-number="3"><span class="im">import</span> mlai</a></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb43-1" data-line-number="1"></a>
<a class="sourceLine" id="cb43-2" data-line-number="2">xlim <span class="op">=</span> (<span class="dv">1875</span>,<span class="dv">2030</span>)</a>
<a class="sourceLine" id="cb43-3" data-line-number="3">ylim <span class="op">=</span> (<span class="fl">2.5</span>, <span class="fl">6.5</span>)</a>
<a class="sourceLine" id="cb43-4" data-line-number="4">yhat <span class="op">=</span> (y<span class="op">-</span>offset)<span class="op">/</span>scale</a>
<a class="sourceLine" id="cb43-5" data-line-number="5"></a>
<a class="sourceLine" id="cb43-6" data-line-number="6">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</a>
<a class="sourceLine" id="cb43-7" data-line-number="7">_ <span class="op">=</span> ax.plot(x, y, <span class="st">&#39;r.&#39;</span>,markersize<span class="op">=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb43-8" data-line-number="8">ax.set_xlabel(<span class="st">&#39;year&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb43-9" data-line-number="9">ax.set_ylabel(<span class="st">&#39;pace min/km&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb43-10" data-line-number="10">ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb43-11" data-line-number="11">ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb43-12" data-line-number="12"></a>
<a class="sourceLine" id="cb43-13" data-line-number="13">mlai.write_figure(figure<span class="op">=</span>fig, </a>
<a class="sourceLine" id="cb43-14" data-line-number="14">                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/datasets/olympic-marathon.svg&#39;</span>, </a>
<a class="sourceLine" id="cb43-15" data-line-number="15">                  transparent<span class="op">=</span><span class="va">True</span>, </a>
<a class="sourceLine" id="cb43-16" data-line-number="16">                  frameon<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1892.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in this year, the olympics was in St Louis, USA. Organizational problems and challenges with dust kicked up by the cars following the race meant that participants got lost, and only very few participants completed.</p>
<p>More recent years see more consistently quick marathons.</p>
<h2 id="alan-turing-edit">Alan Turing <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/alan-turing-marathon.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/alan-turing-marathon.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="turing-run-times-magnify" class="magnify" onclick="magnifyFigure(&#39;turing-run-times&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="turing-run-times-caption" class="caption-frame">
<p>Figure: Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html">Alan Turing Internet Scrapbook</a>.</p>
</div>
</div>
<p>If we had to summarise the objectives of machine learning in one word, a very good candidate for that word would be <em>generalization</em>. What is generalization? From a human perspective it might be summarised as the ability to take lessons learned in one domain and apply them to another domain. If we accept the definition given in the first session for machine learning, <br /><span class="math display">$$
\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}
$$</span><br /> then we see that without a model we can’t generalise: we only have data. Data is fine for answering very specific questions, like “Who won the Olympic Marathon in 2012?”, because we have that answer stored, however, we are not given the answer to many other questions. For example, Alan Turing was a formidable marathon runner, in 1946 he ran a time 2 hours 46 minutes (just under four minutes per kilometer, faster than I and most of the other <a href="http://www.parkrun.org.uk/sheffieldhallam/">Endcliffe Park Run</a> runners can do 5 km). What is the probability he would have won an Olympics if one had been held in 1946?</p>
<p>To answer this question we need to generalize, but before we formalize the concept of generalization let’s introduce some formal representation of what it means to generalize in machine learning.</p>
<p>Our first objective will be to perform a Gaussian process fit to the data, we’ll do this using the <a href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="im">import</span> GPy</a></code></pre></div>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb45-1" data-line-number="1">m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2">_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></a></code></pre></div>
<p>The first command sets up the model, then <code>m_full.optimize()</code> optimizes the parameters of the covariance function and the noise level of the model. Once the fit is complete, we’ll try creating some test points, and computing the output of the GP model in terms of the mean and standard deviation of the posterior functions between 1870 and 2030. We plot the mean function and the standard deviation at 200 locations. We can obtain the predictions using <code>y_mean, y_var = m_full.predict(xt)</code></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb46-1" data-line-number="1">xt <span class="op">=</span> np.linspace(<span class="dv">1870</span>,<span class="dv">2030</span>,<span class="dv">200</span>)[:,np.newaxis]</a>
<a class="sourceLine" id="cb46-2" data-line-number="2">yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</a>
<a class="sourceLine" id="cb46-3" data-line-number="3">yt_sd<span class="op">=</span>np.sqrt(yt_var)</a></code></pre></div>
<p>Now we plot the results using the helper function in <code>teaching_plots</code>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a></code></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb48-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</a>
<a class="sourceLine" id="cb48-2" data-line-number="2">plot.model_output(m_full, scale<span class="op">=</span>scale, offset<span class="op">=</span>offset, ax<span class="op">=</span>ax, xlabel<span class="op">=</span><span class="st">&#39;year&#39;</span>, ylabel<span class="op">=</span><span class="st">&#39;pace min/km&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>, portion<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb48-3" data-line-number="3">ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb48-4" data-line-number="4">ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb48-5" data-line-number="5">mlai.write_figure(figure<span class="op">=</span>fig,</a>
<a class="sourceLine" id="cb48-6" data-line-number="6">                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp/olympic-marathon-gp.svg&#39;</span>, </a>
<a class="sourceLine" id="cb48-7" data-line-number="7">                  transparent<span class="op">=</span><span class="va">True</span>, frameon<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-marathon-gp.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-marathon-gp&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="olympic-marathon-gp-caption" class="caption-frame">
<p>Figure: Gaussian process fit to the Olympic Marathon data. The error bars are too large, perhaps due to the outlier from 1904.</p>
</div>
</div>
<h2 id="fit-quality">Fit Quality</h2>
<p>In the fit we see that the error bars (coming mainly from the noise variance) are quite large. This is likely due to the outlier point in 1904, ignoring that point we can see that a tighter fit is obtained. To see this making a version of the model, <code>m_clean</code>, where that point is removed.</p>
<pre><code>x_clean=np.vstack((x[0:2, :], x[3:, :]))
y_clean=np.vstack((y[0:2, :], y[3:, :]))

m_clean = GPy.models.GPRegression(x_clean,y_clean)
_ = m_clean.optimize()</code></pre>
<h2 id="gene-expression-example-edit">Gene Expression Example <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/della-gatta-gene-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/della-gatta-gene-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>We now consider an example in gene expression. Gene expression is the measurement of mRNA levels expressed in cells. These mRNA levels show which genes are ‘switched on’ and producing data. In the example we will use a Gaussian process to determine whether a given gene is active, or we are merely observing a noise response.</p>
<h2 id="della-gatta-gene-data-edit">Della Gatta Gene Data <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/della-gatta-gene-data.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/della-gatta-gene-data.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb50-2" data-line-number="2"><span class="im">import</span> pods</a></code></pre></div>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb51-1" data-line-number="1">data <span class="op">=</span> pods.datasets.della_gatta_TRP63_gene_expression(data_set<span class="op">=</span><span class="st">&#39;della_gatta&#39;</span>,gene_number<span class="op">=</span><span class="dv">937</span>)</a>
<a class="sourceLine" id="cb51-2" data-line-number="2"></a>
<a class="sourceLine" id="cb51-3" data-line-number="3">x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]</a>
<a class="sourceLine" id="cb51-4" data-line-number="4">y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]</a>
<a class="sourceLine" id="cb51-5" data-line-number="5"></a>
<a class="sourceLine" id="cb51-6" data-line-number="6">offset <span class="op">=</span> y.mean()</a>
<a class="sourceLine" id="cb51-7" data-line-number="7">scale <span class="op">=</span> np.sqrt(y.var())</a></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb52-1" data-line-number="1"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb52-2" data-line-number="2"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a>
<a class="sourceLine" id="cb52-3" data-line-number="3"><span class="im">import</span> mlai</a></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb53-1" data-line-number="1"></a>
<a class="sourceLine" id="cb53-2" data-line-number="2">xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">20</span>,<span class="dv">260</span>)</a>
<a class="sourceLine" id="cb53-3" data-line-number="3">ylim <span class="op">=</span> (<span class="dv">5</span>, <span class="fl">7.5</span>)</a>
<a class="sourceLine" id="cb53-4" data-line-number="4">yhat <span class="op">=</span> (y<span class="op">-</span>offset)<span class="op">/</span>scale</a>
<a class="sourceLine" id="cb53-5" data-line-number="5"></a>
<a class="sourceLine" id="cb53-6" data-line-number="6">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</a>
<a class="sourceLine" id="cb53-7" data-line-number="7">_ <span class="op">=</span> ax.plot(x, y, <span class="st">&#39;r.&#39;</span>,markersize<span class="op">=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb53-8" data-line-number="8">ax.set_xlabel(<span class="st">&#39;time/min&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb53-9" data-line-number="9">ax.set_ylabel(<span class="st">&#39;expression&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb53-10" data-line-number="10">ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb53-11" data-line-number="11">ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb53-12" data-line-number="12"></a>
<a class="sourceLine" id="cb53-13" data-line-number="13">mlai.write_figure(figure<span class="op">=</span>fig, </a>
<a class="sourceLine" id="cb53-14" data-line-number="14">                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/datasets/della-gatta-gene.svg&#39;</span>, </a>
<a class="sourceLine" id="cb53-15" data-line-number="15">                  transparent<span class="op">=</span><span class="va">True</span>, </a>
<a class="sourceLine" id="cb53-16" data-line-number="16">                  frameon<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-data-magnify" class="magnify" onclick="magnifyFigure(&#39;della-gatta-gene-data&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="della-gatta-gene-data-caption" class="caption-frame">
<p>Figure: Gene expression levels over time for a gene from data provided by <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>. We would like to understand whethere there is signal in the data, or we are only observing noise.</p>
</div>
</div>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="a-simple-approach-to-ranking-magnify" class="magnify" onclick="magnifyFigure(&#39;a-simple-approach-to-ranking&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="a-simple-approach-to-ranking-caption" class="caption-frame">
<p>Figure: The example is taken from the paper “A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression.” <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</p>
</div>
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
<p>Our first objective will be to perform a Gaussian process fit to the data, we’ll do this using the <a href="https://github.com/SheffieldML/GPy">GPy software</a>.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb54-1" data-line-number="1"><span class="im">import</span> GPy</a></code></pre></div>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb55-1" data-line-number="1">m_full <span class="op">=</span> GPy.models.GPRegression(x,yhat)</a>
<a class="sourceLine" id="cb55-2" data-line-number="2">m_full.kern.lengthscale<span class="op">=</span><span class="dv">50</span></a>
<a class="sourceLine" id="cb55-3" data-line-number="3">_ <span class="op">=</span> m_full.optimize() <span class="co"># Optimize parameters of covariance function</span></a></code></pre></div>
<p>Initialize the length scale parameter (which here actually represents a <em>time scale</em> of the covariance function) to a reasonable value. Default would be 1, but here we set it to 50 minutes, given points are arriving across zero to 250 minutes.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb56-1" data-line-number="1">xt <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">20</span>,<span class="dv">260</span>,<span class="dv">200</span>)[:,np.newaxis]</a>
<a class="sourceLine" id="cb56-2" data-line-number="2">yt_mean, yt_var <span class="op">=</span> m_full.predict(xt)</a>
<a class="sourceLine" id="cb56-3" data-line-number="3">yt_sd<span class="op">=</span>np.sqrt(yt_var)</a></code></pre></div>
<p>Now we plot the results using the helper function in <code>teaching_plots</code>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a></code></pre></div>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb58-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</a>
<a class="sourceLine" id="cb58-2" data-line-number="2">plot.model_output(m_full, scale<span class="op">=</span>scale, offset<span class="op">=</span>offset, ax<span class="op">=</span>ax, xlabel<span class="op">=</span><span class="st">&#39;time/min&#39;</span>, ylabel<span class="op">=</span><span class="st">&#39;expression&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>, portion<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb58-3" data-line-number="3">ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb58-4" data-line-number="4">ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb58-5" data-line-number="5">ax.set_title(<span class="st">&#39;log likelihood: </span><span class="sc">{ll:.3}</span><span class="st">&#39;</span>.<span class="bu">format</span>(ll<span class="op">=</span>m_full.log_likelihood()), fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb58-6" data-line-number="6">mlai.write_figure(figure<span class="op">=</span>fig,</a>
<a class="sourceLine" id="cb58-7" data-line-number="7">                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp/della-gatta-gene-gp.svg&#39;</span>, </a>
<a class="sourceLine" id="cb58-8" data-line-number="8">                  transparent<span class="op">=</span><span class="va">True</span>, frameon<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp-magnify" class="magnify" onclick="magnifyFigure(&#39;della-gatta-gene-gp&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="della-gatta-gene-gp-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the time scale parameter initialized to 50 minutes.</p>
</div>
</div>
<p>Now we try a model initialized with a longer length scale.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb59-1" data-line-number="1">m_full2 <span class="op">=</span> GPy.models.GPRegression(x,yhat)</a>
<a class="sourceLine" id="cb59-2" data-line-number="2">m_full2.kern.lengthscale<span class="op">=</span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb59-3" data-line-number="3">_ <span class="op">=</span> m_full2.optimize() <span class="co"># Optimize parameters of covariance function</span></a></code></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb60-1" data-line-number="1"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a></code></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb61-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</a>
<a class="sourceLine" id="cb61-2" data-line-number="2">plot.model_output(m_full2, scale<span class="op">=</span>scale, offset<span class="op">=</span>offset, ax<span class="op">=</span>ax, xlabel<span class="op">=</span><span class="st">&#39;time/min&#39;</span>, ylabel<span class="op">=</span><span class="st">&#39;expression&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>, portion<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb61-3" data-line-number="3">ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb61-4" data-line-number="4">ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb61-5" data-line-number="5">ax.set_title(<span class="st">&#39;log likelihood: </span><span class="sc">{ll:.3}</span><span class="st">&#39;</span>.<span class="bu">format</span>(ll<span class="op">=</span>m_full2.log_likelihood()), fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb61-6" data-line-number="6">mlai.write_figure(figure<span class="op">=</span>fig,</a>
<a class="sourceLine" id="cb61-7" data-line-number="7">                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp/della-gatta-gene-gp2.svg&#39;</span>, </a>
<a class="sourceLine" id="cb61-8" data-line-number="8">                  transparent<span class="op">=</span><span class="va">True</span>, frameon<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp2-magnify" class="magnify" onclick="magnifyFigure(&#39;della-gatta-gene-gp2&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="della-gatta-gene-gp2-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the time scale parameter initialized to 2000 minutes.</p>
</div>
</div>
<p>Now we try a model initialized with a lower noise.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb62-1" data-line-number="1">m_full3 <span class="op">=</span> GPy.models.GPRegression(x,yhat)</a>
<a class="sourceLine" id="cb62-2" data-line-number="2">m_full3.kern.lengthscale<span class="op">=</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb62-3" data-line-number="3">m_full3.likelihood.variance<span class="op">=</span><span class="fl">0.001</span></a>
<a class="sourceLine" id="cb62-4" data-line-number="4">_ <span class="op">=</span> m_full3.optimize() <span class="co"># Optimize parameters of covariance function</span></a></code></pre></div>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb63-1" data-line-number="1"><span class="im">import</span> teaching_plots <span class="im">as</span> plot</a></code></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb64-1" data-line-number="1">fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)</a>
<a class="sourceLine" id="cb64-2" data-line-number="2">plot.model_output(m_full3, scale<span class="op">=</span>scale, offset<span class="op">=</span>offset, ax<span class="op">=</span>ax, xlabel<span class="op">=</span><span class="st">&#39;time/min&#39;</span>, ylabel<span class="op">=</span><span class="st">&#39;expression&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>, portion<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb64-3" data-line-number="3">ax.set_xlim(xlim)</a>
<a class="sourceLine" id="cb64-4" data-line-number="4">ax.set_ylim(ylim)</a>
<a class="sourceLine" id="cb64-5" data-line-number="5">ax.set_title(<span class="st">&#39;log likelihood: </span><span class="sc">{ll:.3}</span><span class="st">&#39;</span>.<span class="bu">format</span>(ll<span class="op">=</span>m_full3.log_likelihood()), fontsize<span class="op">=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb64-6" data-line-number="6">mlai.write_figure(figure<span class="op">=</span>fig,</a>
<a class="sourceLine" id="cb64-7" data-line-number="7">                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp/della-gatta-gene-gp3.svg&#39;</span>, </a>
<a class="sourceLine" id="cb64-8" data-line-number="8">                  transparent<span class="op">=</span><span class="va">True</span>, frameon<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
<div id="della-gatta-gene-gp3-magnify" class="magnify" onclick="magnifyFigure(&#39;della-gatta-gene-gp3&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="della-gatta-gene-gp3-caption" class="caption-frame">
<p>Figure: Result of the fit of the Gaussian process model with the noise initialized low (standard deviation 0.1) and the time scale parameter initialized to 20 minutes.</p>
</div>
</div>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
<div id="gp-multiple-optima000-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-multiple-optima000&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="gp-multiple-optima000-caption" class="caption-frame">
<p>Figure:</p>
</div>
</div>
<!--

<object class="svgplot " data="../slides/diagrams/gp/multiple-optima001.svg" width="" style=" "></object>-->
<h2 id="example-prediction-of-malaria-incidence-in-uganda-edit">Example: Prediction of Malaria Incidence in Uganda <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_health/includes/malaria-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_health/includes/malaria-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p><span style="text-align:right"><img class="" src="../slides/diagrams/people/2013_03_28_180606.JPG" width="1.5cm" style="background:none; border:none; box-shadow:none; position:absolute; clip:rect(2662px,1780px,1110px,600px);vertical-align:middle"></span></p>
<p>As an example of using Gaussian process models within the full pipeline from data to decsion, we’ll consider the prediction of Malaria incidence in Uganda. For the purposes of this study malaria reports come in two forms, HMIS reports from health centres and Sentinel data, which is curated by the WHO. There are limited sentinel sites and many HMIS sites.</p>
<p>The work is from Ricardo Andrade Pacheco’s PhD thesis, completed in collaboration with John Quinn and Martin Mubangizi <span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al. 2014; Mubangizi et al. 2014)</span>. John and Martin were initally from the AI-DEV group from the University of Makerere in Kampala and more latterly they were based at UN Global Pulse in Kampala.</p>
<p>Malaria data is spatial data. Uganda is split into districts, and health reports can be found for each district. This suggests that models such as conditional random fields could be used for spatial modelling, but there are two complexities with this. First of all, occasionally districts split into two. Secondly, sentinel sites are a specific location within a district, such as Nagongera which is a sentinel site based in the Tororo district.</p>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="uganda-districts-2006-magnify" class="magnify" onclick="magnifyFigure(&#39;uganda-districts-2006&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="uganda-districts-2006-caption" class="caption-frame">
<p>Figure: Ugandan districs. Data SRTM/NASA from <a href="https://dds.cr.usgs.gov/srtm/version2_1" class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.</p>
</div>
</div>
<p><span style="text-align:right"><span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al. 2014; Mubangizi et al. 2014)</span></span></p>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class width="50%" data="../slides/diagrams/health/Kapchorwa_District_in_Uganda.svg">
</object>
</div>
<div id="kapchorwa-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;kapchorwa-district-in-uganda&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="kapchorwa-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kapchorwa District, home district of Stephen Kiprotich.</p>
</div>
</div>
<p>Stephen Kiprotich, the 2012 gold medal winner from the London Olympics, comes from Kapchorwa district, in eastern Uganda, near the border with Kenya.</p>
<p>The common standard for collecting health data on the African continent is from the Health management information systems (HMIS). However, this data suffers from missing values <span class="citation" data-cites="Gething:hmis06">(Gething et al. 2006)</span> and diagnosis of diseases like typhoid and malaria may be confounded.</p>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class width="50%" data="../slides/diagrams/health/Tororo_District_in_Uganda.svg">
</object>
</div>
<div id="tororo-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;tororo-district-in-uganda&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="tororo-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Tororo district, where the sentinel site, Nagongera, is located.</p>
</div>
</div>
<p><a href="https://www.who.int/immunization/monitoring_surveillance/burden/vpd/surveillance_type/sentinel/en/">World Health Organization Sentinel Surveillance systems</a> are set up “when high-quality data are needed about a particular disease that cannot be obtained through a passive system”. Several sentinel sites give accurate assessment of malaria disease levels in Uganda, including a site in Nagongera.</p>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="sentinel-nagongera-magnify" class="magnify" onclick="magnifyFigure(&#39;sentinel-nagongera&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="sentinel-nagongera-caption" class="caption-frame">
<p>Figure: Sentinel and HMIS data along with rainfall and temperature for the Nagongera sentinel station in the Tororo district.</p>
</div>
</div>
<p>In collaboration with the AI Research Group at Makerere we chose to investigate whether Gaussian process models could be used to assimilate information from these two different sources of disease informaton. Further, we were interested in whether local information on rainfall and temperature could be used to improve malaria estimates.</p>
<p>The aim of the project was to use WHO Sentinel sites, alongside rainfall and temperature, to improve predictions from HMIS data of levels of malaria.</p>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class width="50%" data="../slides/diagrams/health/Mubende_District_in_Uganda.svg">
</object>
</div>
<div id="mubende-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;mubende-district-in-uganda&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="mubende-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Mubende District.</p>
</div>
</div>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="malaria-prediction-mubende-magnify" class="magnify" onclick="magnifyFigure(&#39;malaria-prediction-mubende&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="malaria-prediction-mubende-caption" class="caption-frame">
<p>Figure: Prediction of malaria incidence in Mubende.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The project arose out of the Gaussian process summer school held at Makerere in Kampala in 2013. The school led, in turn, to the Data Science Africa initiative.</p>
</div>
</div>
<h2 id="early-warning-systems">Early Warning Systems</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class width="50%" data="../slides/diagrams/health/Kabarole_District_in_Uganda.svg">
</object>
</div>
<div id="kabarole-district-in-uganda-magnify" class="magnify" onclick="magnifyFigure(&#39;kabarole-district-in-uganda&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="kabarole-district-in-uganda-caption" class="caption-frame">
<p>Figure: The Kabarole district in Uganda.</p>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="kabarole-disease-over-time-magnify" class="magnify" onclick="magnifyFigure(&#39;kabarole-disease-over-time&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="kabarole-disease-over-time-caption" class="caption-frame">
<p>Figure: Estimate of the current disease situation in the Kabarole district over time. Estimate is constructed with a Gaussian process with an additive covariance funciton.</p>
</div>
</div>
<p>Health monitoring system for the Kabarole district. Here we have fitted the reports with a Gaussian process with an additive covariance function. It has two components, one is a long time scale component (in red above) the other is a short time scale component (in blue).</p>
<p>Monitoring proceeds by considering two aspects of the curve. Is the blue line (the short term report signal) above the red (which represents the long term trend? If so we have higher than expected reports. If this is the case <em>and</em> the gradient is still positive (i.e. reports are going up) we encode this with a <em>red</em> color. If it is the case and the gradient of the blue line is negative (i.e. reports are going down) we encode this with an <em>amber</em> color. Conversely, if the blue line is below the red <em>and</em> decreasing, we color <em>green</em>. On the other hand if it is below red but increasing, we color <em>yellow</em>.</p>
<p>This gives us an early warning system for disease. Red is a bad situation getting worse, amber is bad, but improving. Green is good and getting better and yellow good but degrading.</p>
<p>Finally, there is a gray region which represents when the scale of the effect is small.</p>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="early-warning-system-map-magnify" class="magnify" onclick="magnifyFigure(&#39;early-warning-system-map&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="early-warning-system-map-caption" class="caption-frame">
<p>Figure: The map of Ugandan districts with an overview of the Malaria situation in each district.</p>
</div>
</div>
<p>These colors can now be observed directly on a spatial map of the districts to give an immediate impression of the current status of the disease across the country.</p>
<h2 id="additive-covariance-edit">Additive Covariance <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/add-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/add-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>An additive covariance function is derived from considering the result of summing two Gaussian processes together. If the first Gaussian process is <span class="math inline"><em>g</em>( ⋅ )</span>, governed by covariance <span class="math inline">$\kernelScalar_g(\cdot, \cdot)$</span> and the second process is <span class="math inline"><em>h</em>( ⋅ )</span>, governed by covariance <span class="math inline">$\kernelScalar_h(\cdot, \cdot)$</span> then the combined process <span class="math inline"><em>f</em>( ⋅ ) = <em>g</em>( ⋅ ) + <em>h</em>( ⋅ )</span> is govererned by a covariance function, <br /><span class="math display">$$
\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)
$$</span><br /></p>
<center>
<br /><span class="math display">$$\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class width="100%" data="../slides/diagrams/kern/add_covariance.svg">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="add-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;add-covariance-plot&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="add-covariance-plot-caption" class="caption-frame">
<p>Figure: An additive covariance function formed by combining two exponentiated quadratic covariance functions.</p>
</div>
</div>
<h2 id="analysis-of-us-birth-rates-edit">Analysis of US Birth Rates <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/bda-forecasting.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/bda-forecasting.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="bialik-friday-the-13th-magnify" class="magnify" onclick="magnifyFigure(&#39;bialik-friday-the-13th&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="bialik-friday-the-13th-caption" class="caption-frame">
<p>Figure: This is a retrospective analysis of US births by Aki Vehtari. The challenges of forecasting. Even with seasonal and weekly effects removed there are significant effects on holidays, weekends, etc.</p>
</div>
</div>
<p>There’s a nice analysis of US birth rates by Gaussian processes with additive covariances in <span class="citation" data-cites="Gelman:bayesian13">Gelman et al. (2013)</span>. A combination of covariance functions are used to take account of weekly and yearly trends. The analysis is summarized on the cover of the book.</p>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="bayesian-data-analysis-magnify" class="magnify" onclick="magnifyFigure(&#39;bayesian-data-analysis&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="bayesian-data-analysis-caption" class="caption-frame">
<p>Figure: Two different editions of Bayesian Data Analysis <span class="citation" data-cites="Gelman:bayesian13">(Gelman et al. 2013)</span>.</p>
</div>
</div>
<h2 id="basis-function-covariance-edit">Basis Function Covariance <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/basis-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/basis-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>The fixed basis function covariance just comes from the properties of a multivariate Gaussian, if we decide <br /><span class="math display">$$
\mappingFunctionVector=\basisMatrix\mappingVector
$$</span><br /> and then we assume <br /><span class="math display">$$
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye}
$$</span><br /> then it follows from the properties of a multivariate Gaussian that <br /><span class="math display">$$
\mappingFunctionVector \sim \gaussianSamp{\zerosVector}{\alpha\basisMatrix\basisMatrix^\top}
$$</span><br /> meaning that the vector of observations from the function is jointly distributed as a Gaussian process and the covariance matrix is <span class="math inline">$\kernelMatrix = \alpha\basisMatrix \basisMatrix^\top$</span>, each element of the covariance matrix can then be found as the inner product between two rows of the basis funciton matrix.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s basis_cov mlai.py</a></code></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s radial mlai.py</a></code></pre></div>
<center>
<br /><span class="math display">$$\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)$$</span><br />
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class width="100%" data="../slides/diagrams/kern/basis_covariance.svg">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="basis-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;basis-covariance-plot&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="basis-covariance-plot-caption" class="caption-frame">
<p>Figure: A covariance function based on a non-linear basis given by <span class="math inline">$\basisVector(\inputVector)$</span>.</p>
</div>
</div>
<h2 id="brownian-covariance-edit">Brownian Covariance <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/brownian-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/brownian-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s brownian_cov mlai.py</a></code></pre></div>
<p>Brownian motion is also a Gaussian process. It follows a Gaussian random walk, with diffusion occuring at each time point driven by a Gaussian input. This implies it is both Markov and Gaussian. The covariance function for Brownian motion has the form <br /><span class="math display">$$
\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)
$$</span><br /></p>
<center>
<br /><span class="math display">$$\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)$$</span><br />
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class width="100%" data="../slides/diagrams/kern/brownian_covariance.svg">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="brownian-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;brownian-covariance-plot&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="brownian-covariance-plot-caption" class="caption-frame">
<p>Figure: Brownian motion covariance function.</p>
</div>
</div>
<h2 id="mlp-covariance-edit">MLP Covariance <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/mlp-covariance.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_kern/includes/mlp-covariance.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="op">%</span>load <span class="op">-</span>s mlp_cov mlai.py</a></code></pre></div>
<p>The multi-layer perceptron (MLP) covariance, also known as the neural network covariance or the arcsin covariance, is derived by considering the infinite limit of a neural network.</p>
<center>
<br /><span class="math display">$$\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)$$</span><br />
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class width="100%" data="../slides/diagrams/kern/mlp_covariance.svg">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="mlp-covariance-plot-magnify" class="magnify" onclick="magnifyFigure(&#39;mlp-covariance-plot&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="mlp-covariance-plot-caption" class="caption-frame">
<p>Figure: The multi-layer perceptron covariance function. This is derived by considering the infinite limit of a neural network with probit activation functions.</p>
</div>
</div>
<h2 id="gpss-gaussian-process-summer-school-edit">GPSS: Gaussian Process Summer School <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-summer-school.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-summer-school.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div style="width:1.5cm;text-align:center">

</div>
<p>If you’re interested in finding out more about Gaussian processes, you can attend the Gaussian process summer school, or view the lectures and material on line. Details of the school, future events and past events can be found at the website <a href="http://gpss.cc" class="uri">http://gpss.cc</a>.</p>
<h2 id="gpy-a-gaussian-process-framework-in-python-edit">GPy: A Gaussian Process Framework in Python <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e. you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Andrade:consistent14">
<p>Andrade-Pacheco, Ricardo, Martin Mubangizi, John Quinn, and Neil D. Lawrence. 2014. “Consistent Mapping of Government Malaria Records Across a Changing Territory Delimitation.” <em>Malaria Journal</em> 13 (Suppl 1). <a href="https://doi.org/10.1186/1475-2875-13-S1-P5" class="uri">https://doi.org/10.1186/1475-2875-13-S1-P5</a>.</p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, Giusy, Mukesh Bansal, Alberto Ambesi-Impiombato, Dario Antonini, Caterina Missero, and Diego di Bernardo. 2008. “Direct Targets of the Trp63 Transcription Factor Revealed by a Combination of Gene Expression Profiling and Reverse Engineering.” <em>Genome Research</em> 18 (6). Telethon Institute of Genetics; Medicine, 80131 Naples, Italy.: 939–48. <a href="https://doi.org/10.1101/gr.073601.107" class="uri">https://doi.org/10.1101/gr.073601.107</a>.</p>
</div>
<div id="ref-Gelman:bayesian13">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. 3rd ed. Chapman; Hall.</p>
</div>
<div id="ref-Gething:hmis06">
<p>Gething, Peter W., Abdisalan M. Noor, Priscilla W. Gikandi, Esther A. A. Ogara, Simon I. Hay, Mark S. Nixon, Robert W. Snow, and Peter M. Atkinson. 2006. “Improving Imperfect Data from Health Management Information Systems in Africa Using Space–Time Geostatistics.” <em>PLoS Medicine</em> 3 (6). Public Library of Science. <a href="https://doi.org/10.1371/journal.pmed.0030271" class="uri">https://doi.org/10.1371/journal.pmed.0030271</a>.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, Alfredo A., and Neil D. Lawrence. 2011. “A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses Through Gaussian Process Regression.” <em>BMC Bioinformatics</em> 12 (180). <a href="https://doi.org/10.1186/1471-2105-12-180" class="uri">https://doi.org/10.1186/1471-2105-12-180</a>.</p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, David J. C. 1992. “Bayesian Methods for Adaptive Models.” PhD thesis, California Institute of Technology.</p>
</div>
<div id="ref-Mubangizi:malaria14">
<p>Mubangizi, Martin, Ricardo Andrade-Pacheco, Michael Thomas Smith, John Quinn, and Neil D. Lawrence. 2014. “Malaria Surveillance with Multiple Data Sources Using Gaussian Process Models.” In <em>1st International Conference on the Use of Mobile ICT in Africa</em>.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, Radford M. 1994. “Bayesian Learning for Neural Networks.” PhD thesis, Dept. of Computer Science, University of Toronto.</p>
</div>
</div>


