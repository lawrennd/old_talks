---
title: "Generalization: Model Validation"
venue: "University of Sheffield"
abstract: "Generalization is the main objective of a machine learning algorithm. The models we design should work on data they have not seen before. Confirming whether a model generalizes well or not is the domain of <em>model validation</em>. In this lecture we introduce approaches to model validation such as hold out validation and cross validation."
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: University of Sheffield
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
date: 2015-10-27
published: 2015-10-27
reveal: 2015-10-27-week5.slides.html
ipynb: 2015-10-27-week5.ipynb
layout: talk
categories:
- notes
---


<div style="display:none">
  $${% include talk-notation.tex %}$$
</div>

<script src="/talks/figure-magnify.js"></script>
<script src="/talks/figure-animate.js"></script>
    
<div id="modal-frame" class="modal">
  <span class="close" onclick="closeMagnify()">&times;</span>
  <div class="modal-figure">
    <div class="figure-frame">
      <div class="modal-content" id="modal01"></div>
      <!--<img class="modal-content" id="object01">-->
    </div>
    <div class="caption-frame" id="modal-caption"></div>
  </div>
</div>	  

<!-- Front matter -->
<p>.</p>
<!---->
<!--Back matter-->
<p>.</p>
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<h2 id="review">Review</h2>
<ul>
<li>Last time: introduced basis functions.</li>
<li>Showed how to maximize the likelihood of a non-linear model that's linear in parameters.</li>
<li>Explored the different characteristics of different basis function models</li>
</ul>
<h2 id="alan-turing-edit">Alan Turing <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/alan-turing-marathon.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/alan-turing-marathon.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
<div id="turing-run-times-magnify" class="magnify" onclick="magnifyFigure(&#39;turing-run-times&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="turing-run-times-caption" class="caption-frame">
<p>Figure: Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html">Alan Turing Internet Scrapbook</a>.</p>
</div>
</div>
<p>If we had to summarise the objectives of machine learning in one word, a very good candidate for that word would be <em>generalization</em>. What is generalization? From a human perspective it might be summarised as the ability to take lessons learned in one domain and apply them to another domain. If we accept the definition given in the first session for machine learning, <br /><span class="math display">$$
\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}
$$</span><br /> then we see that without a model we can't generalise: we only have data. Data is fine for answering very specific questions, like &quot;Who won the Olympic Marathon in 2012?&quot;, because we have that answer stored, however, we are not given the answer to many other questions. For example, Alan Turing was a formidable marathon runner, in 1946 he ran a time 2 hours 46 minutes (just under four minutes per kilometer, faster than I and most of the other <a href="http://www.parkrun.org.uk/sheffieldhallam/">Endcliffe Park Run</a> runners can do 5 km). What is the probability he would have won an Olympics if one had been held in 1946?</p>
<p>To answer this question we need to generalize, but before we formalize the concept of generalization let's introduce some formal representation of what it means to generalize in machine learning.</p>
<h2 id="expected-loss-edit">Expected Loss <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/expected-loss.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/expected-loss.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>Our objective function so far has been the negative log likelihood, which we have minimized (via the sum of squares error) to obtain our model. However, there is an alternative perspective on an objective function, that of a <em>loss function</em>. A loss function is a cost function associated with the penalty you might need to pay for a particular incorrect decision. One approach to machine learning involves specifying a loss function and considering how much a particular model is likely to cost us across its lifetime. We can represent this with an expectation. If our loss function is given as <span class="math inline">$L(\dataScalar, \inputScalar, \mappingVector)$</span> for a particular model that predicts <span class="math inline">$\dataScalar$</span> given <span class="math inline">$\inputScalar$</span> and <span class="math inline">$\mappingVector$</span> then we are interested in minimizing the expected loss under the likely distribution of <span class="math inline">$\dataScalar$</span> and <span class="math inline">$\inputScalar$</span>. To understand this formally we define the <em>true</em> distribution of the data samples, <span class="math inline">$\dataScalar$</span>, <span class="math inline">$\inputScalar$</span>. This is a particularl distribution that we don't have access to very often, and to represent that we define it with a variant of the letter 'P', <span class="math inline">$\mathbb{P}(\dataScalar, \inputScalar)$</span>. If we genuinely pay <span class="math inline">$L(\dataScalar, \inputScalar, \mappingVector)$</span> for every mistake we make, and the future test data is genuinely drawn from <span class="math inline">$\mathbb{P}(\dataScalar, \inputScalar)$</span> then we can define our expected loss, or risk, to be, <br /><span class="math display">$$
R(\mappingVector) = \int L(\dataScalar, \inputScalar, \mappingVector) \mathbb{P}(\dataScalar, \inputScalar) \text{d}\dataScalar
\text{d}\inputScalar.
$$</span><br /> Of course, in practice, this value can't be computed <em>but</em> it serves as a reminder of what it is we are aiming to minimize and under certain circumstances it can be approximated.</p>
<h2 id="sample-based-approximations">Sample Based Approximations</h2>
<p>A sample based approximation to an expectation involves replacing the true expectation with a sum over samples from the distribution.</p>
<p><br /><span class="math display">$$
\int \mappingFunction(z) p(z) \text{d}z\approx \frac{1}{s}\sum_{i=1}^s \mappingFunction(z_i).
$$</span><br /> if <span class="math inline">{<em>z</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>s</em></sup></span> are a set of <span class="math inline"><em>s</em></span> independent and identically distributed samples from the distribution <span class="math inline"><em>p</em>(<em>z</em>)</span>. This approximation becomes better for larger <span class="math inline"><em>s</em></span>, although the <em>rate of convergence</em> to the true integral will be very dependent on the distribution <span class="math inline"><em>p</em>(<em>z</em>)</span> <em>and</em> the function <span class="math inline">$\mappingFunction(z)$</span>.</p>
<p>That said, this means we can approximate our true integral with the sum, <br /><span class="math display">$$
R(\mappingVector) \approx \frac{1}{\numData}\sum_{i=1}^{\numData} L(\dataScalar_i, \inputScalar_i, \mappingVector).
$$</span><br /></p>
<p>if <span class="math inline">$\dataScalar_i$</span> and <span class="math inline">$\inputScalar_i$</span> are independent samples from the true distribution <span class="math inline">$\mathbb{P}(\dataScalar, \inputScalar)$</span>. Minimizing this sum directly is known as <em>empirical risk minimization</em>. The sum of squares error we have been using can be recovered for this case by considering a <em>squared loss</em>, <br /><span class="math display">$$
L(\dataScalar, \inputScalar, \mappingVector) = (\dataScalar-\mappingVector^\top\boldsymbol{\phi}(\inputScalar))^2,
$$</span><br /> which gives an empirical risk of the form <br /><span class="math display">$$
R(\mappingVector) \approx \frac{1}{\numData} \sum_{i=1}^{\numData}
(\dataScalar_i - \mappingVector^\top \boldsymbol{\phi}(\inputScalar_i))^2
$$</span><br /> which up to the constant <span class="math inline">$\frac{1}{\numData}$</span> is identical to the objective function we have been using so far.</p>
<h2 id="estimating-risk-through-validation-edit">Estimating Risk through Validation <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/empirical-risk-minimization.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/empirical-risk-minimization.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>Unfortuantely, minimising the empirial risk only guarantees something about our performance on the training data. If we don't have enough data for the approximation to the risk to be valid, then we can end up performing significantly worse on test data. Fortunately, we can also estimate the risk for test data through estimating the risk for unseen data. The main trick here is to 'hold out' a portion of our data from training and use the models performance on that sub-set of the data as a proxy for the true risk. This data is known as 'validation' data. It contrasts with test data, because its values are known at the model design time. However, in contrast to test date we don't use it to fit our model. This means that it doesn't exhibit the same bias that the empirical risk does when estimating the true risk.</p>
<h2 id="validation-edit">Validation <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/validation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/validation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>In this lab we will explore techniques for model selection that make use of validation data. Data that isn't seen by the model in the learning (or fitting) phase, but is used to <em>validate</em> our choice of model from amoungst the different designs we have selected.</p>
<p>In machine learning, we are looking to minimise the value of our objective function <span class="math inline"><em>E</em></span> with respect to its parameters <span class="math inline">$\mappingVector$</span>. We do this by considering our training data. We minimize the value of the objective function as it's observed at each training point. However we are really interested in how the model will perform on future data. For evaluating that we choose to <em>hold out</em> a portion of the data for evaluating the quality of the model.</p>
<p>We will review the different methods of model selection on the Olympics marathon data. Firstly we import the Olympic marathon data.</p>
<h2 id="olympic-marathon-data">Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
<p>The first thing we will do is load a standard data set for regression modelling. The data consists of the pace of Olympic Gold Medal Marathon winners for the Olympics from 1896 to present. First we load in the data and plot.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> pods</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">data <span class="op">=</span> pods.datasets.olympic_marathon_men()
x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]
y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]

offset <span class="op">=</span> y.mean()
scale <span class="op">=</span> np.sqrt(y.var())</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> teaching_plots <span class="im">as</span> plot
<span class="im">import</span> mlai</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
xlim <span class="op">=</span> (<span class="dv">1875</span>,<span class="dv">2030</span>)
ylim <span class="op">=</span> (<span class="fl">2.5</span>, <span class="fl">6.5</span>)
yhat <span class="op">=</span> (y<span class="op">-</span>offset)<span class="op">/</span>scale

fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>plot.big_wide_figsize)
_ <span class="op">=</span> ax.plot(x, y, <span class="st">&#39;r.&#39;</span>,markersize<span class="op">=</span><span class="dv">10</span>)
ax.set_xlabel(<span class="st">&#39;year&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)
ax.set_ylabel(<span class="st">&#39;pace min/km&#39;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)
ax.set_xlim(xlim)
ax.set_ylim(ylim)

mlai.write_figure(figure<span class="op">=</span>fig, 
                  filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/datasets/olympic-marathon.svg&#39;</span>, 
                  transparent<span class="op">=</span><span class="va">True</span>, 
                  frameon<span class="op">=</span><span class="va">True</span>)</code></pre></div>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width style=" ">
</object>
</div>
<div id="olympic-marathon-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-marathon&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="olympic-marathon-caption" class="caption-frame">
<p>Figure: Olympic marathon pace times since 1892.</p>
</div>
</div>
<p>Things to notice about the data include the outlier in 1904, in this year, the olympics was in St Louis, USA. Organizational problems and challenges with dust kicked up by the cars following the race meant that participants got lost, and only very few participants completed.</p>
<p>More recent years see more consistently quick marathons.</p>
<h2 id="validation-on-the-olympic-marathon-data-edit">Validation on the Olympic Marathon Data <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/validation-olympic-fit.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/validation-olympic-fit.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>The first thing we'll do is fit a standard linear model to the data. We recall from previous lectures and lab classes that to do this we need to solve the system <br /><span class="math display">$$
\basisMatrix^\top \basisMatrix \mappingVector = \basisMatrix^\top \dataVector
$$</span><br /> for <span class="math inline">$\mappingVector$</span> and use the resulting vector to make predictions at the training points and test points, <br /><span class="math display">$$
\mappingFunctionVector = \basisMatrix \mappingVector.
$$</span><br /> The prediction function can be used to compute the objective function, <br /><span class="math display">$$
E(\mappingVector) = \sum_{i}^{\numData} (\dataScalar_i - \mappingVector^\top\phi(\dataVector_i))^2
$$</span><br /> by substituting in the prediction in vector form we have <br /><span class="math display">$$
E(\mappingVector) =  (\dataVector - \mappingFunctionVector)^\top(\dataVector - \mappingFunctionVector)
$$</span><br /></p>

<h2 id="polynomial-fit-training-error">Polynomial Fit: Training Error</h2>

<h2 id="polynomial-fits-to-olympics-data">Polynomial Fits to Olympics Data</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pods
<span class="im">import</span> mlai</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">max_basis <span class="op">=</span> <span class="dv">26</span>
basis <span class="op">=</span> mlai.polynomial

data <span class="op">=</span> pods.datasets.olympic_marathon_men()
x <span class="op">=</span> data[<span class="st">&#39;X&#39;</span>]
y <span class="op">=</span> data[<span class="st">&#39;Y&#39;</span>]

data_limits <span class="op">=</span> [<span class="dv">1892</span>, <span class="dv">2020</span>]
num_data <span class="op">=</span> x.shape[<span class="dv">0</span>]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> teaching_plots <span class="im">as</span> plot
<span class="op">%</span>matplotlib inline</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pods.notebook.display_plots(<span class="st">&#39;olympic_LM_polynomial_num_basis</span><span class="sc">{num_basis:0&gt;3}</span><span class="st">.svg&#39;</span>, 
                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, 
                            num_basis<span class="op">=</span>IntSlider(<span class="dv">1</span>, <span class="dv">1</span>, max_basis, <span class="dv">1</span>))</code></pre></div>
<div class="figure">
<div id="olympic-lm-polynomial-num-basis-26-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis026.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-lm-polynomial-num-basis-26-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-lm-polynomial-num-basis-26&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="olympic-lm-polynomial-num-basis-26-caption" class="caption-frame">
<p>Figure: Polynomial fit to olympic data with 26 basis functions.</p>
</div>
</div>
<h2 id="hold-out-validation-on-olympic-marathon-data-edit">Hold Out Validation on Olympic Marathon Data <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/hold-out-validation-olympics.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/hold-out-validation-olympics.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pods
<span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pods.notebook.display_plots(<span class="st">&#39;olympic_val_extra_LM_polynomial_num_basis</span><span class="sc">{num_basis:0&gt;3}</span><span class="st">.svg&#39;</span>, 
                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, 
                            num_basis<span class="op">=</span>IntSlider(<span class="dv">1</span>, <span class="dv">1</span>, max_basis, <span class="dv">1</span>))</code></pre></div>
<div class="figure">
<div id="olympic-val-extra-LM-polynomial-num-basis-26-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_extra_LM_polynomial_num_basis026.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-val-extra-LM-polynomial-num-basis-26-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-val-extra-LM-polynomial-num-basis-26&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="olympic-val-extra-LM-polynomial-num-basis-26-caption" class="caption-frame">
<p>Figure: Olympic marathon data with validation error for extrapolation.</p>
</div>
</div>
<h2 id="extrapolation">Extrapolation</h2>
<h2 id="interpolation">Interpolation</h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pods
<span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pods.notebook.display_plots(<span class="st">&#39;olympic_val_inter_LM_polynomial_num_basis</span><span class="sc">{num_basis:0&gt;3}</span><span class="st">.svg&#39;</span>, 
                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, 
                            num_basis<span class="op">=</span>IntSlider(<span class="dv">1</span>, <span class="dv">1</span>, max_basis, <span class="dv">1</span>))</code></pre></div>
<div class="figure">
<div id="olympic-val-inter-LM-polynomial-num-basis-26-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_inter_LM_polynomial_num_basis026.svg" width="80%" style=" ">
</object>
</div>
<div id="olympic-val-inter-LM-polynomial-num-basis-26-magnify" class="magnify" onclick="magnifyFigure(&#39;olympic-val-inter-LM-polynomial-num-basis-26&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="olympic-val-inter-LM-polynomial-num-basis-26-caption" class="caption-frame">
<p>Figure: Olympic marathon data with validation error for interpolation.</p>
</div>
</div>
<h2 id="choice-of-validation-set">Choice of Validation Set</h2>
<h2 id="hold-out-data">Hold Out Data</h2>
<p>You have a conclusion as to which model fits best under the training error, but how do the two models perform in terms of validation? In this section we consider <em>hold out</em> validation. In hold out validation we remove a portion of the training data for <em>validating</em> the model on. The remaining data is used for fitting the model (training). Because this is a time series prediction, it makes sense for us to hold out data at the end of the time series. This means that we are validating on future predictions. We will hold out data from after 1980 and fit the model to the data before 1980.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># select indices of data to &#39;hold out&#39;</span>
indices_hold_out <span class="op">=</span> np.flatnonzero(x<span class="op">&gt;</span><span class="dv">1980</span>)

<span class="co"># Create a training set</span>
x_train <span class="op">=</span> np.delete(x, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)
y_train <span class="op">=</span> np.delete(y, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)

<span class="co"># Create a hold out set</span>
x_valid <span class="op">=</span> np.take(x, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)
y_valid <span class="op">=</span> np.take(y, indices_hold_out, axis<span class="op">=</span><span class="dv">0</span>)</code></pre></div>

<h2 id="richer-basis-set">Richer Basis Set</h2>
<p>Now we have an approach for deciding which model to retain, we can consider the entire family of polynomial bases, with arbitrary degrees.</p>

<h2 id="leave-one-out-validation-edit">Leave One Out Validation <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/loo-validation-olympics.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/loo-validation-olympics.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider
<span class="im">import</span> pods</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pods.notebook.display_plots(<span class="st">&#39;olympic_loo</span><span class="sc">{part:0&gt;3}</span><span class="st">_LM_polynomial_number</span><span class="sc">{num_basis:0&gt;3}</span><span class="st">.svg&#39;</span>, 
                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, 
                            num_basis<span class="op">=</span>IntSlider(<span class="dv">1</span>, <span class="dv">1</span>, max_basis, <span class="dv">1</span>), 
                            part<span class="op">=</span>IntSlider(<span class="dv">0</span>, <span class="dv">0</span>, x.shape[<span class="dv">0</span>], <span class="dv">1</span>))</code></pre></div>
<p>Hold out validation uses a portion of the data to hold out and a portion of the data to train on. There is always a compromise between how much data to hold out and how much data to train on. The more data you hold out, the better the estimate of your performance at 'run-time' (when the model is used to make predictions in real applications). However, by holding out more data, you leave less data to train on, so you have a better validation, but a poorer quality model fit than you could have had if you'd used all the data for training. Leave one out cross validation leaves as much data in the training phase as possible: you only take <em>one point</em> out for your validation set. However, if you do this for hold-out validation, then the quality of your validation error is very poor because you are testing the model quality on one point only. In <em>cross validation</em> the approach is to improve this estimate by doing more than one model fit. In <em>leave one out cross validation</em> you fit <span class="math inline">$\numData$</span> different models, where <span class="math inline">$\numData$</span> is the number of your data. For each model fit you take out one data point, and train the model on the remaining <span class="math inline"><em>n</em> − 1</span> data points. You validate the model on the data point you've held out, but you do this <span class="math inline">$\numData$</span> times, once for each different model. You then take the <em>average</em> of all the <span class="math inline">$\numData$</span> badly estimated hold out validation errors. The average of this estimate is a good estimate of performance of those models on the test data.</p>

<h2 id="k-fold-cross-validation-edit"><span class="math inline"><em>k</em></span>-fold Cross Validation <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/k-fold-validation-olympics.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/k-fold-validation-olympics.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider
<span class="im">import</span> pods</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pods.notebook.display_plots(<span class="st">&#39;olympic_</span><span class="sc">{num_parts}</span><span class="st">&#39;</span>.<span class="bu">format</span>(num_parts<span class="op">=</span>num_parts) <span class="op">+</span> <span class="st">&#39;cv</span><span class="sc">{part:0&gt;2}</span><span class="st">_LM_polynomial_number</span><span class="sc">{number:0&gt;3}</span><span class="st">.svg&#39;</span>, 
                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, 
                            part<span class="op">=</span>IntSlider(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">1</span>),
                            number<span class="op">=</span>IntSlider(<span class="dv">1</span>, <span class="dv">1</span>, max_basis, <span class="dv">1</span>))</code></pre></div>
<p>Leave one out cross validation produces a very good estimate of the performance at test time, and is particularly useful if you don't have a lot of data. In these cases you need to make as much use of your data for model fitting as possible, and having a large hold out data set (to validate model performance) can have a significant effect on the size of the data set you have to fit your model, and correspondingly, the complexity of the model you can fit. However, leave one out cross validation involves fitting <span class="math inline">$\numData$</span> models, where <span class="math inline">$\numData$</span> is your number of training data. For the olympics example, this is only 27 model fits, but in practice many data sets consist thousands or millions of data points, and fitting many millions of models for estimating validation error isn't really practical. One option is to return to <em>hold out</em> validation, but another approach is to perform <span class="math inline"><em>k</em></span>-fold cross validation. In <span class="math inline"><em>k</em></span>-fold cross validation you split your data into <span class="math inline"><em>k</em></span> parts. Then you use <span class="math inline"><em>k</em> − 1</span> of those parts for training, and hold out one part for validation. Just like we did for the hold out validation above. In <em>cross</em> validation, however, you repeat this process. You swap the part of the data you just used for validation back in to the training set and select another part for validation. You then fit the model to the new training data and validate on the portion of data you've just extracted. Each split of training/validation data is called a <em>fold</em> and since you do this process <span class="math inline"><em>k</em></span> times, the procedure is known as <span class="math inline"><em>k</em></span>-fold cross validation. The term <em>cross</em> refers to the fact that you cross over your validation portion back into the training data every time you perform a fold.</p>

<p>Expected test error for different variations of the <em>training data</em> sampled from, <span class="math inline">$\Pr(\dataVector, \dataScalar)$</span> <br /><span class="math display">$$\mathbb{E}\left[ \left(\dataScalar - \mappingFunction^*(\dataVector)\right)^2 \right]$$</span><br /> Decompose as <br /><span class="math display">$$\mathbb{E}\left[ \left(\dataScalar - \mappingFunction(\dataVector)\right)^2 \right] = \text{bias}\left[\mappingFunction^*(\dataVector)\right]^2 + \text{variance}\left[\mappingFunction^*(\dataVector)\right] +\sigma^2$$</span><br /></p>
<ul>
<li>Given by <br /><span class="math display">$$\text{bias}\left[\mappingFunction^*(\dataVector)\right] =
\mathbb{E}\left[\mappingFunction^*(\dataVector)\right] * \mappingFunction(\dataVector)$$</span><br /></li>
<li><p>Error due to bias comes from a model that's too simple.</p></li>
<li>Given by <br /><span class="math display">$$\text{variance}\left[\mappingFunction^*(\dataVector)\right] = \mathbb{E}\left[\left(\mappingFunction^*(\dataVector) - \mathbb{E}\left[\mappingFunction^*(\dataVector)\right]\right)^2\right]$$</span><br /></li>
<li><p>Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model.</p></li>
</ul>
<h2 id="bias-vs-variance-error-plots-edit">Bias vs Variance Error Plots <span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-plots.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ml/includes/bias-variance-plots.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></h2>
<p>Helper function for sampling data from two different classes.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> create_data(per_cluster<span class="op">=</span><span class="dv">30</span>):
    <span class="co">&quot;&quot;&quot;Create a randomly sampled data set</span>
<span class="co">    </span>
<span class="co">    :param per_cluster: number of points in each cluster</span>
<span class="co">    &quot;&quot;&quot;</span>
    X <span class="op">=</span> []
    y <span class="op">=</span> []
    scale <span class="op">=</span> <span class="dv">3</span>
    prec <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(scale<span class="op">*</span>scale)
    pos_mean <span class="op">=</span> [[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>],[<span class="dv">0</span>,<span class="fl">0.5</span>],[<span class="dv">1</span>,<span class="dv">0</span>]]
    pos_cov <span class="op">=</span> [[prec, <span class="dv">0</span>.], [<span class="dv">0</span>., prec]]
    neg_mean <span class="op">=</span> [[<span class="dv">0</span>, <span class="op">-</span><span class="fl">0.5</span>],[<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.5</span>],[<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.5</span>]]
    neg_cov <span class="op">=</span> [[prec, <span class="dv">0</span>.], [<span class="dv">0</span>., prec]]
    <span class="cf">for</span> mean <span class="kw">in</span> pos_mean:
        X.append(np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>pos_cov, size<span class="op">=</span>per_class))
        y.append(np.ones((per_class, <span class="dv">1</span>)))
    <span class="cf">for</span> mean <span class="kw">in</span> neg_mean:
        X.append(np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>neg_cov, size<span class="op">=</span>per_class))
        y.append(np.zeros((per_class, <span class="dv">1</span>)))
    <span class="cf">return</span> np.vstack(X), np.vstack(y).flatten()</code></pre></div>
<p>Helper function for plotting the decision boundary of the SVM.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> plot_contours(ax, cl, xx, yy, <span class="op">**</span>params):
    <span class="co">&quot;&quot;&quot;Plot the decision boundaries for a classifier.</span>

<span class="co">    :param ax: matplotlib axes object</span>
<span class="co">    :param cl: a classifier</span>
<span class="co">    :param xx: meshgrid ndarray</span>
<span class="co">    :param yy: meshgrid ndarray</span>
<span class="co">    :param params: dictionary of params to pass to contourf, optional</span>
<span class="co">    &quot;&quot;&quot;</span>
    Z <span class="op">=</span> cl.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z <span class="op">=</span> Z.reshape(xx.shape)
    <span class="co"># Plot decision boundary and regions</span>
    out <span class="op">=</span> ax.contour(xx, yy, Z, 
                     levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>., <span class="dv">0</span>., <span class="dv">1</span>], 
                     colors<span class="op">=</span><span class="st">&#39;black&#39;</span>, 
                     linestyles<span class="op">=</span>[<span class="st">&#39;dashed&#39;</span>, <span class="st">&#39;solid&#39;</span>, <span class="st">&#39;dashed&#39;</span>])
    out <span class="op">=</span> ax.contourf(xx, yy, Z, 
                     levels<span class="op">=</span>[Z.<span class="bu">min</span>(), <span class="dv">0</span>, Z.<span class="bu">max</span>()], 
                     colors<span class="op">=</span>[[<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>], [<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>]])
    <span class="cf">return</span> out</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> mlai
<span class="im">import</span> os</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> decision_boundary_plot(models, X, y, axs, filename, titles, xlim, ylim):
    <span class="co">&quot;&quot;&quot;Plot a decision boundary on the given axes</span>
<span class="co">    </span>
<span class="co">    :param axs: the axes to plot on.</span>
<span class="co">    :param models: the SVM models to plot</span>
<span class="co">    :param titles: the titles for each axis</span>
<span class="co">    :param X: input training data</span>
<span class="co">    :param y: target training data&quot;&quot;&quot;</span>
    <span class="cf">for</span> ax <span class="kw">in</span> axs.flatten():
        ax.clear()
    X0, X1 <span class="op">=</span> X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>]
    <span class="cf">if</span> xlim <span class="kw">is</span> <span class="va">None</span>:
        xlim <span class="op">=</span> [X0.<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X0.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>]
    <span class="cf">if</span> ylim <span class="kw">is</span> <span class="va">None</span>:
        ylim <span class="op">=</span> [X1.<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X1.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>]
    xx, yy <span class="op">=</span> np.meshgrid(np.arange(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="fl">0.02</span>),
                         np.arange(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="fl">0.02</span>))
    <span class="cf">for</span> cl, title, ax <span class="kw">in</span> <span class="bu">zip</span>(models, titles, axs.flatten()):
        plot_contours(ax, cl, xx, yy,
                      cmap<span class="op">=</span>plt.cm.coolwarm, alpha<span class="op">=</span><span class="fl">0.8</span>)
        ax.plot(X0[y<span class="op">==</span><span class="dv">1</span>], X1[y<span class="op">==</span><span class="dv">1</span>], <span class="st">&#39;r.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)
        ax.plot(X0[y<span class="op">==</span><span class="dv">0</span>], X1[y<span class="op">==</span><span class="dv">0</span>], <span class="st">&#39;g.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)
        ax.set_xlim(xlim)
        ax.set_ylim(ylim)
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(title)
        mlai.write_figure(os.path.join(filename),
                          figure<span class="op">=</span>fig,
                          transparent<span class="op">=</span><span class="va">True</span>)
    <span class="cf">return</span> xlim, ylim</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib
font <span class="op">=</span> {<span class="st">&#39;family&#39;</span> : <span class="st">&#39;sans&#39;</span>,
        <span class="st">&#39;weight&#39;</span> : <span class="st">&#39;bold&#39;</span>,
        <span class="st">&#39;size&#39;</span>   : <span class="dv">22</span>}

matplotlib.rc(<span class="st">&#39;font&#39;</span>, <span class="op">**</span>font)
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Create an instance of SVM and fit the data. </span>
C <span class="op">=</span> <span class="fl">100.0</span>  <span class="co"># SVM regularization parameter</span>
gammas <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>]


per_class<span class="op">=</span><span class="dv">30</span>
num_samps <span class="op">=</span> <span class="dv">20</span>
<span class="co"># Set-up 2x2 grid for plotting.</span>
fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">3</span>))
xlim<span class="op">=</span><span class="va">None</span>
ylim<span class="op">=</span><span class="va">None</span>
<span class="cf">for</span> samp <span class="kw">in</span> <span class="bu">range</span>(num_samps):
    X, y<span class="op">=</span>create_data(per_class)
    models <span class="op">=</span> []
    titles <span class="op">=</span> []
    <span class="cf">for</span> gamma <span class="kw">in</span> gammas:
        models.append(svm.SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>, gamma<span class="op">=</span>gamma, C<span class="op">=</span>C))
        titles.append(<span class="st">&#39;$\gamma=</span><span class="sc">{}</span><span class="st">$&#39;</span>.<span class="bu">format</span>(gamma))
    models <span class="op">=</span> (cl.fit(X, y) <span class="cf">for</span> cl <span class="kw">in</span> models)
    xlim, ylim <span class="op">=</span> decision_boundary_plot(models, X, y, 
                           axs<span class="op">=</span>ax, 
                           filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml/bias-variance</span><span class="sc">{samp:0&gt;3}</span><span class="st">.svg&#39;</span>.<span class="bu">format</span>(samp<span class="op">=</span>samp), 
                           titles<span class="op">=</span>titles,
                          xlim<span class="op">=</span>xlim,
                          ylim<span class="op">=</span>ylim)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pods
<span class="im">from</span> ipywidgets <span class="im">import</span> IntSlider</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pods.notebook.display_plots(<span class="st">&#39;bias-variance</span><span class="sc">{samp:0&gt;3}</span><span class="st">.svg&#39;</span>, 
                            directory<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml&#39;</span>, 
                            samp<span class="op">=</span>IntSlider(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">19</span>,<span class="dv">1</span>))</code></pre></div>
<div class="figure">
<div id="bias-variance-errors-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class="svgplot " data="../slides/diagrams/ml/bias-variance000.svg" width="80%" style=" ">
</object>
</td>
<td width="45%">
<object class="svgplot " data="../slides/diagram/ml/bias-variance019.svg" width="80%" style=" ">
</object>
</td>
</tr>
</table>
</div>
<div id="bias-variance-errors-magnify" class="magnify" onclick="magnifyFigure(&#39;bias-variance-errors&#39;)">
<p><img class="img-button" src="/icons/Magnify_Large.svg" style="width:1.5ex"></p>
</div>
<div id="bias-variance-errors-caption" class="caption-frame">
<p>Figure: In each figure the more simple model is on the left, and the more complex model is on the right. Each fit is done to a different version of the data set. The simpler model is more consistent in its errors (bias error), whereas the more complex model is varying in its errors (variance error).</p>
</div>
</div>
<h2 id="reading">Reading</h2>
<ul>
<li>Section 1.5 of <span class="citation">Rogers and Girolami (2011)</span></li>
</ul>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Rogers:book11">
<p>Rogers, Simon, and Mark Girolami. 2011. <em>A First Course in Machine Learning</em>. CRC Press.</p>
</div>
</div>


