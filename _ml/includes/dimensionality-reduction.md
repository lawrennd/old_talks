\ifndef{dimensionalityReduction}
\define{dimensionalityReduction}

\editme

talk-macros.gpp}imred/includes/dimensionality-reduction-intro.md}

\subsubsection{Principal Component Analysis}
\slides{
* Principal component analysis (PCA) a linear dimensionality reduction technique
* In Hotelling's formulation of PCA: a assume $\inputVector$ is a linear weighted sum of the latent factors of interest.
* E.g. IQ test we would try and predict subject $i$'s answer to the $j$th question with the following function
$$
\dataScalar_{ij} = \mappingFunction_j(\latentScalar_i; \weightVector).
$$
$\latentScalar_i$ would be the IQ of subject $i$ and $\mappingFunction_j(\cdot)$ would function relating IQ and question answer.}
\notes{Principal component analysis (PCA) is arguably the queen of dimensionality reduction techniques. PCA was developed as an approach to dimensionality reduction in 1930s by Hotelling as a method for the social sciences. In Hotelling's formulation of PCA it was assumed that any data point, $\mathbf{x}$ could be represented as a weighted sum of the latent factors of interest, so that Hotelling described prediction functions (like in regression and classification above), only the regression is now *multiple output*.  And instead of predicting a label, $y_i$, we now try and force the regression to predict the observed feature vector, $\dataVector_i$. So, for example, on an IQ test we would try and predict subject $i$'s answer to the $j$th question with the following function
$$
\dataScalar_{ij} = \mappingFunction_j(\latentScalar_i; \weightVector).
$$
Here $z_i$ would be the IQ of subject $i$ and $\mappingFunction_j(\cdot)$ would be a function representing the relationship between the subject's IQ and their score on the answer to question $j$. This function is the same for all subjects, but the subject's IQ is assumed to differ leading to different scores for each subject.}

\newslide{Principal Component Analysis}

\figure{\includediagram{\diagramsDir/ml/demManifoldPrint_all_1_2}{60%}}{Visualization of the first two principal components of an artificial data set. The data was generated by taking an image of a handwritten digit, 6, and rotating it 360 times, one degree each time. The first two principal components have been extracted in the diagram. The underlying circular shape is derived from the rotation of the data. Each image in the data set is projected on to the location its projected to in the latent space.}{dem-manifold-print-all-1-2}

\subsubsection{Hotelling's PCA}
\slides{
* Assume function is linear function. This idea is taken from a wider field known as *factor analysis*, so Hotelling described the challenge as
$$
\mappingFunction_j(\latentScalar_i; \weightVector) = \weightScalar_j \latentScalar_i
$$
* Answer to the $j$th question is predicted to be a scaling of the subject's IQ.
* Scale factor is given by $\weightScalar_j$.}
\notes{In Hotelling's formulation he assumed that the function was a linear function. This idea is taken from a wider field known as *factor analysis*, so Hotelling described the challenge as
$$
\mappingFunction_j(\latentScalar_i; \weightVector) = \weightScalar_j \latentScalar_i
$$
so the answer to the $j$th question is predicted to be a scaling of the subject's IQ. The scale factor is given by $\weightScalar_j$. If there are more latent dimensions then a matrix of parameters, $\weightMatrix$ is used, for example if there were two latent dimensions, we'd have
$$
\mappingFunction_j(\mathbf{\latentScalar}_i; \weightMatrix) = \weightScalar_{1j} \latentScalar_{1i} + \weightScalar_{2j} \latentScalar_{2i}
$$
where, if this were a personality test, then $\latentScalar_{1i}$ might represent the spectrum over a subject's extrovert/introvert and $\latentScalar_{2i}$ might represent where the subject was on the rational/perceptual scale. The function would make a prediction about the subjects answer to a particular question on the test (e.g. preference for office job vs preference for outdoor job). In factor analysis the parameters $\weightMatrix$ are known as the factor *loadings* and in PCA they are known as the principal components.}

\newslide{Higher Latent Dimensions}
\slides{
* For more latent dimensions matrix of scales, $\weightVector$
$$
\mappingFunction_j(\latentVector_i; \weightVector) = \weightScalar_{1j} \latentScalar_{1i} + \weightScalar_{2j} \latentScalar_{2i}
$$
*  $\latentScalar_{1i}$ might be extrovert/introvert and $\latentScalar_{2i}$ might rational/perceptual}


\subsubsection{Parameters}
\slides{
* Parameters $\weightVector$ are known as the factor *loadings* in FA.
* In PCA they are known as the principal components.
* To fit the model need *loadings*, $\weightVector$, and latent variables, $\latentMatrix$.
* Can use least squares (leads to *matrix factorization* and recommender systems).
* Recommender systems most elements of $\inputVector_i$ are missing.}
\notes{Fitting the model involves finding estimates for the loadings, $\weightMatrix$, and latent variables, $\latentMatrix$. There are different approaches including least squares. The least squares approach is used, for example, in recommender systems. In recommender systems this method is called *matrix factorization*. The customer characteristics, $\dataVector_i$ is the customer rating for each different product (or item) and the latent variables can be seen as a space of customer preferences. In the recommender system case, the loadings matrix also has an interpretation as product similarities.[^recommender] Recommender systems have a particular characteristic in that most of the entries of the vector $\dataVector_i$ are missing most of the time. 

[^recommender]: One way of thinking about this is to flip the model on its side. Instead of thinking about the $i$th subject and the $j$th characteristic. Assume that each product is the subject. So, the $j$th item is thought of as the subject, and each item's characteristic is given by the rating from a particular user. In this case symmetries in the model show that the matrix $\weightMatrix$ can now be seen as a matrix of *latent variables* and the matrix $\latentMatrix$ can be seen as *factor loadings*. So, you can think of the method as simultaneously doing a dimensionality reduction on the products and the users.  Recommender systems also use other approaches, some of them based on similarity measures. In a similarity measure-based recommender system the rating prediction is given by looking for similar products in the user profile and scoring the new product with a score that is a weighted sum  of those products.} 

\newslide{Probability}
\slides{
* PCA and factor analysis the unknown latent factors are dealt with through a probability distribution.
* Assume these "unknowns" are  drawn from a zero mean, unit variance normal distribution.
* That implies a particular *probability* density for data (PDF).
* The PDF has parameters depending on factor loadings to be estimated.}
\notes{In PCA and factor analysis the unknown latent factors are dealt with through a probability distribution. They are each assumed to be drawn from a zero mean, unit variance normal distribution. This leaves the factor loadings to be estimated. For PCA the maximum likelihood solution for the factor loadings can be shown to be given by the *eigenvalue decomposition* of the data covariance matrix. This is algorithmically simple and convenient, although slow to compute for very large data sets with many features and many subjects. The eigenvalue problem can also be derived from many other starting points: e.g. the directions of maximum variance in the data or finding a latent space that best preserves inter-point distances between the data, or the optimal linear compression of the data given a linear reconstruction. These many and varied justifications for the eigenvalue decomposition may account for the popularity of PCA. Indeed, there is even an interpretation for Google's original PageRank algorithm (which computed the *smallest* eigenvector of the internet's linkage matrix) as seeking the dominant principal component of the web.[^pagerankinterpretation]

[^pagerankinterpretation]: The interpretation requires you to think of the web as a series of web pages in a high dimensional space where distances between web pages are computed by moving along the links (in either direction). The PageRank is the one-dimensional space that best preserves those distances in the sense of an L1 norm. The interpretation works because the smallest eigenvalue of the linkage matrix is the *largest* eigenvalue of the inverse of the linkage matrix. The inverse linkage matrix (which would be impossible to compute) embeds similarities between pages according to how far apart they are via a random walk along the linkage matrix.}


\newslide{Maximum Likelihood}
\slides{
* Fit model by "maximising likelihood of data" under the PDF.
* Maxium likelihood for PCA is the *eigenvalue decomposition* of the data covariance matrix.
* Algorithmically simple and convenient, but slow to compute for very large data sets with many features and many subjects.}
\notes{Characterizing users according to past buying behavior and combining this with characteristics about products, is key to making good recommendations and returning useful search results. Further advances can be made if we understand the context of a particular session. For example, if a user is buying Christmas presents and searches for a dress, then it could be the case that the user is willing to spend a little more on the dress than in normal circumstances. Characterizing these effects requires more data and more complex algorithms. However, in domains such a search we are normally constrained by the speed with which we need to return results. Accounting for each of these factors while returning results with acceptable latency is a particular challenge.} 



\endif
