\ifndef{matrixFactorization}
\define{matrixFactorization}
\editme

\subsection{Measuring Similarity}

\notes{We now need a measure for determining the similarity between
the item and the user: how close the user is sitting to the item in
the rooom if you like. We are going to use the inner product between
the vector representing the item and the vector representing the
user.}

\notes{An inner product (or
[dot product](http://en.wikipedia.org/wiki/Dot_product)) between two
vectors $\mathbf{a}$ and $\mathbf{b}$ is written as
$\mathbf{a}\cdot\mathbf{b}$. Or in vector notation we sometimes write
it as $\mathbf{a}^\top\mathbf{b}$. An inner product is simply the sume
of the products of each element of the vector,}
$$
\mathbf{a}^\top\mathbf{b} = \sum_{i} a_i b_i
$$
\notes{The inner product can be seen as a measure of similarity. The
inner product gives us the cosine of the angle between the two vectors
multiplied by their length. The smaller the angle between two vectors
the larger the inner product. }
$$
\mathbf{a}^\top\mathbf{b} = |\mathbf{a}||\mathbf{b}| \cos(\theta)
$$
\notes{where $\theta$ is the angle between two vectors and
$|\mathbf{a}|$ and $|\mathbf{b}|$ are the respective lengths of the
two vectors.}

\notes{Since we want each user to be sitting near each item, then we
want the inner product to be large for any two items which are rated
highly by that user. We can do this by trying to force the inner
product $\mathbf{u}_i^\top\mathbf{v}_j$ to be similar to the rating
given by the user, $y_{i,j}$. To ensure this we will use a least
squares objective function for all user ratings.}

\subsection{Objective Function}

The error function (or objective function, or cost function) we will
choose is known as 'sum of squares', we will aim to minimize the sum
of squared squared error between the inner product of $\mathbf{u}_i$
and $\mathbf{v}_i$ and the observed score for the user/item pairing,
given by $y_{i, j}$.

The total objective function can be written as
$$
E(\mathbf{U}, \mathbf{V}) = \sum_{i,j} s_{i,j} (y_{i,j} - \mathbf{u}_i^\top \mathbf{v}_j)^2
$$
where $s_{i,j}$ is an *indicator* variable that is 1 if user $i$ has rated item $j$ and is zero otherwise. Here $\mathbf{U}$ is the matrix made up of all the vectors $\mathbf{u}$,
$$
\mathbf{U} = \begin{bmatrix} \mathbf{u}_1 \dots \mathbf{u}_n\end{bmatrix}^\top
$$
where we note that $i$th *row* of $\mathbf{U}$ contains the vector associated with the $i$th user and $n$ is the total number of users. This form of matrix is known as a *design matrix*. Similarly, we define the matrix
$$
\mathbf{V} = \begin{bmatrix} \mathbf{v}_1 \dots \mathbf{v}_m\end{bmatrix}^\top
$$
where again the $j$th row of $\mathbf{V}$ contains the vector associated with the $j$th item and $m$ is the total number of items in the data set.

\subsection{Objective Optimization}

The idea is to mimimize this objective. A standard, simple, technique
for minimizing an objective is *gradient descent* or *steepest
descent*. In gradient descent we simply choose to update each
parameter in the model by subtracting a multiple of the objective
function's gradient with respect to the parameters. So for a parameter
$u_{i,j}$ from the matrix $\mathbf{U}$ we would have an update as
follows:
$$ 
u_{k,\ell} \leftarrow u_{k,\ell} - \eta \frac{\text{d}
E(\mathbf{U}, \mathbf{V})}{\text{d}u_{k,\ell}} 
$$
where $\eta$ (which is pronounced *eta* in English) is a Greek letter representing the *learning rate*.

We can compute the gradient of the objective function with respect to
$u_{k,\ell}$ as
$$
\frac{\text{d}E(\mathbf{U}, \mathbf{V})}{\text{d}u_{k,\ell}} = -2
\sum_j s_{k,j}v_{j,\ell}(y_{k, j} - \mathbf{u}_k^\top\mathbf{v}_{j}).
$$
Similarly each parameter $v_{i,j}$ needs to be updated according to
its gradient.

\writeassignment{What is the gradient of the objective function with
respect to $v_{k, \ell}$? Write your answer in the box below, and
explain which differentiation techniques you used to get there. You
will be expected to justify your answer in class by oral
questioning. Create a function for computing this gradient that is
used in the algorithm below.}{20}

\subsection{Steepest Descent Algorithm}

In the steepest descent algorithm we aim to minimize the objective
function by subtacting the gradient of the objective function from the
parameters.

\subsection{Initialisation}

To start with though, we need initial values for the matrix
$\mathbf{U}$ and the matrix $\mathbf{V}$. Let's create them as
`pandas` data frames and initialise them randomly with small values.

\setupcode{import numpy as np}

\code{q = 2 # the dimension of our map of the 'library'
learn_rate = 0.01
U = pd.DataFrame(np.random.normal(size=(len(user_names), q))*0.001, index=user_names)
V = pd.DataFrame(np.random.normal(size=(len(movies.index), q))*0.001, index=movies.index)}

We also will subtract the mean from the rating before we try and
predict them predictions. Have a think about why this might be a good
idea (*Hint*: what will the gradients be if we don't subtract the mean?).

\code{Y['rating'] -= Y['rating'].mean()}

Now that we have the initial values set, we can start the
optimization. First we define a function for the gradient of the
objective and the objective function itself.

\code{def objective_gradient(Y, U, V):
    gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)
    gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)
    obj = 0.
    for ind, series in Y.iterrows():
        film = series['index']
        user = series['user']
        rating = series['rating']
        prediction = np.dot(U.loc[user], V.loc[film]) # vTu
        diff = prediction - rating # vTu - y
        obj += diff*diff
        gU.loc[user] += 2*diff*V.loc[film]
        gV.loc[film] += 2*diff*U.loc[user]
    return obj, gU, gV}
	
Now we can write our simple optimisation route. This allows us to
observe the objective function as the optimization proceeds.

\code{import sys
iterations = 100
for i in range(iterations):
    obj, gU, gV = objective_gradient(Y, U, V)
    print("Iteration", i, "Objective function: ", obj)
    U -= learn_rate*gU
    V -= learn_rate*gV}
	
\codeassignment{What happens as you increase the number of iterations?
What happens if you increase the learning rate?}{}{10}

\subsection{Stochastic Gradient Descent or Robbins Monroe Algorithm}

Stochastic gradient descent [@Robbins:stoch51] involves updating separating each gradient
update according to each separate observation, rather than summing
over them all. It is an approximate optimization method, but it has
proven convergence under certain conditions and can be much faster in
practice. It is used widely by internet companies for doing machine
learning in practice. For example, Facebook's ad ranking algorithm
uses stochastic gradient descent.

\codeassignment{Create a stochastic gradient descent version of the
algorithm. Monitor the objective function after every 1000 updates to
ensure that it is decreasing. When you have finished, plot the movie
map and the user map in two dimensions. Label the plots with the name
of the movie or user.}{}{30}

\subsection{Making Predictions}

Predictions can be made from the model of the appropriate rating for a
given user, $i$, for a given film, $j$, by simply taking the inner
product between their vectors $\mathbf{u}_i$ and $\mathbf{v}_j$.

\subsection{Is Our Map Enough? Are Our Data Enough?}

Is two dimensions really enough to capture the complexity of humans
and their artforms? Perhaps we need even more dimensions to capture
that complexity. Extending our books analogy further, consider how we
should place books that have a historical timeframe as well as some
geographical location. Do we really want books from the 2nd World War
to sit alongside books from the Roman Empire? Books on the American
invasion of Sicily in 1943 are perhaps less related to books about
Carthage than those that study the Jewish Revolt from 66-70 (in the
Roman Province of Judaea). So books that relate to subjects which are
closer in time should be stored together. However, a student of
rebellion against empire may also be interested in the relationship
between the Jewish Revolt of 66-70 and the Indian Rebellion of 1857,
nearly 1800 years later. Whilst the technologies are different, the
psychology of the people is shared: a rebellious nation angainst their
imperial masters, triggered by misrule with a religious and cultural
background. To capture such complexities we would need further
dimensions in our latent representation. But are further dimensions
justified by the amount of data we have? Can we really understand the
facets of a film that only has at most three or four ratings?

\subsection{Going Further}

If you want to take this model further then you'll need more data. One
possible source of data is the
[`movielens` data set](http://grouplens.org/datasets/movielens/). They
have data sets containing up to ten million movie ratings. The few
ratings we were able to collect in the class are not enough to capture
the rich structure underlying these films. Imagine if we assume that
the ratings are uniformly distributed between 1 and 5. If you know
something about information theory then you could use that to work out
the maximum number of *bits* of information we could gain per rating.

Now we'll download the movielens 100k data and see if we can extract
information about these movies.

\setupcode{import pods}
\code{d = pods.datasets.movielens100k()
Y=d['Y']}

\codeassignment{Use stochastic gradient descent to make a movie map
for the movielens data. Plot the map of the movies when you are
finished.}{}{15}

\endif
