\ifndef{newMethodsRequired}
\define{newMethodsRequired}

\editme 

\section{New Methods Required}

\notes{The challenges of big data emerged due to companies being able
to rapidly interconnect multiple sources of information. This leads to
challenges in storage, distributed processing and modeling. Google's
search engine was at the forefront of this revolution in data
processing. Google researchers were among the first to notice that
some tasks can be easily resolved with fairly simple models and very
large data sets [@Halevy:unreasonable09]. What we are now learning is
that many tasks can be solved with complex models and even bigger data
sets.}

\notes{While GPT-3 does an impressive job on language generation, it
can do so because of the vast quantities of language data we have made
available to it. What happens if we take a more complex system, for
which such vast data is not available. Or, at least not available in
the homogeneous form that language data can be found. Let's take human
health.}

\notes{Consider we take a holistic view of health and the many ways in
which we can become unhealthy, through genomic and environmental
effects. Firstly, let's remember that we don't have a full
understanding, even on all the operations in a single eukaryotic
cell. Indeed, we don't even understand all the mechanisms by which
transcription and translation occur in bacterial and archaeal
cells. That is despite the wealth of gene expression and protein data
about these cells. Even if we were to pull all this information
together, would it be enough to develop that understanding?}

\notes{There are large quantities of data, but the complexity of the underlying 
systems in these domains, even the terabytes of data we have today may not be enough to determine the parameters of
such complex models.}

\endif
