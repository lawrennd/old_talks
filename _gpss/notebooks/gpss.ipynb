{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "### Neil D. Lawrence\n",
    "\n",
    "### 12th May 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0bbf161a930f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmlai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mteaching_plots\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlai'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pods\n",
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot \n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../diagrams/9780262182539-f30.jpg\">\n",
    "@Rasmussen:book06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "$$ \\text{data} + \\text{model} = \\text{prediction}$$\n",
    "\n",
    "-   $\\text{data}$ : observations, could be actively or passively\n",
    "    acquired (meta-data).\n",
    "\n",
    "-   $\\text{model}$ : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of\n",
    "    the universe. Inductive bias.\n",
    "\n",
    "-   $\\text{prediction}$ : an action to be taken or a categorization or a\n",
    "    quality score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "![image](./diagrams/Stephen_Kiprotich.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/16kMKHQ>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "f, ax = plt.subplots(figsize=plot.one_figsize)\n",
    "ax.plot(data['X'], data['Y'], 'r.',markersize=10)\n",
    "ax.set_xlabel('year', fontsize=20)\n",
    "ax.set_ylabel('pace min/km', fontsize=20)\n",
    "plt.savefig('../../slides/diagrams/ml/olympic_marathon.svg', transparent=True, frameon=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "<img src=\"../diagrams/olympic_marathon.svg\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.over_determined_system(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overdetermined System\n",
    "\n",
    "![](diagrams/over_determined_system007.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('over_determined_system{samp:0>3}.svg', directory='../../slides/diagrams/ml', samp=(1, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../diagrams/Pierre-Simon_Laplace.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../diagrams/LaplacesDeterminismFrench.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../diagrams/LaplacesDeterminismEnglish.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../diagrams/philosophicaless00lapliala.png\" align=center width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c + \\epsilon$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ \n",
    "$$3 = m + c + \\epsilon_1$$ \n",
    "\n",
    "point 2: $x = 3$, $y=1$ \n",
    "$$1 = 3m + c + \\epsilon_2$$ \n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Density\n",
    "- Perhaps the most common probability density.\n",
    "\\begin{align*}\n",
    "p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "& \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "- The Gaussian density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.gaussian_of_height(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "![](./diagrams/gaussian_of_height.svg)\n",
    "The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n",
    "  0.0225$. Mean shown as red line. It could represent the heights of a population of\n",
    "  students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Sum of Gaussian**\n",
    "\n",
    "-   Sum of Gaussian variables is also Gaussian.\n",
    "    $$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$$ \n",
    "    And the sum is distributed as\n",
    "    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n",
    "    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n",
    "    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Scaling a Gaussian**\n",
    "\n",
    "-   Scaling a Gaussian leads to a Gaussian.\n",
    "    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "    And the scaled density is distributed as\n",
    "    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Laplace's Idea\n",
    "\n",
    "### A Probabilistic Process\n",
    "\n",
    "-   Set the mean of Gaussian to be a function.\n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   This gives us a ‘noisy function’.\n",
    "\n",
    "-   This is known as a stochastic process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height as a Function of Weight\n",
    "\n",
    "-   In the standard Gaussian, parametized by mean and variance.\n",
    "\n",
    "-   Make the mean a linear function of an *input*.\n",
    "\n",
    "-   This leads to a regression model. \n",
    "    \\begin{align*}\n",
    "       y_i=&f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "         \\epsilon_i \\sim &\\mathcal{N}(0, \\sigma^2).\n",
    "     \\end{align*}\n",
    "        \n",
    "-   Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "-   Likelihood of an individual data point\n",
    "    $$p\\left(y_i|x_i,\\sigma^2,f()\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f(x_i)\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   If $f(x_i) = mx_i + c$ then parameters are gradient, $m$, offset, $c$ of the function and noise\n",
    "    variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "-   If the noise, $\\epsilon_i$ is sampled independently for each\n",
    "    data point.\n",
    "\n",
    "-   Each data point is independent (given $m$ and $c$).\n",
    "\n",
    "-   For independent variables:\n",
    "    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c) = \\prod_{i=1}^n p(y_i|x_i, m, c)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Simultaneous Equations\n",
    "\n",
    "A system of two simultaneous equations with two\n",
    "unknowns.\n",
    "\n",
    "How do we deal with three simultaneous\n",
    "equations with only two unknowns?\n",
    "<table><tr><td>\n",
    "$$\\begin{aligned}\n",
    "        y_1 = & mx_1 + c\\\\\n",
    "        y_2 = & mx_2 + c\n",
    "      \\end{aligned}$$ \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        y_1-y_2 = & m(x_1 - x_2)\n",
    "      \\end{aligned}$$  \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        \\frac{y_1-y_2}{x_1 - x_2} = & m\n",
    "      \\end{aligned}$$ \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        m & =\\frac{y_2-y_1}{x_2 - x_1}\\\\\n",
    "        c & = y_1 - m x_1\n",
    "      \\end{aligned}$$ \n",
    "      \n",
    "$$\\begin{aligned}\n",
    "        y_1 = & mx_1 + c\\\\\n",
    "        y_2 = & mx_2 + c\\\\\n",
    "        y_3 = & mx_3 + c\n",
    "      \\end{aligned}$$</td></tr>/</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.under_determined_system(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "- What about two unknowns and *one* observation?\n",
    "    $$y_1 =  mx_1 + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underdetermined System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('under_determined_system{samp:0>3}.svg', \n",
    "                            directory='../../slides/diagrams/ml', samp=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "- Can compute $m$ given $c$.\n",
    "$$m = \\frac{y_1 -c}{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underdetermined System\n",
    "\n",
    "- Can compute $m$ given $c$.\n",
    "\n",
    "Assume \n",
    "$$c \\sim \\mathcal{N}(0, 4),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overdetermined System\n",
    "\n",
    "-   With two unknowns and two observations: \n",
    "    $$\\begin{aligned}\n",
    "          y_1 = & mx_1 + c\\\\\n",
    "          y_2 = & mx_2 + c\n",
    "        \\end{aligned}$$\n",
    "\n",
    "-   Additional observation leads to *overdetermined* system.\n",
    "    $$y_3 =  mx_3 + c$$\n",
    "\n",
    "-   This problem is solved through a noise model\n",
    "    $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ $$\\begin{aligned}\n",
    "          y_1 = mx_1 + c + \\epsilon_1\\\\\n",
    "          y_2 = mx_2 + c + \\epsilon_2\\\\\n",
    "          y_3 = mx_3 + c + \\epsilon_3\n",
    "        \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noise Models\n",
    "\n",
    "-   We aren’t modeling entire system.\n",
    "\n",
    "-   Noise model gives mismatch between model and data.\n",
    "\n",
    "-   Gaussian model justified by appeal to central limit theorem.\n",
    "\n",
    "-   Other models also possible (Student-$t$ for heavy tails).\n",
    "\n",
    "-   Maximum likelihood with Gaussian noise leads to *least squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different Types of Uncertainty\n",
    "\n",
    "-   The first type of uncertainty we are assuming is\n",
    "    *aleatoric* uncertainty.\n",
    "\n",
    "-   The second type of uncertainty we are assuming is\n",
    "    *epistemic* uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aleatoric Uncertainty\n",
    "\n",
    "-   This is uncertainty we couldn’t know even if we wanted to. e.g. the\n",
    "    result of a football match before it’s played.\n",
    "\n",
    "-   Where a sheet of paper might land on the floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Epistemic Uncertainty\n",
    "\n",
    "-   This is uncertainty we could in principal know the answer too. We\n",
    "    just haven’t observed enough yet, e.g. the result of a football\n",
    "    match *after* it’s played.\n",
    "\n",
    "-   What colour socks your lecturer is wearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior Distribution\n",
    "\n",
    "-   Bayesian inference requires a prior on the parameters.\n",
    "\n",
    "-   The prior represents your belief *before* you see the data of the\n",
    "    likely value of the parameters.\n",
    "\n",
    "-   For linear regression, consider a Gaussian prior on the intercept:\n",
    "    $$c \\sim \\mathcal{N}(0, \\alpha_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior Distribution\n",
    "\n",
    "-   Posterior distribution is found by combining the prior with\n",
    "    the likelihood.\n",
    "\n",
    "-   Posterior distribution is your belief *after* you see the data of\n",
    "    the likely value of the parameters.\n",
    "\n",
    "-   The posterior is found through **Bayes’ Rule**\n",
    "    $$p(c|y) = \\frac{p(y|c)p(c)}{p(y)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.bayes_update(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('dem_gaussian{stage:0>3}.svg', \n",
    "                            '../../slides/diagrams/ml', stage=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stages to Derivation of the Posterior\n",
    "\n",
    "-   Multiply likelihood by prior\n",
    "\n",
    "    -   they are \"exponentiated quadratics\", the answer is always also\n",
    "        an exponentiated quadratic because\n",
    "        $$\\exp(a^2)\\exp(b^2) = \\exp(a^2 + b^2)$$\n",
    "\n",
    "-   Complete the square to get the resulting density in the form of\n",
    "    a Gaussian.\n",
    "\n",
    "-   Recognise the mean and (co)variance of the Gaussian. This is the\n",
    "    estimate of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Main Trick\n",
    "\n",
    "$$p(c) = \\frac{1}{\\sqrt{2\\pi\\alpha_1}} \\exp\\left(-\\frac{1}{2\\alpha_1}c^2\\right)$$\n",
    "$$p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2) = \\frac{1}{\\left(2\\pi\\sigma^2\\right)^{\\frac{n}{2}}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - mx_i - c)^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) = \\frac{p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)}{p(\\mathbf{y}|\\mathbf{x}, m, \\sigma^2)}$$\n",
    "\n",
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) =  \\frac{p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)}{\\int p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c) \\text{d} c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$p(c| \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) \\propto  p(\\mathbf{y}|\\mathbf{x}, c, m, \\sigma^2)p(c)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\log p(c | \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) =&-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n(y_i-c - mx_i)^2-\\frac{1}{2\\alpha_1} c^2 + \\text{const}\\\\\n",
    "     = &-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-mx_i)^2 -\\left(\\frac{n}{2\\sigma^2} + \\frac{1}{2\\alpha_1}\\right)c^2\\\\\n",
    "    & + c\\frac{\\sum_{i=1}^n(y_i-mx_i)}{\\sigma^2},\n",
    "  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "complete the square of the quadratic form to obtain\n",
    "$$\\log p(c | \\mathbf{y}, \\mathbf{x}, m, \\sigma^2) = -\\frac{1}{2\\tau^2}(c - \\mu)^2 +\\text{const},$$\n",
    "where $\\tau^2 = \\left(n\\sigma^{-2} +\\alpha_1^{-1}\\right)^{-1}$\n",
    "and\n",
    "$\\mu = \\frac{\\tau^2}{\\sigma^2} \\sum_{i=1}^n(y_i-mx_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### The Joint Density\n",
    "\n",
    "-   Really want to know the *joint* posterior density over the\n",
    "    parameters $c$ *and* $m$.\n",
    "\n",
    "-   Could now integrate out over $m$, but it’s easier to consider the\n",
    "    multivariate case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.height_weight(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.independent_height_weight(num_samps=8, \n",
    "                               diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('independent_height_weight{stage:0>3}.svg', \n",
    "                            '../../slides/diagrams/ml', stage=(0, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.correlated_height_weight(num_samps=8, \n",
    "                              diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('correlated_height_weight{stage:0>3}.svg', \n",
    "                            '../../slides/diagrams/ml', stage=(0, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampling a Function\n",
    "\n",
    "**Multi-variate Gaussians**\n",
    "\n",
    "* We will consider a Gaussian with a particular structure of covariance\n",
    "    matrix.\n",
    "* Generate a single sample from this 25 dimensional Gaussian distribution, $\\mathbf{f}=\\left[f_{1},f_{2}\\dots f_{25}\\right]$.\n",
    "\n",
    "* We will plot these points against their index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s compute_kernel mlai.py\n",
    "def compute_kernel(X, X2, kernel, **kwargs):\n",
    "    K = np.zeros((X.shape[0], X2.shape[0]))\n",
    "    for i in np.arange(X.shape[0]):\n",
    "        for j in np.arange(X2.shape[0]):\n",
    "            K[i, j] = kernel(X[i, :], X2[j, :], **kwargs)\n",
    "        \n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s polynomial_cov mlai.py\n",
    "def polynomial_cov(x, x_prime, variance=1., degree=2., w=1., b=1.):\n",
    "    \"Polynomial covariance function.\"\n",
    "    return variance*(np.dot(x, x_prime)*w + b)**degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s exponentiated_quadratic mlai.py\n",
    "def exponentiated_quadratic(x, x_prime, variance=1., lengthscale=1.):\n",
    "    \"Exponentiated quadratic covariance function.\"\n",
    "    squared_distance = ((x-x_prime)**2).sum()\n",
    "    return variance*np.exp((-0.5*squared_distance)/lengthscale**2)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.two_point_sample(compute_kernel, kernel=exponentiated_quadratic, \n",
    "                      lengthscale=0.5, diagrams='../../slides/diagrams/gp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            '../../slides/diagrams/gp', sample=(0,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../../slides/diagrams/gp/799px-Uluru_Panorama.jpg\" alignment='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction with Correlated Gaussians\n",
    "  * Prediction of $f_2$ from $f_1$ requires *conditional density*.\n",
    "  * Conditional density is *also* Gaussian.\n",
    "    $$\n",
    "    p(f_2|f_1) = \\mathcal{N}\\left(f_2|\\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2} - \\frac{k_{1,2}^2}{k_{1,1}}\\right)\n",
    "    $$\n",
    "    where covariance of joint density is given by\n",
    "    $$\n",
    "    \\mathbf{K} = \\begin{bmatrix} k_{1, 1} & k_{1, 2}\\\\ k_{2, 1} & k_{2, 2}\\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('two_point_sample{sample:0>3}.svg', \n",
    "                            '../../slides/diagrams/gp', sample=(13,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction with Correlated Gaussians\n",
    "  * Prediction of $\\mathbf{f}_*$ from $\\mathbf{f}$ requires multivariate *conditional density*.\n",
    "  * Multivariate conditional density is *also* Gaussian.\n",
    "    $$\n",
    "    p(\\mathbf{f}_*|\\mathbf{f}) = \\mathcal{N}\\left(\\mathbf{f}_*|\\mathbf{K}_{*,\\mathbf{f}}\\mathbf{K}_{\\mathbf{f},\\mathbf{f}}^{-1}\\mathbf{f},\\mathbf{K}_{*,*}-\\mathbf{K}_{*,\\mathbf{f}} \\mathbf{K}_{\\mathbf{f},\\mathbf{f}}^{-1}\\mathbf{K}_{\\mathbf{f},*}\\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction with Correlated Gaussians\n",
    "  * Prediction of $\\mathbf{f}_*$ from $\\mathbf{f}$ requires multivariate *conditional density*.\n",
    "  * Multivariate conditional density is *also* Gaussian.\n",
    "    $$\n",
    "    p(\\mathbf{f}_*|\\mathbf{f}) = \\mathcal{N}\\left(\\mathbf{f}_*|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)\n",
    "    $$\n",
    "    $$\n",
    "    \\boldsymbol{\\mu} = \\mathbf{K}_{*,\\mathbf{f}}\\mathbf{K}_{\\mathbf{f},\\mathbf{f}}^{-1}\\mathbf{f}     \n",
    "    $$\n",
    "    $$\n",
    "    \\boldsymbol{\\Sigma} = \\mathbf{K}_{*,*}-\\mathbf{K}_{*,\\mathbf{f}} \\mathbf{K}_{\\mathbf{f},\\mathbf{f}}^{-1}\\mathbf{K}_{\\mathbf{f},*}\n",
    "    $$\n",
    "  * Here covariance of joint density is given by\n",
    "    $$\n",
    "    \\mathbf{K} = \\begin{bmatrix} \\mathbf{K}_{\\mathbf{f}, \\mathbf{f}} & \\mathbf{K}_{*, \\mathbf{f}}\\\\ \\mathbf{K}_{\\mathbf{f}, *} & \\mathbf{K}_{*, *}\\end{bmatrix}\n",
    "    $$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.one_figsize)\n",
    "hcolor = [1., 0., 1.]\n",
    "K = compute_kernel(x, x, exponentiated_quadratic, lengthscale=0.5)\n",
    "obj = plot.matrix(K, ax=ax, type='image', bracket_style='boxes')\n",
    "plt.savefig('eq_covariance.svg', transparent=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=plot.one_figsize)\n",
    "#plot.kern_circular_sample([], K, filename='eq_covariance.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where Did This Covariance Matrix Come From?\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{x} - \\mathbf{x}^\\prime\\right\\Vert^2_2}{2\\ell^2}\\right)$$\n",
    "\n",
    "<table cellspacing=\"0\" cellpadding=\"0\" border=\"none\">\n",
    "<tr><td>\n",
    "<ul><li>Covariance matrix is\n",
    "built using the *inputs* to\n",
    "the function x.</li>\n",
    "\n",
    "<li>For the example above it\n",
    "was based on Euclidean\n",
    "distance.</li>\n",
    "\n",
    "<li>The covariance function\n",
    "is also know as a kernel.</li>\n",
    "</ul></td><td><img src='./diagrams/eq_covariance.svg' align='right'></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where Did This Covariance Matrix Come From?\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{x} - \\mathbf{x}^\\prime\\right\\Vert^2_2}{2\\ell^2}\\right)$$\n",
    "\n",
    "<table cellspacing=\"0\" cellpadding=\"0\" border=\"none\">\n",
    "<tr><td>\n",
    "<ul><li>Covariance matrix is\n",
    "built using the *inputs* to\n",
    "the function x.</li>\n",
    "\n",
    "<li>For the example above it\n",
    "was based on Euclidean\n",
    "distance.</li>\n",
    "\n",
    "<li>The covariance function\n",
    "is also know as a kernel.</li>\n",
    "</ul></td><td><img src='./diagrams/eq_covariance.gif' align='right'></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r'$$k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\alpha(w \\mathbf{x}^\\top \\mathbf{x}^\\prime + b)^d$$', \n",
    "                     shortname='poly', \n",
    "                     longname='Polynomial', kernel=polynomial_cov,\n",
    "                     degree=4., diagrams='../../slides/diagrams/kern')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('poly_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "= \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{x} - \\mathbf{x}^\\prime\\right\\Vert^2_2}{2\\ell^2}\\right)$$\"\"\", \n",
    "                     shortname='eq', \n",
    "                     longname='Exponentiated Quadratic', kernel=exponentiated_quadratic,\n",
    "                     lengthscale=0.5, diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('eq_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s mlp_cov mlai.py\n",
    "def mlp_cov(x, x_prime, variance=1., w=1., b=1.):\n",
    "    \"MLP covariance function.\"\n",
    "    return variance*np.arcsin((w*np.dot(x, x_prime) + b)/np.sqrt((np.dot(x, x)*w +b + 1)*(np.dot(x_prime, x_prime)*w + b + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) = \n",
    "\\alpha \\arcsin\\left(\\frac{w \\mathbf{x}^\\top \\mathbf{x}^\\prime + b}\n",
    "{\\sqrt{\\left(w \\mathbf{x}^\\top \\mathbf{x} + b + 1\\right)\n",
    "\\left(w \\left.\\mathbf{x}^\\prime\\right.^\\top \\mathbf{x}^\\prime + b + 1\\right)}}\\right)$$\"\"\",\n",
    "                     shortname='mlp', \n",
    "                     longname='Multilayer Perceptron', kernel=mlp_cov,\n",
    "                     w=10., b=10., diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('mlp_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s sinc_cov mlai.py\n",
    "def sinc_cov(x, x_prime, variance=1., w=1.):\n",
    "    \"Sinc covariance function.\"\n",
    "    r = np.linalg.norm(x-x_prime, 2)\n",
    "    return np.sinc(np.pi*w*r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "= \\text{sinc}\\left(w \\pi  \\left\\Vert\\mathbf{x}-\\mathbf{x}^\\prime\\right\\Vert\\right)$$\"\"\",\n",
    "                     shortname='sinc', \n",
    "                     longname='Sinc', kernel=sinc_cov,\n",
    "                     w=0.25, diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('sinc_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s periodic_cov mlai.py\n",
    "def periodic_cov(x, x_prime, variance=1., lengthscale=1., w=1.):\n",
    "    \"Periodic covariance function\"\n",
    "    r = np.linalg.norm(x-x_prime, 2)\n",
    "    return variance*np.exp(-2./(lengthscale*lengthscale)*np.sin(np.pi*r*w)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "= \\alpha\\exp\\left(-\\frac{2\\sin^2\\left(w \\pi  \\left\\Vert\\mathbf{x}-\\mathbf{x}^\\prime\\right\\Vert\\right)}{\\ell^2}\\right)$$\"\"\",\n",
    "                     shortname='periodic', \n",
    "                     longname='Periodic', kernel=periodic_cov,\n",
    "                     w=0.25, diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('periodic_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s ratquad_cov mlai.py\n",
    "def ratquad_cov(x, x_prime, variance=1., lengthscale=1., alpha=1.):\n",
    "    \"Rational quadratic covariance function\"\n",
    "    r = np.linalg.norm(x-x_prime, 2)\n",
    "    return variance*(1. + r*r/(2*alpha*lengthscale*lengthscale))**-alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) = \n",
    "\\alpha\\left(1+\\frac{\\left\\Vert\\mathbf{x}-\\mathbf{x}^\\prime\\right\\Vert^2}{2\\beta\\ell^2}\\right)^{-\\beta}$$\"\"\",\n",
    "                     shortname='ratquad', \n",
    "                     longname='Rational Quadratic', kernel=ratquad_cov,\n",
    "                     alpha=0.25, diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('ratquad_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "= \\alpha \\exp\\left(-\\frac{\\left\\Vert \\mathbf{x} - \\mathbf{x}^\\prime\\right\\Vert_2}{\\ell}\\right)$$\"\"\",\n",
    "                     shortname='ou', \n",
    "                     longname='Ornstein Uhlenbeck', kernel=mlai.ou_cov,\n",
    "                     lengthscale=1., diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('ou_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s brownian_cov mlai.py\n",
    "def brownian_cov(t, t_prime, variance=1.):\n",
    "    \"Brownian motion covariance function.\"\n",
    "    if t>=0 and t_prime>=0:\n",
    "        return variance*np.min([t, t_prime])\n",
    "    else:\n",
    "        raise ValueError(\"For Brownian motion covariance only positive times are valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 1, 100)[:, None]\n",
    "plot.covariance_func(t, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(t, t^\\prime) = \\alpha \\min(t, t^\\prime)$$\"\"\",\n",
    "                     shortname='brownian', \n",
    "                     longname='Brownian Motion', kernel=brownian_cov,\n",
    "                    diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('brownian_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) = \\alpha\\mathbf{x}^\\top\\mathbf{x}^\\prime$$\",\n",
    "                     shortname='linear', \n",
    "                     longname='Linear', kernel=mlai.linear_cov, \n",
    "                    diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('linear_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "= \\alpha_1 k_1(\\mathbf{x}, \\mathbf{x}^\\prime) + \\alpha_2 k_2(\\mathbf{x}, \\mathbf{x}^\\prime)$$\"\"\",\n",
    "                     shortname='add', \n",
    "                     longname='Additive', kernel=mlai.add_cov, \n",
    "                     kerns=[mlai.exponentiated_quadratic, mlai.linear_cov], \n",
    "                     kern_args = [{'lengthscale': 0.1}, {'variance': 2}],\n",
    "                    diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('add_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "=  k_1(\\mathbf{x}, \\mathbf{x}^\\prime) k_2(\\mathbf{x}, \\mathbf{x}^\\prime)$$\"\"\",\n",
    "                     shortname='prod', \n",
    "                     longname='Product', kernel=mlai.prod_cov, \n",
    "                     kerns=[mlai.exponentiated_quadratic, mlai.polynomial_cov], \n",
    "                     kern_args = [{'lengthscale': 0.1}, {'variance': 1, 'degree': 3, 'b': 0}],\n",
    "                    diagrams ='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "HTML(open('prod_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot.covariance_func(x, compute_kernel, \n",
    "                     formula = r\"\"\"$$k(\\mathbf{x}, \\mathbf{x}^\\prime) \n",
    "= \\boldsymbol{\\phi}(\\mathbf{x})^\\top \\boldsymbol{\\phi(\\mathbf{x}^\\prime)}$$\"\"\",\n",
    "                     shortname='basis', \n",
    "                     longname='Basis Function', kernel=mlai.basis_cov, \n",
    "                     basis=mlai.radial,\n",
    "                    diagrams='../../slides/diagrams/kern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(open('basis_covariance.html', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change point example (automatic statistician)\n",
    "##  Boechner's theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (noiseless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:\n",
    "* Add points at a time, C14 calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pods\n",
    "import matplotlib.pyplot as plt\n",
    "import mlai\n",
    "import teaching_plots as plot \n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = mlai.exponentiated_quadratic\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "num_data = x.shape[0]\n",
    "\n",
    "data_limits = [1892, 2020]\n",
    "\n",
    "max_basis = y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengthscales=(5, 100, 5)\n",
    "plot.rmse_fit(x, y, param_name='lengthscale', param_range=lengthscales,  \n",
    "              model=mlai.GP, kernel=kernel, variance=1., sigma2=0.04, xlim=data_limits,\n",
    "              objective_ylim=[1.9,4], diagrams='../../slides/diagrams/gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_GP_exponentiated_quadratic_lengthscale{lengthscale:0>3}.svg', \n",
    "                            directory='../../slides/diagrams/gp', lengthscale=lengthscales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengthscales=(5, 100, 5)\n",
    "num_parts = 5\n",
    "plot.cv_fit(x, y, param_name='lengthscale', param_range=lengthscales,  \n",
    "              model=mlai.GP, kernel=kernel, variance=1., sigma2=0.04, xlim=data_limits,\n",
    "              objective_ylim=[0.2,0.85], num_parts=5, diagrams='../../slides/diagrams/gp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pods.notebook.display_plots('olympic_{num_parts}'.format(num_parts=5) \n",
    "                            + 'cv{part:0>2}_GP_exponentiated_quadratic_lengthscale{lengthscale:0>3}.svg', \n",
    "                            directory='../../slides/diagrams/gp', part=(0,5),lengthscale=lengthscales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Gaussian Noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log Odds\n",
    "\n",
    "* model the *log-odds* with the basis functions.\n",
    "* [odds](http://en.wikipedia.org/wiki/Odds) are defined as the ratio of the probability of a positive outcome, to the probability of a negative outcome. \n",
    "* Probability is between zero and one, odds are:\n",
    "    $$ \\frac{\\pi}{1-\\pi} $$\n",
    "* Odds are between $0$ and $\\infty$. \n",
    "* Logarithm of odds maps them to $-\\infty$ to $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logit Link Function\n",
    "\n",
    "* The [Logit function](http://en.wikipedia.org/wiki/Logit), $$g^{-1}(\\pi_i) = \\log\\frac{\\pi_i}{1-\\pi_i}.$$ This function is known as a *link function*.\n",
    "\n",
    "* For a standard regression we take,\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i),$$\n",
    "* For classification we perform a logistic regression. \n",
    "    $$\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse Link Function\n",
    "\n",
    "We have defined the link function as taking the form $g^{-1}(\\cdot)$ implying that the inverse link function is given by $g(\\cdot)$. Since we have defined,\n",
    "$$\n",
    "g^{-1}(\\pi(\\mathbf{x})) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "we can write $\\pi$ in terms of the *inverse link* function, $g(\\cdot)$ as \n",
    "$$\n",
    "\\pi(\\mathbf{x}) = g(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.logistic(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic function\n",
    "\n",
    "* [Logistic](http://en.wikipedia.org/wiki/Logistic_function) (or sigmoid) squashes real line to between 0   & 1. Sometimes also called a 'squashing function'. \n",
    "![](../diagrams/logistic.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Prediction Function\n",
    "* Can now write $\\pi$ as a function of the input and the parameter vector as, $$\\pi(\\mathbf{x},\\mathbf{w}) = \\frac{1}{1+ \\exp\\left(-\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)}.$$\n",
    "\n",
    "* Compute the output of a standard linear basis function composition ($\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})$, as we did for linear regression)\n",
    "\n",
    "* Apply the inverse link function, $g(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}))$. \n",
    "\n",
    "* Use this value in a Bernoulli distribution to form the likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bernoulli Reminder\n",
    "\n",
    "$$P(y_i|\\mathbf{w}, \\mathbf{x}) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}$$\n",
    "\n",
    "* Trick for switching betwen probabilities\n",
    "```python\n",
    "def bernoulli(y, pi):\n",
    "    if y == 1:\n",
    "        return pi\n",
    "    else:\n",
    "        return 1-pi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "\n",
    "* Conditional independence of data:$$P(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) = \\prod_{i=1}^n P(y_i|\\mathbf{w}, \\mathbf{x}_i). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood\n",
    "\n",
    "\\begin{align*}\n",
    "\\log P(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) = & \\sum_{i=1}^n \\log P(y_i|\\mathbf{w}, \\mathbf{x}_i) \\\\ = &\\sum_{i=1}^n y_i \\log \\pi_i \\\\ & + \\sum_{i=1}^n (1-y_i)\\log (1-\\pi_i)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective Function\n",
    "\n",
    "* Probability of positive outcome for the $i$th data point $$\\pi_i = g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)\\right),$$ where $g(\\cdot)$ is the *inverse* link function\n",
    "\n",
    "* Objective function of the form \\begin{align*}\n",
    "E(\\mathbf{w}) = & -  \\sum_{i=1}^n y_i \\log g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)\\right) \\\\& - \\sum_{i=1}^n(1-y_i)\\log \\left(1-g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimize Objective\n",
    "\n",
    "* Grdient wrt  $\\pi(\\mathbf{x};\\mathbf{w})$ \n",
    "\\begin{align*}\n",
    "\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}} = & -\\sum_{i=1}^n \\frac{y_i}{g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)}\\frac{\\text{d}g(f_i)}{\\text{d}f_i} \\boldsymbol{\\phi(\\mathbf{x}_i)} \\\\ & +  \\sum_{i=1}^n \\frac{1-y_i}{1-g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)}\\frac{\\text{d}g(f_i)}{\\text{d}f_i} \\boldsymbol{\\phi(\\mathbf{x}_i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Link Function Gradient\n",
    "\n",
    "* Also need gradient of inverse link function wrt parameters.\n",
    "\\begin{align*}\n",
    "g(f_i) &= \\frac{1}{1+\\exp(-f_i)}\\\\\n",
    "&=(1+\\exp(-f_i))^{-1}\n",
    "\\end{align*}\n",
    "and the gradient can be computed as\n",
    "\\begin{align*}\n",
    "\\frac{\\text{d}g(f_i)}{\\text{d} f_i} & = \\exp(-f_i)(1+\\exp(-f_i))^{-2}\\\\\n",
    "& = \\frac{1}{1+\\exp(-f_i)} \\frac{\\exp(-f_i)}{1+\\exp(-f_i)} \\\\\n",
    "& = g(f_i) (1-g(f_i))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective Gradient\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}} = & -\\sum_{i=1}^n y_i\\left(1-g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)\\right) \\boldsymbol{\\phi(\\mathbf{x}_i)} \\\\ & + \\sum_{i=1}^n (1-y_i)\\left(g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)\\right) \\boldsymbol{\\phi(\\mathbf{x}_i)}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization of the Function\n",
    "\n",
    "* Can't find a stationary point of the objective function analytically.\n",
    "\n",
    "* Optimization has to proceed by *numerical methods*. \n",
    "    * [Newton's method](http://en.wikipedia.org/wiki/Newton%27s_method) or \n",
    "    * [gradient based optimization methods](http://en.wikipedia.org/wiki/Gradient_method) \n",
    "    \n",
    "* Similarly to matrix factorization, for large data *stochastic gradient descent*  (Robbins Munroe optimization procedure) works well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "* Poisson distribution is used for 'count data'. For non-negative integers, $y$, $$P(y) = \\frac{\\lambda^y}{y!}\\exp(-y)$$\n",
    "\n",
    "* Here $\\lambda$ is a *rate* parameter that can be thought of as the number of arrivals per unit time.\n",
    "\n",
    "* Poisson distributions can be used for disease count data. E.g. number of incidence of malaria in a district.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plot.poisson(diagrams='../../slides/diagrams/ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "![](../diagrams/poisson.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson Regression\n",
    "\n",
    "* In a Poisson regression make rate a function of space/time.$$\\log \\lambda(\\mathbf{x}, t) = \\mathbf{w}_x^\\top \\boldsymbol{\\phi_x(\\mathbf{x})} + \\mathbf{w}_t^\\top \\boldsymbol{\\phi}_t(t)$$\n",
    "\n",
    "* This is known as a *log linear* or *log additive* model. \n",
    "\n",
    "* The link function is a logarithm.\n",
    "\n",
    "* We can rewrite such a function as \n",
    "$$\\log \\lambda(\\mathbf{x}, t) = f_x(\\mathbf{x}) + f_t(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiplicative Model\n",
    "\n",
    "* Be careful though ... a log additive model is really multiplicative.\n",
    "$$\\log \\lambda(\\mathbf{x}, t) = f_x(\\mathbf{x}) + f_t(t)$$\n",
    "\n",
    "* Becomes $$\\lambda(\\mathbf{x}, t) = \\exp(f_x(\\mathbf{x}) + f_t(t))$$\n",
    "\n",
    "* Which is equivalent to  $$\\lambda(\\mathbf{x}, t) = \\exp(f_x(\\mathbf{x}))\\exp(f_t(t))$$\n",
    "\n",
    "* Link functions can be deceptive in this way.\n",
    "\n",
    "@Saul:chained16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import GPy\n",
    "GPy.examples.regression.multiple_optima()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.path.basename(GPy.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load -s _contour_data ~/SheffieldML/GPy/GPy/examples/regression.py\n",
    "def _contour_data(data, length_scales, log_SNRs, kernel_call=GPy.kern.RBF):\n",
    "    \"\"\"\n",
    "    Evaluate the GP objective function for a given data set for a range of\n",
    "    signal to noise ratios and a range of lengthscales.\n",
    "\n",
    "    :data_set: A data set from the utils.datasets director.\n",
    "    :length_scales: a list of length scales to explore for the contour plot.\n",
    "    :log_SNRs: a list of base 10 logarithm signal to noise ratios to explore for the contour plot.\n",
    "    :kernel: a kernel to use for the 'signal' portion of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    lls = []\n",
    "    total_var = np.var(data['Y'])\n",
    "    kernel = kernel_call(1, variance=1., lengthscale=1.)\n",
    "    model = GPy.models.GPRegression(data['X'], data['Y'], kernel=kernel)\n",
    "    for log_SNR in log_SNRs:\n",
    "        SNR = 10.**log_SNR\n",
    "        noise_var = total_var / (1. + SNR)\n",
    "        signal_var = total_var - noise_var\n",
    "        model.kern['.*variance'] = signal_var\n",
    "        model.likelihood.variance = noise_var\n",
    "        length_scale_lls = []\n",
    "\n",
    "        for length_scale in length_scales:\n",
    "            model['.*lengthscale'] = length_scale\n",
    "            length_scale_lls.append(model.log_likelihood())\n",
    "\n",
    "        lls.append(length_scale_lls)\n",
    "\n",
    "    return np.array(lls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s multiple_optima ~/SheffieldML/GPy/GPy/examples/regression.py\n",
    "def multiple_optima(gene_number=937, resolution=80, model_restarts=10, seed=10000, max_iters=300, optimize=True, plot=True):\n",
    "    \"\"\"\n",
    "    Show an example of a multimodal error surface for Gaussian process\n",
    "    regression. Gene 937 has bimodal behaviour where the noisy mode is\n",
    "    higher.\n",
    "    \"\"\"\n",
    "\n",
    "    # Contour over a range of length scales and signal/noise ratios.\n",
    "    length_scales = np.linspace(0.1, 60., resolution)\n",
    "    log_SNRs = np.linspace(-3., 4., resolution)\n",
    "\n",
    "    try:import pods\n",
    "    except ImportError:\n",
    "        print('pods unavailable, see https://github.com/sods/ods for example datasets')\n",
    "        return\n",
    "    data = pods.datasets.della_gatta_TRP63_gene_expression(data_set='della_gatta',gene_number=gene_number)\n",
    "\n",
    "    data['Y'] -= np.mean(data['Y'])\n",
    "\n",
    "    lls = _contour_data(data, length_scales, log_SNRs, GPy.kern.RBF)\n",
    "    if plot:\n",
    "        plt.contour(length_scales, log_SNRs, np.exp(lls), 20, cmap=plt.cm.jet)\n",
    "        ax = plt.gca()\n",
    "        plt.xlabel('length scale')\n",
    "        plt.ylabel('log_10 SNR')\n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "\n",
    "    # Now run a few optimizations\n",
    "    models = []\n",
    "    optim_point_x = np.empty(2)\n",
    "    optim_point_y = np.empty(2)\n",
    "    np.random.seed(seed=seed)\n",
    "    for i in range(0, model_restarts):\n",
    "        # kern = GPy.kern.RBF(1, variance=np.random.exponential(1.), lengthscale=np.random.exponential(50.))\n",
    "        kern = GPy.kern.RBF(1, variance=np.random.uniform(1e-3, 1), lengthscale=np.random.uniform(5, 50))\n",
    "\n",
    "        m = GPy.models.GPRegression(data['X'], data['Y'], kernel=kern)\n",
    "        m.likelihood.variance = np.random.uniform(1e-3, 1)\n",
    "        optim_point_x[0] = m.rbf.lengthscale\n",
    "        optim_point_y[0] = np.log10(m.rbf.variance) - np.log10(m.likelihood.variance);\n",
    "\n",
    "        # optimize\n",
    "        if optimize:\n",
    "            m.optimize('scg', xtol=1e-6, ftol=1e-6, max_iters=max_iters)\n",
    "        \n",
    "        optim_point_x[1] = m.rbf.lengthscale\n",
    "        optim_point_y[1] = np.log10(m.rbf.variance) - np.log10(m.likelihood.variance);\n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            arr = plt.Arrow(optim_point_x[0], optim_point_y[0], optim_point_x[1] - optim_point_x[0], optim_point_y[1] - optim_point_y[0])#, label=str(i), head_length=1, head_width=0.5, fc='k', ec='k')\n",
    "            ax.add_patch(arr)\n",
    "            m.plot()\n",
    "        models.append(m)\n",
    "        \n",
    "    if plot:\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "    return m # (models, lls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "m = multiple_optima()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
