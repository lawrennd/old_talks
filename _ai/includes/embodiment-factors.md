\ifndef{embodimentFactors}
\define{embodimentFactors}

\include{_ai/includes/embodiment-factors-short.md}
\include{_ai/includes/formula-one-engine.md}
\include{_ai/includes/marcel-renault.md}
\include{_ai/includes/caleb-mcduff.md}

\editme

\notes{For humans, that means much of our computation should be dedicated to considering *what* we should compute. To do that efficiently we need to model the world around us. The most complex thing in the world around us is other humans. So, it is no surprise that we model them. We second guess what their intentions are, and our communication is only necessary when they are departing from how we model them. Naturally, for this to work well, we need to understand those we work closely with. It is no surprise that social communication, social bonding, forms so much of a part of our use of our limited bandwidth. 

There is a second effect here, our need to anthropomorphize objects around us. Our tendency to model our fellow humans extends to when we interact with other entities in our environment. To our pets as well as inanimate objects around us, such as computers or even our cars. This tendency to over interpret could be a consequence of our limited ability to communicate.[^store] 

For more details see this paper ["Living Together: Mind and Machine Intelligence"](https://arxiv.org/abs/1705.07996), and this [TEDx talk](http://inverseprobability.com/talks/lawrence-tedx17/living-together.html) and Chapter 1 in @Lawrence-atomic24.

[^store]: Another related factor is our ability to *store* information. @Moillica-humansstore19 suggest that during language acquisition we store 1.5 Megabytes of data (12 million bits). That would take around 2000 hours, or nearly twelve weeks, to transmit verbally.}

\addreading{@Lawrence-atomic24}{Chapter 1}

\endif
