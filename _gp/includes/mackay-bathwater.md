\ifndef{mackayBathwater}
\define{mackayBathwater}
\editme

\newslide{}

\slides{\includepng{../slides/diagrams/gp/mackay-baby}}

\newslide{Structure of Priors}

\notes{Even in the early days of Gaussian processes in machine learning, it was understood that we were throwing something fundamental away. This is perhaps captured best by David MacKay in his 1997 NeurIPS tutorial on Gaussian processes, where he asked "Have we thrown out the baby with the bathwater?". The quote below is from his summarization paper.

> According to the hype of 1987, neural networks were meant to be intelligent models which discovered features and patterns in data. Gaussian processes in contrast are simply smoothing devices. How can Gaussian processes possibly repalce neural networks? What is going on?
>
> @MacKay:gpintroduction98
}
\slides{MacKay: NeurIPS Tutorial 1997 “Have we thrown out the baby with the bathwater?” [Published as @MacKay:gpintroduction98]}

\endif
