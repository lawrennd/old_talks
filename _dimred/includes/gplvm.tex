\ifndef{gplvm}
\define{gplvm}

\editme

\section{Dual Probabilistic PCA and GP-LVM}

\begin{frame}
  \frametitle{Outline}

  \tableofcontents[currentsection,hideallsubsections] 

\end{frame}

\subsection{Dual Probabilistic PCA}

\begin{frame}
  \frametitle{Dual Probabilistic PCA}

  \textbf{Probabilistic PCA}
  \begin{itemize}
  \item We have seen that PCA has a probabilistic interpretation {\scriptsize \citep{Tipping:probpca99}}.
  \item It is difficult to `non-linearise' directly.
  \item GTM and Density Networks are an attempt to do so.
  \end{itemize}
  ~

  \textbf{Dual Probabilistic PCA}
  \begin{itemize}
  \item There is an alternative probabilistic interpretation of PCA {\scriptsize \citep{Lawrence:pnpca05}}.
  \item This interpretation can be made non-linear.
  \item The result is non-linear probabilistic PCA.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Linear Latent Variable Model III}
  \begin{columns}[c]%{}


    \column{5cm}

    {Dual Probabilistic PCA}
    \begin{itemize}
    \item <1->Define \emph{linear-Gaussian relationship} between latent variables
      and data.

      \begin{itemize}
      \item <2->\textbf{Novel} Latent variable approach:
      \item <3->Define Gaussian prior over \emph{parameters}, $\mappingMatrix$.
      \item <4->Integrate out \emph{parameters}.
      \end{itemize}
    \end{itemize}

    \column{5cm}

    \begin{center}
      \includegraphics<1-4>[width=0.5\textwidth]{../../../gplvm/tex/diagrams/gplvmGraph}
      \par\end{center}

    \vspace{-1cm}


    \begin{center}
      {\scriptsize \only<1-4>{\[
          p\left(\dataMatrix|\latentMatrix,\mappingMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\dataVector_{i,:}}{\mappingMatrix\latentVector_{i,:}}{\dataStd^{2}\eye}\]
        }\only<3-4>{\[
          p\left(\mappingMatrix\right)=\prod_{i=1}^{\dataDim}\gaussianDist{\mappingVector_{i,:}}{\zerosVector}{\eye}\]
        }\only<4->{\[
          p\left(\dataMatrix|\latentMatrix\right)=\prod_{j=1}^{\dataDim}\gaussianDist{\dataVector_{:,j}}{\zerosVector}{\latentMatrix\latentMatrix^{\top}+\dataStd^{2}\eye}\]
        }}
    \end{center}
    
  \end{columns}%{}

\end{frame}

 \begin{frame}
   \frametitle{Linear Latent Variable Model IV}
  
   \only<1-2>{\textbf{\emph{Dual}}} \textbf{Probabilistic PCA Max.
     Likelihood Soln} {\scriptsize \only<1-2>{\citep{Lawrence:gplvm03,Lawrence:pnpca05}}}
   {\scriptsize \only<3>{\citep{Tipping:probpca99}}}

   \begin{center}
     \includegraphics<1>[width=0.25\textwidth]{../../../gplvm/tex/diagrams/gplvmGraph}
   \end{center}
  
   \begin{center}
     {\scriptsize \vspace{-1cm}
       \only<1>{
         \[
         p\left(\dataMatrix|\latentMatrix\right)=\prod_{j=1}^{\dataDim}\gaussianDist{\dataVector_{:,j}}{\zerosVector}{\latentMatrix\latentMatrix^{\top}+\dataStd^{2}\eye}
         \]
       }\only<2>{
         \[
         p\left(\dataMatrix|\latentMatrix\right)=\prod_{j=1}^{\dataDim}\gaussianDist{\dataVector_{:,j}}{\zerosVector}{\kernelMatrix},\quad\quad\kernelMatrix=\latentMatrix\mathbf{\latentMatrix}^{\top}+\dataStd^{2}\eye
         \]
         \[
         \log p\left(\dataMatrix|\latentMatrix\right)=-\frac{\dataDim}{2}\log\left|\kernelMatrix\right|-\frac{1}{2}\tr{\kernelMatrix^{-1}\dataMatrix\dataMatrix^{\top}}+\mbox{const.}
         \]
         If $\eigenvectorMatrix_{\latentDim}^{\prime}$ are first $\latentDim$
         principal eigenvectors of $\dataDim^{-1}\dataMatrix\dataMatrix^{\top}$
         and the corresponding eigenvalues are $\Lambda_{\latentDim}$,
         \[
         \latentMatrix=\mathbf{U^{\prime}}_{\latentDim}\mathbf{L}\rotationMatrix^{\top},\quad\quad\mathbf{L}=\left(\Lambda_{\latentDim}-\dataStd^{2}\eye\right)^{\frac{1}{2}}
         \]
         where $\rotationMatrix$ is an arbitrary rotation matrix.
       }\only<3>{\[
         p\left(\dataMatrix|\mappingMatrix\right)=\prod_{i=1}^{\numData}\gaussianDist{\dataVector_{i,:}}{\zerosVector}{\covarianceMatrix},\quad\quad\covarianceMatrix=\mappingMatrix\mappingMatrix^{\top}+\dataStd^{2}\eye\]
         \[
         \log p\left(\dataMatrix|\mappingMatrix\right)=-\frac{\numData}{2}\log\left|\covarianceMatrix\right|-\frac{1}{2}\tr{\covarianceMatrix^{-1}\dataMatrix^{\top}\dataMatrix}+\mbox{const.}
         \]
         If $\eigenvectorMatrix_{\latentDim}$ are first $\latentDim$ principal
         eigenvectors of $\numData^{-1}\dataMatrix^{\top}\dataMatrix$
         and the corresponding eigenvalues are $\Lambda_{\latentDim}$,
         \[
         \mappingMatrix=\eigenvectorMatrix_{\latentDim}\mathbf{L}\rotationMatrix^{\top},\quad\quad\mathbf{L}=\left(\Lambda_{\latentDim}-\dataStd^{2}\eye\right)^{\frac{1}{2}}
         \]
         where $\rotationMatrix$ is an arbitrary rotation matrix.}
     }
   \end{center}


\end{frame}

\begin{frame}
  \frametitle{Equivalence of Formulations}

  \textbf{The Eigenvalue Problems are equivalent}
  \begin{itemize}
  \item Solution for Probabilistic PCA (solves for the mapping)
    {\footnotesize 
      \[
      \dataMatrix^{\top}\dataMatrix\eigenvectorMatrix_{\latentDim}=\eigenvectorMatrix_{\latentDim}\Lambda_{\latentDim}\quad\quad\quad\mappingMatrix=\eigenvectorMatrix_{\latentDim}\mathbf{L}\mathbf{V}^{\top}
      \]
    }

  \item Solution for Dual Probabilistic PCA (solves for the latent positions)
    {\footnotesize 
      \[
      \dataMatrix\dataMatrix^{\top}\eigenvectorMatrix_{\latentDim}^{\prime}=\eigenvectorMatrix_{\latentDim}^{\prime}\Lambda_{\latentDim}\quad\quad\quad\latentMatrix=\eigenvectorMatrix_{\latentDim}^{\prime}\mathbf{L}\mathbf{V}^{\top}
      \]
    }
    
  \item Equivalence is from
    {\footnotesize 
      \[
      \eigenvectorMatrix_{\latentDim}=\dataMatrix^{\top}\eigenvectorMatrix_{\latentDim}^{\prime}\Lambda_{\latentDim}^{-\frac{1}{2}}
      \]
    }
  \end{itemize}


\end{frame}

\subsection{Gaussian Processes}



\begin{frame}
  \frametitle{Gaussian Process (GP)}

  \textbf{Prior for Functions}
  \begin{itemize}
  \item Probability Distribution over Functions
  \item Functions are infinite dimensional.

    \begin{itemize}
    \item Prior distribution over \emph{instantiations} of the function: finite
      dimensional objects.
    \item Can prove by induction that GP is `consistent'.
    \end{itemize}
  \item Mean and Covariance Functions
  \item Instead of mean and covariance matrix, GP is defined by mean function
    and covariance function.

    \begin{itemize}
    \item Mean function often taken to be zero or constant.
    \item Covariance function must be \emph{positive definite}.
    \item Class of valid covariance functions is the same as the class
      of \emph{Mercer kernels}.
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Gaussian Processes II}

  \textbf{Zero mean Gaussian Process}
  \begin{itemize}
  \item A (zero mean) Gaussian process likelihood is of the form\[
    p\left(\dataVector|\latentMatrix\right)=N\left(\dataVector|\mathbf{0},\kernelMatrix\right),\]
    where $\kernelMatrix$ is the covariance function or \emph{kernel}.
  \item The \emph{linear kernel} with noise has the form\[
    \kernelMatrix=\latentMatrix\latentMatrix^{\top}+\dataStd^{2}\eye\]

  \item Priors over non-linear functions are also possible.

    \begin{itemize}
    \item To see what functions look like, we can sample from the prior process.
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Covariance Samples}

  \texttt{demCovFuncSample}%
  \begin{figure}
    \includegraphics<2>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample1}
    \includegraphics<3>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample2}
    \includegraphics<4>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample3}
    \includegraphics<1>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample4}
    \includegraphics<5>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample5}
    \includegraphics<6>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample6}
    \includegraphics<7>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample7}
    \includegraphics<8>[width=0.5\textwidth]{../../../gp/tex/diagrams/demCovFuncSample8}

    \caption{\only<2>{RBF kernel with $\rbfWidth=0.3$, $\alpha=1$ }\only<3>{ RBF
        kernel with $\rbfWidth=1$, $\alpha=1$ }\only<4>{ RBF kernel with $\rbfWidth=0.3$,
        $\alpha=4$}\only<1>{linear kernel, $\kernelMatrix=\latentMatrix\latentMatrix^{\top}$}
      \only<5>{ MLP kernel with $\alpha=8$, $w=100$ and $b=100$}\only<6>{
        MLP kernel with $\alpha=8$, $b=0$ and $w=100$}\only<7>{bias kernel
        with $\alpha=1$ and }\only<8>{summed combination of: RBF kernel,
        $\alpha=1$, $\rbfWidth=0.3$; bias kernel, $\alpha=$1; and white noise kernel,
        $\beta=100$}\label{cap:kernelSamples}}

  \end{figure}



\end{frame}

\begin{frame}
  \frametitle{Gaussian Process Regression}

  \textbf{Posterior Distribution over Functions}
  \begin{itemize}
  \item Gaussian processes are often used for regression.
  \item We are given a known inputs $\latentMatrix$ and targets $\dataMatrix$.
  \item We assume a prior distribution over functions by selecting a kernel.
  \item Combine the prior with data to get a \emph{posterior} distribution
    over functions.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Gaussian Process Regression}

  \texttt{demRegression}%
  \begin{figure}
    \includegraphics<1>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression1}
    \includegraphics<2>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression2}
    \includegraphics<3>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression3}
    \includegraphics<4>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression4}
    \includegraphics<5>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression5}
    \includegraphics<6>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression6}
    \includegraphics<7>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression7}
    \includegraphics<8>[width=0.5\textwidth]{../../../gp/tex/diagrams/demRegression8}

    \caption{Examples include WiFi localization, C14 callibration curve.}

  \end{figure}



\end{frame}

\begin{frame}
  \frametitle{Learning Kernel Parameters}


  \framesubtitle{Can we determine length scales and noise levels from the data?}

  \texttt{demOptimiseGp}

  \includegraphics<1>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp1}
  \includegraphics<2>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp3}
  \includegraphics<3>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp5}
  \includegraphics<4>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp7}
  \includegraphics<5>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp9}
  \includegraphics<6>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp11}
  \includegraphics<7>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp13}
  \includegraphics<8>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp15}
  \includegraphics<9>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp17}\hfill
  \includegraphics<1>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp2}
  \includegraphics<2>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp4}
  \includegraphics<3>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp6}
  \includegraphics<4>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp8}
  \includegraphics<5>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp10}
  \includegraphics<6>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp12}
  \includegraphics<7>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp14}
  \includegraphics<8>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp16}
  \includegraphics<9>[width=0.45\textwidth]{../../../gp/tex/diagrams/demOptimiseGp18}


\end{frame}

\begin{frame}
  \frametitle{Non-Linear Latent Variable Model}
  \begin{columns}[c]


    \column{5cm}

    \textbf{Dual Probabilistic PCA}

    \only<1>{
      \begin{itemize}
      \item Define \emph{linear-Gaussian relationship} between latent variables
        and data.
      \item \textbf{Novel} Latent variable approach:

        \begin{itemize}
        \item Define Gaussian prior over \emph{parameteters}, $\mappingMatrix$.
        \item Integrate out \emph{parameters}.
        \end{itemize}
      \end{itemize}
    }\only<2->{
      \begin{itemize}
      \item <2->Inspection of the marginal likelihood shows ...

        \begin{itemize}
        \item <3->The covariance matrix is a covariance function.
        \item <4->We recognise it as the `linear kernel'.
        \end{itemize}
      \end{itemize}
    }


    \column{5cm}

    \begin{center}
      \includegraphics<1->[width=0.5\textwidth]{../../../gplvm/tex/diagrams/gplvmGraph}
      \par\end{center}

    \vspace{-1cm}


    \begin{center}
      {\scriptsize \only<1>{\[
          p\left(\dataMatrix|\latentMatrix,\mappingMatrix\right)=\prod_{i=1}^{n}N\left(\dataVector_{i,:}|\mappingMatrix\latentVector_{i,:},\dataStd^{2}\eye\right)\]
          \[
          p\left(\mappingMatrix\right)=\prod_{i=1}^{d}N\left(\mappingVector_{i,:}|\mathbf{0},\eye\right)\]
        }\only<1-2>{\[
          p\left(\dataMatrix|\latentMatrix\right)=\prod_{j=1}^{d}N\left(\dataVector_{:,j}|\mathbf{0},\latentMatrix\latentMatrix^{\top}+\dataStd^{2}\eye\right)\]
        }\only<3->{\[
          p\left(\dataMatrix|\latentMatrix\right)=\prod_{j=1}^{d}N\left(\dataVector_{:,j}|\mathbf{0},\kernelMatrix\right)\]
        }\only<3-4>{\[
          \kernelMatrix=\latentMatrix\latentMatrix^{\top}+\dataStd^{2}\eye\]
        }\only<4>{This is a product of Gaussian processes with linear kernels.}\only<5>{\[
          \kernelMatrix=?\]
          Replace linear kernel with non-linear kernel for non-linear model.}}
      \par\end{center}{\scriptsize \par}

  \end{columns}%{}

\end{frame}

\begin{frame}
  \frametitle{Non-Linear Latent Variable Model}

  \textbf{RBF Kernel}
  \begin{itemize}
  \item The RBF kernel has the form $\kernelScalar_{i,j}=\kernelScalar\left(\latentVector_{i,:},\latentVector_{j,:}\right),$
    where


    \[
    \kernelScalar\left(\latentVector_{i,:},\latentVector_{j,:}\right)=\alpha\exp\left(-\frac{\left(\latentVector_{i,:}-\latentVector_{j,:}\right)^{\top}\left(\latentVector_{i,:}-\latentVector_{j,:}\right)}{2\rbfWidth^{2}}\right).\]


  \item No longer possible to optimise wrt $\latentMatrix$ via an eigenvalue
    problem.
  \item Instead find gradients with respect to $\latentMatrix,\alpha,\rbfWidth$
    and $\dataStd^{2}$ and optimise using gradient methods.
  \end{itemize}
  \begin{center}

    \par\end{center}





\end{frame}

\subsection{Oil Data}

\begin{frame}
  \frametitle{Oil Data Set}

  % 
  \begin{figure}
    \centering{}\includegraphics[width=0.8\textwidth]{../../../fgplvm/tex/diagrams/demOilFull}
  \end{figure}



\end{frame}

\begin{frame}
  \frametitle{Oil Data Set II}

  \textbf{Nearest Neighbour error in }$\latentMatrix$
  \begin{itemize}
  \item Nearest neighbour classification in latent space.


    \begin{center}
      {\small }
      \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Method & PCA & LLE & GTM & GP-LVM\tabularnewline
        \hline
        Errors & 162 & 18 & 11 (best) & 1 \tabularnewline
        \hline
      \end{tabular}
    \end{center}

    \begin{center}
      \emph{cf} 2 errors in data space.
      \par\end{center}

  \end{itemize}

\end{frame}

\subsection{Stick Man Data}

\begin{frame}
  \frametitle{Stick Man}

  \texttt{demStick1}

  % 
  \begin{figure}
    \begin{centering}
      \only<1>{\vspace{4cm}
      }\only<2>{\includegraphics[width=0.5\textwidth]{../../../fgplvm/tex/diagrams/demStick1Connected}}
      \par\end{centering}

    \caption{The latent space for the stick man motion capture data. \vspace{-1cm}
    }
    
  \end{figure}

\end{frame}

\subsection{Applications}

\begin{frame}
  \frametitle{Applications}
  \begin{itemize}
  \item Style based inverse kinematics{\footnotesize \citep{Grochow:styleik04}.}
  \item Prior distributions for tracking {\footnotesize \citep{Urtasun:3dpeople06,Urtasun:priors05}.}
  \item Assisted drawing {\footnotesize \citep{Baxter:doodle06}.}
    \item \textbf{Speech Synthesis Demo}.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
  \item GPLVM based on a dual probabilistic interpretation of PCA.
  \item Straightforward to non-linearise it using Gaussian processes.
  \item Result is a non-linear probabilistic PCA.
  \item \emph{Optimise latent variables} rather than integrate them out.
  \end{itemize}
\end{frame}
